{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "  \n",
    "from statistics import mode\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import re\n",
    "import enchant\n",
    "import pickle\n",
    "import time\n",
    "import editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Record_id', 'Attribute_name', 'y_pred', 'y_act', 'Reason',\n",
       "       'total_vals', 'num_nans', '%_nans', 'mean_word_count',\n",
       "       'std_dev_word_count', 'has_delimiters', 'sample_1', 'sample_2',\n",
       "       'sample_3', 'sample_4', 'sample_5'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_label = {\n",
    "    'Datetime':0, \n",
    "    'Sentence':1, \n",
    "    'Custom Object': 2, \n",
    "    'URL': 3, \n",
    "    'Numbers': 4, \n",
    "    'List': 5}\n",
    "\n",
    "data = pd.read_csv('data/needs_extraction_data/labelled_data.csv')\n",
    "data['y_act'] = [dict_label[i] for i in data['y_act']]\n",
    "y = data.loc[:,['y_act']]\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Data mean: scaled_perc_nans             -2.745801e-16\n",
      "scaled_mean_token_count      -1.117919e-16\n",
      "scaled_std_dev_token_count   -2.236863e-17\n",
      "has_delimiters                3.105360e-01\n",
      "dtype: float64\n",
      "\n",
      "> Data median: scaled_perc_nans             -0.653046\n",
      "scaled_mean_token_count      -0.144106\n",
      "scaled_std_dev_token_count   -0.171320\n",
      "has_delimiters                0.000000\n",
      "dtype: float64\n",
      "\n",
      "> Data stdev: scaled_perc_nans              1.000925\n",
      "scaled_mean_token_count       1.000925\n",
      "scaled_std_dev_token_count    1.000925\n",
      "has_delimiters                0.463141\n",
      "dtype: float64\n",
      "\n",
      "===[VECTORIZATION]===\n",
      "> Length of vectorized feature_names: 8528\n",
      "X_train preview:\n",
      "     scaled_perc_nans  scaled_mean_token_count  scaled_std_dev_token_count  \\\n",
      "453         -0.653097                 0.686283                    3.364514   \n",
      "43          -0.653120                 0.162079                   -0.054513   \n",
      "133          1.978459                -0.148544                   -0.167108   \n",
      "205         -0.653120                -0.141062                   -0.175870   \n",
      "282         -0.653120                -0.148960                   -0.175870   \n",
      "\n",
      "     has_delimiters  0  1  2  3  4  5  ...   8518  8519  8520  8521  8522  \\\n",
      "453            True  0  0  0  0  0  0  ...      0     0     0     0     0   \n",
      "43             True  0  0  0  0  0  0  ...      0     0     0     0     0   \n",
      "133            True  0  0  0  0  0  0  ...      0     0     0     0     0   \n",
      "205           False  0  0  0  0  0  0  ...      0     0     0     0     0   \n",
      "282           False  0  0  0  0  0  0  ...      0     0     0     0     0   \n",
      "\n",
      "     8523  8524  8525  8526  8527  \n",
      "453     0     0     0     0     0  \n",
      "43      0     0     0     0     0  \n",
      "133     0     0     0     0     0  \n",
      "205     0     0     0     0     0  \n",
      "282     0     0     0     0     0  \n",
      "\n",
      "[5 rows x 18519 columns]\n",
      "y_train preview:\n",
      "     y_act\n",
      "453    1.0\n",
      "43     1.0\n",
      "133    2.0\n",
      "205    0.0\n",
      "282    0.0\n"
     ]
    }
   ],
   "source": [
    "data1 = data[['%_nans', 'mean_word_count', 'std_dev_word_count', 'has_delimiters']]\n",
    "data1 = data1.fillna(0)\n",
    "\n",
    "data1 = data1.rename(columns={'mean_word_count': 'scaled_mean_token_count', 'std_dev_word_count': 'scaled_std_dev_token_count', '%_nans': 'scaled_perc_nans'})\n",
    "data1.loc[data1['scaled_mean_token_count'] > 10000, 'scaled_mean_token_count'] = 10000\n",
    "data1.loc[data1['scaled_mean_token_count'] < -10000, 'scaled_mean_token_count'] = -10000\n",
    "data1.loc[data1['scaled_std_dev_token_count'] > 10000, 'scaled_std_dev_token_count'] = 10000\n",
    "data1.loc[data1['scaled_std_dev_token_count'] < -10000, 'scaled_std_dev_token_count'] = -10000\n",
    "data1.loc[data1['scaled_perc_nans'] > 10000, 'scaled_perc_nans'] = 10000\n",
    "data1.loc[data1['scaled_perc_nans'] < -10000, 'scaled_perc_nans'] = -10000\n",
    "column_names_to_normalize = ['scaled_mean_token_count', 'scaled_std_dev_token_count','scaled_perc_nans']\n",
    "x = data1[column_names_to_normalize].values\n",
    "x = np.nan_to_num(x)\n",
    "x_scaled = StandardScaler().fit_transform(x)\n",
    "df_temp = pd.DataFrame(x_scaled, columns=column_names_to_normalize, index = data1.index)\n",
    "data1[column_names_to_normalize] = df_temp\n",
    "\n",
    "y.y_act = y.y_act.astype(float)\n",
    "\n",
    "print(f\"> Data mean: {data1.mean()}\\n\")\n",
    "print(f\"> Data median: {data1.median()}\\n\")\n",
    "print(f\"> Data stdev: {data1.std()}\\n\")\n",
    "\n",
    "print(\"===[VECTORIZATION]===\")\n",
    "arr = data['Attribute_name'].values\n",
    "data = data.fillna(0)\n",
    "arr1 = data['sample_1'].values\n",
    "arr1 = [str(x) for x in arr1]\n",
    "arr2 = data['sample_2'].values\n",
    "arr2 = [str(x) for x in arr2]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(3, 3), analyzer='char')\n",
    "X = vectorizer.fit_transform(arr)\n",
    "X1 = vectorizer.fit_transform(arr1)\n",
    "X2 = vectorizer.fit_transform(arr2)\n",
    "\n",
    "print(f\"> Length of vectorized feature_names: {len(vectorizer.get_feature_names())}\")\n",
    "\n",
    "# data1.to_csv('data/preprocessing/before.csv')\n",
    "attr_df = pd.DataFrame(X.toarray())\n",
    "sample1_df = pd.DataFrame(X1.toarray())\n",
    "sample2_df = pd.DataFrame(X2.toarray())\n",
    "\n",
    "data2 = pd.concat([data1, attr_df, sample1_df, sample2_df], axis=1, sort=False)\n",
    "# data2.to_csv('data/preprocessing/after.csv')\n",
    "data2.head()\n",
    "\n",
    "X_train, X_test_held, y_train, y_test_held = train_test_split(\n",
    "    data2, y, test_size=0.2, random_state=100)\n",
    "\n",
    "X_train_new = X_train.reset_index(drop=True)\n",
    "y_train_new = y_train.reset_index(drop=True)\n",
    "print(f\"X_train preview:\\n{X_train.head()}\")\n",
    "print(f\"y_train preview:\\n{y_train.head()}\")\n",
    "\n",
    "X_train_new = X_train_new.values\n",
    "y_train_new = y_train_new.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_train, X_val, y_train_train, y_val = train_test_split(\n",
    "    X_train_new, y_train_new, test_size=0.25, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_failed(model, x_test, y_test):\n",
    "    data = pd.read_csv('data/needs_extraction_data/labelled_data.csv')\n",
    "    dict_label = {\n",
    "        0: 'Datetime',\n",
    "        1: 'Sentence',\n",
    "        2: 'Custom Object',\n",
    "        3: 'URL',\n",
    "        4: 'Numbers',\n",
    "        5: 'List'\n",
    "    }\n",
    "    preds = model.predict(x_test)\n",
    "    ret_df = pd.DataFrame(columns=['Index', 'y_pred', 'y_act', 'sample_1', 'sample_2', 'sample_3'])\n",
    "    count = 0\n",
    "    for pred in preds:\n",
    "        y_true = int(y_test.values[count])\n",
    "        if pred != y_true:\n",
    "            index = x_test.index[count]\n",
    "            row = data.loc[index]\n",
    "            samples = [\n",
    "                row.sample_1,\n",
    "                row.sample_2,\n",
    "                row.sample_3\n",
    "            ]\n",
    "            ret_df.loc[count] = [index, pred, y_true] + samples\n",
    "        count += 1\n",
    "    ret_df['y_act'] = ret_df['y_act'].astype(int)\n",
    "    ret_df['y_pred'] = ret_df['y_pred'].astype(int)\n",
    "    ret_df['y_act'] = [dict_label[i] for i in ret_df['y_act']]\n",
    "    ret_df['y_pred'] = [dict_label[i] for i in ret_df['y_pred']]\n",
    "    return ret_df\n",
    "\n",
    "\n",
    "def get_false_nums(df):\n",
    "    false_labels = df.y_pred.unique()\n",
    "    true_labels = df.y_act.unique()\n",
    "    length = len(df)\n",
    "    for label in false_labels:\n",
    "        total = len(df.loc[df.y_pred == label])\n",
    "        print(f\">>> Incorrectly predicted as {label}: {total}, {'{0:.3g}'.format((total/length)*100)}%\")\n",
    "        \n",
    "        temp_df = df.loc[df.y_pred == label]\n",
    "        temp_labels = temp_df.y_act.unique()\n",
    "        t_len = len(temp_df)\n",
    "        for t_label in temp_labels:\n",
    "            t_total = len(temp_df[temp_df.y_act == t_label])\n",
    "            print(f\"\\t>>> {t_label} predicted as {label}: {t_total}, {'{0:.5g}'.format((t_total/t_len)*100)}%\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \"\"\"\n",
      "/Users/admin/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== [Random Forest] ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Train: 0.9328703703703703\n",
      "> Validate: 0.7592592592592593\n",
      "> Test: 0.8073394495412844\n",
      "> Precision Numbers: 0.9285714285714286\n",
      "> Precision Not Numbers: 0.9230769230769231\n",
      "> Recall Numbers: 0.5909090909090909\n",
      "> Recall Not Numbers: 0.8888888888888888\n",
      "\n",
      "==================== [Logistic Regression] ====================\n",
      "> Train: 0.9351851851851852\n",
      "> Validate: 0.7592592592592593\n",
      "> Test: 0.8807339449541285\n",
      "> Precision Numbers: 1.0\n",
      "> Precision Not Numbers: 1.0\n",
      "> Recall Numbers: 0.7727272727272727\n",
      "> Recall Not Numbers: 0.9259259259259259\n",
      "\n",
      "==================== [SVM] ====================\n",
      "> Train: 0.8842592592592593\n",
      "> Validate: 0.5370370370370371\n",
      "> Test: 0.5963302752293578\n",
      "> Precision Numbers: 1.0\n",
      "> Precision Not Numbers: 0.8571428571428571\n",
      "> Recall Numbers: 0.09090909090909091\n",
      "> Recall Not Numbers: 0.4444444444444444\n"
     ]
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=50, max_depth=50, random_state=100)\n",
    "lr_clf = LogisticRegression(C=1, random_state=100)\n",
    "svm_clf = svm.SVC(C=10, gamma=0.1, random_state=100)\n",
    "\n",
    "rf_clf.fit(X_train_train, y_train_train)\n",
    "lr_clf.fit(X_train_train, y_train_train)\n",
    "svm_clf.fit(X_train_train, y_train_train)\n",
    "\n",
    "print(\"=\"*20,\"[Random Forest]\",\"=\"*20)\n",
    "trainsc = rf_clf.score(X_train, y_train)\n",
    "valsc = rf_clf.score(X_val, y_val)\n",
    "heldsc = rf_clf.score(X_test_held, y_test_held)\n",
    "held_pr = metrics.precision_recall_fscore_support(y_test_held, rf_clf.predict(X_test_held))\n",
    "print(f\"> Train: {trainsc}\")\n",
    "print(f\"> Validate: {valsc}\")\n",
    "print(f\"> Test: {heldsc}\")\n",
    "print(f\"> Precision Numbers: {held_pr[0][1]}\")\n",
    "print(f\"> Precision Not Numbers: {held_pr[0][0]}\")\n",
    "print(f\"> Recall Numbers: {held_pr[1][1]}\")\n",
    "print(f\"> Recall Not Numbers: {held_pr[1][0]}\")\n",
    "print()\n",
    "print(\"=\"*20,\"[Logistic Regression]\",\"=\"*20)\n",
    "trainsc = lr_clf.score(X_train, y_train)\n",
    "valsc = lr_clf.score(X_val, y_val)\n",
    "heldsc = lr_clf.score(X_test_held, y_test_held)\n",
    "held_pr = metrics.precision_recall_fscore_support(y_test_held, lr_clf.predict(X_test_held))\n",
    "print(f\"> Train: {trainsc}\")\n",
    "print(f\"> Validate: {valsc}\")\n",
    "print(f\"> Test: {heldsc}\")\n",
    "print(f\"> Precision Numbers: {held_pr[0][1]}\")\n",
    "print(f\"> Precision Not Numbers: {held_pr[0][0]}\")\n",
    "print(f\"> Recall Numbers: {held_pr[1][1]}\")\n",
    "print(f\"> Recall Not Numbers: {held_pr[1][0]}\")\n",
    "print()\n",
    "print(\"=\"*20,\"[SVM]\",\"=\"*20)\n",
    "trainsc = svm_clf.score(X_train, y_train)\n",
    "valsc = svm_clf.score(X_val, y_val)\n",
    "heldsc = svm_clf.score(X_test_held, y_test_held)\n",
    "held_pr = metrics.precision_recall_fscore_support(y_test_held, svm_clf.predict(X_test_held))\n",
    "print(f\"> Train: {trainsc}\")\n",
    "print(f\"> Validate: {valsc}\")\n",
    "print(f\"> Test: {heldsc}\")\n",
    "print(f\"> Precision Numbers: {held_pr[0][1]}\")\n",
    "print(f\"> Precision Not Numbers: {held_pr[0][0]}\")\n",
    "print(f\"> Recall Numbers: {held_pr[1][1]}\")\n",
    "print(f\"> Recall Not Numbers: {held_pr[1][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \"\"\"\n",
      "/Users/admin/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== [Random Forest] ====================\n",
      ">>> Total incorrect predictions: 21, 19.27%\n",
      ">>> Incorrectly predicted as Custom Object: 18, 85.7%\n",
      "\t>>> List predicted as Custom Object: 5, 27.778%\n",
      "\t>>> Sentence predicted as Custom Object: 9, 50%\n",
      "\t>>> Numbers predicted as Custom Object: 1, 5.5556%\n",
      "\t>>> Datetime predicted as Custom Object: 3, 16.667%\n",
      ">>> Incorrectly predicted as Datetime: 2, 9.52%\n",
      "\t>>> Custom Object predicted as Datetime: 2, 100%\n",
      ">>> Incorrectly predicted as Sentence: 1, 4.76%\n",
      "\t>>> Custom Object predicted as Sentence: 1, 100%\n",
      "\n",
      "\n",
      "   Index         y_pred          y_act  \\\n",
      "0    485  Custom Object           List   \n",
      "1    188  Custom Object           List   \n",
      "9    175  Custom Object       Sentence   \n",
      "15   360  Custom Object        Numbers   \n",
      "21   356  Custom Object       Datetime   \n",
      "24   341       Datetime  Custom Object   \n",
      "46    31  Custom Object       Sentence   \n",
      "51   321  Custom Object           List   \n",
      "56   393  Custom Object       Sentence   \n",
      "59    56  Custom Object           List   \n",
      "\n",
      "                                             sample_1  \\\n",
      "0                        Sidewalk: Curb side : Cutout   \n",
      "1                                                 NaN   \n",
      "9   What are some of the biggest challenges you fa...   \n",
      "15                                             30.178   \n",
      "21                                         09/05/1947   \n",
      "24                                         2014-08-20   \n",
      "46  ramakrishna mission continues fiji relief work...   \n",
      "51                                                NaN   \n",
      "56                                                NaN   \n",
      "59  #Fabrics, #Woman Owned Biz, user_favorite, use...   \n",
      "\n",
      "                                             sample_2  \\\n",
      "0                        Sidewalk: Curb side : Cutout   \n",
      "1   http://www.amazon.co.uk/Doctor-Who-Eighth-Dale...   \n",
      "9                              Other (please specify)   \n",
      "15                                             21.806   \n",
      "21                                         14/11/1954   \n",
      "24                                                NaN   \n",
      "46                      hawks down dogs in launceston   \n",
      "51                                                NaN   \n",
      "56            Multiple Indicator Cluster Survey, 2011   \n",
      "59                #Eco-friendly, #Parent, #First Loan   \n",
      "\n",
      "                                             sample_3  \n",
      "0                        Sidewalk: Curb side : Cutout  \n",
      "1   http://www.amazon.co.uk/Dolls-Miniature-Christ...  \n",
      "9   Which of these core competencies do you look f...  \n",
      "15                                             23.845  \n",
      "21                                         25/01/1930  \n",
      "24                                                NaN  \n",
      "46      vic labor punished for not admitting mistakes  \n",
      "51  Comp.Sci., Engineering, Math., Info.Systems, P...  \n",
      "56          Multiple Indicator Cluster Survey 2014/15  \n",
      "59                         #Vegan, #Supporting Family  \n",
      "==================== [Logistic Regression] ====================\n",
      ">>> Total incorrect predictions: 13, 11.93%\n",
      ">>> Incorrectly predicted as Custom Object: 11, 84.6%\n",
      "\t>>> List predicted as Custom Object: 3, 27.273%\n",
      "\t>>> Numbers predicted as Custom Object: 1, 9.0909%\n",
      "\t>>> Datetime predicted as Custom Object: 2, 18.182%\n",
      "\t>>> Sentence predicted as Custom Object: 5, 45.455%\n",
      ">>> Incorrectly predicted as URL: 2, 15.4%\n",
      "\t>>> List predicted as URL: 1, 50%\n",
      "\t>>> Custom Object predicted as URL: 1, 50%\n",
      "\n",
      "\n",
      "   Index         y_pred     y_act  \\\n",
      "0    485  Custom Object      List   \n",
      "1    188            URL      List   \n",
      "15   360  Custom Object   Numbers   \n",
      "21   356  Custom Object  Datetime   \n",
      "46    31  Custom Object  Sentence   \n",
      "51   321  Custom Object      List   \n",
      "56   393  Custom Object  Sentence   \n",
      "59    56  Custom Object      List   \n",
      "62   339  Custom Object  Datetime   \n",
      "63    40  Custom Object  Sentence   \n",
      "\n",
      "                                             sample_1  \\\n",
      "0                        Sidewalk: Curb side : Cutout   \n",
      "1                                                 NaN   \n",
      "15                                             30.178   \n",
      "21                                         09/05/1947   \n",
      "46  ramakrishna mission continues fiji relief work...   \n",
      "51                                                NaN   \n",
      "56                                                NaN   \n",
      "59  #Fabrics, #Woman Owned Biz, user_favorite, use...   \n",
      "62                                                NaN   \n",
      "63             A political party for women's equality   \n",
      "\n",
      "                                             sample_2  \\\n",
      "0                        Sidewalk: Curb side : Cutout   \n",
      "1   http://www.amazon.co.uk/Doctor-Who-Eighth-Dale...   \n",
      "15                                             21.806   \n",
      "21                                         14/11/1954   \n",
      "46                      hawks down dogs in launceston   \n",
      "51                                                NaN   \n",
      "56            Multiple Indicator Cluster Survey, 2011   \n",
      "59                #Eco-friendly, #Parent, #First Loan   \n",
      "62                                         2015-07-24   \n",
      "63                                   Simplicity sells   \n",
      "\n",
      "                                             sample_3  \n",
      "0                        Sidewalk: Curb side : Cutout  \n",
      "1   http://www.amazon.co.uk/Dolls-Miniature-Christ...  \n",
      "15                                             23.845  \n",
      "21                                         25/01/1930  \n",
      "46      vic labor punished for not admitting mistakes  \n",
      "51  Comp.Sci., Engineering, Math., Info.Systems, P...  \n",
      "56          Multiple Indicator Cluster Survey 2014/15  \n",
      "59                         #Vegan, #Supporting Family  \n",
      "62                                         2014-11-03  \n",
      "63                                     The web as art  \n",
      "==================== [SVM] ====================\n",
      ">>> Total incorrect predictions: 44, 40.37%\n",
      ">>> Incorrectly predicted as Custom Object: 42, 95.5%\n",
      "\t>>> List predicted as Custom Object: 5, 11.905%\n",
      "\t>>> Datetime predicted as Custom Object: 15, 35.714%\n",
      "\t>>> Sentence predicted as Custom Object: 20, 47.619%\n",
      "\t>>> URL predicted as Custom Object: 2, 4.7619%\n",
      ">>> Incorrectly predicted as Datetime: 2, 4.55%\n",
      "\t>>> Numbers predicted as Datetime: 1, 50%\n",
      "\t>>> Custom Object predicted as Datetime: 1, 50%\n",
      "\n",
      "\n",
      "   Index         y_pred     y_act  \\\n",
      "0    485  Custom Object      List   \n",
      "1    188  Custom Object      List   \n",
      "8    477  Custom Object  Datetime   \n",
      "9    175  Custom Object  Sentence   \n",
      "11     3  Custom Object  Sentence   \n",
      "12   310  Custom Object  Datetime   \n",
      "15   360       Datetime   Numbers   \n",
      "16   227  Custom Object  Sentence   \n",
      "21   356  Custom Object  Datetime   \n",
      "22   459  Custom Object  Sentence   \n",
      "\n",
      "                                             sample_1  \\\n",
      "0                        Sidewalk: Curb side : Cutout   \n",
      "1                                                 NaN   \n",
      "8                                          2015-12-31   \n",
      "9   What are some of the biggest challenges you fa...   \n",
      "11  Caterpillar Updates 2015 Targets; Expects Prof...   \n",
      "12                                2017-03-10 17:00:00   \n",
      "15                                             30.178   \n",
      "16  Cloud Integration is a group of professionals ...   \n",
      "21                                         09/05/1947   \n",
      "22  Version: Chromium Build 394171OS: 10.11.5What ...   \n",
      "\n",
      "                                             sample_2  \\\n",
      "0                        Sidewalk: Curb side : Cutout   \n",
      "1   http://www.amazon.co.uk/Doctor-Who-Eighth-Dale...   \n",
      "8                                          2016-12-31   \n",
      "9                              Other (please specify)   \n",
      "11  ATCO Ltd. Announces Ron Southern's Intention t...   \n",
      "12                                2017-04-06 10:00:00   \n",
      "15                                             21.806   \n",
      "16  ManhattanSEO follows in the traditions of our ...   \n",
      "21                                         14/11/1954   \n",
      "22  Chrome Version     :  OS Version         :  <F...   \n",
      "\n",
      "                                             sample_3  \n",
      "0                        Sidewalk: Curb side : Cutout  \n",
      "1   http://www.amazon.co.uk/Dolls-Miniature-Christ...  \n",
      "8                                          2012-12-31  \n",
      "9   Which of these core competencies do you look f...  \n",
      "11                    PRESS DIGEST - Ireland - June 7  \n",
      "12                                2017-03-21 20:00:00  \n",
      "15                                             23.845  \n",
      "16  This is for serous people named John who love ...  \n",
      "21                                         25/01/1930  \n",
      "22  We have a bunch of performance problems caused...  \n"
     ]
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=50, max_depth=50, random_state=100)\n",
    "lr_clf = LogisticRegression(C=1, random_state=100)\n",
    "svm_clf = svm.SVC(C=10, gamma=0.1, random_state=100)\n",
    "\n",
    "rf_clf.fit(X_train_train, y_train_train)\n",
    "lr_clf.fit(X_train_train, y_train_train)\n",
    "svm_clf.fit(X_train_train, y_train_train)\n",
    "\n",
    "print(\"=\"*20,\"[Random Forest]\",\"=\"*20)\n",
    "fails = record_failed(rf_clf, X_test_held, y_test_held)\n",
    "print(f\">>> Total incorrect predictions: {len(fails)}, {'{0:.4g}'.format(((len(fails)/len(y_test_held))*100))}%\")\n",
    "get_false_nums(fails)\n",
    "print(fails.head(10))\n",
    "print(\"=\"*20,\"[Logistic Regression]\",\"=\"*20)\n",
    "fails = record_failed(lr_clf, X_test_held, y_test_held)\n",
    "print(f\">>> Total incorrect predictions: {len(fails)}, {'{0:.4g}'.format(((len(fails)/len(y_test_held))*100))}%\")\n",
    "get_false_nums(fails)\n",
    "print(fails.head(10))\n",
    "print(\"=\"*20,\"[SVM]\",\"=\"*20)\n",
    "fails = record_failed(svm_clf, X_test_held, y_test_held)\n",
    "print(f\">>> Total incorrect predictions: {len(fails)}, {'{0:.4g}'.format(((len(fails)/len(y_test_held))*100))}%\")\n",
    "get_false_nums(fails)\n",
    "print(fails.head(10))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
