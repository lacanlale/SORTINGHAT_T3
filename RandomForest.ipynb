{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn import metrics\n",
    "\n",
    "import pickle\n",
    "import math\n",
    "import re\n",
    "import enchant\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "np.random.seed(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv\n",
    "dict_label = {\n",
    "    'Datetime':0, \n",
    "    'Sentence':1, \n",
    "    'Custom Object': 2, \n",
    "    'URL': 3, \n",
    "    'Numbers': 4, \n",
    "    'List': 5}\n",
    "data = pd.read_csv('data/needs_extraction_data/labelled_data.csv')\n",
    "\n",
    "data['y_act'] = [dict_label[i] for i in data['y_act']]\n",
    "y = data.loc[:,['y_act']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Data mean: \n",
      "scaled_perc_nans             -2.745801e-16\n",
      "scaled_mean_token_count      -1.117919e-16\n",
      "scaled_std_dev_token_count   -2.236863e-17\n",
      "has_delimiters                3.105360e-01\n",
      "dtype: float64\n",
      "> Data median: \n",
      "scaled_perc_nans             -0.653046\n",
      "scaled_mean_token_count      -0.144106\n",
      "scaled_std_dev_token_count   -0.171320\n",
      "has_delimiters                0.000000\n",
      "dtype: float64\n",
      "> Data stdev: \n",
      "scaled_perc_nans              1.000925\n",
      "scaled_mean_token_count       1.000925\n",
      "scaled_std_dev_token_count    1.000925\n",
      "has_delimiters                0.463141\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data1 = data[['%_nans', 'mean_word_count', 'std_dev_word_count', 'has_delimiters']]\n",
    "data1 = data1.fillna(0)\n",
    "\n",
    "data1 = data1.rename(columns={'mean_word_count': 'scaled_mean_token_count', 'std_dev_word_count': 'scaled_std_dev_token_count', '%_nans': 'scaled_perc_nans'})\n",
    "data1.loc[data1['scaled_mean_token_count'] > 10000, 'scaled_mean_token_count'] = 10000\n",
    "data1.loc[data1['scaled_mean_token_count'] < -10000, 'scaled_mean_token_count'] = -10000\n",
    "data1.loc[data1['scaled_std_dev_token_count'] > 10000, 'scaled_std_dev_token_count'] = 10000\n",
    "data1.loc[data1['scaled_std_dev_token_count'] < -10000, 'scaled_std_dev_token_count'] = -10000\n",
    "data1.loc[data1['scaled_perc_nans'] > 10000, 'scaled_perc_nans'] = 10000\n",
    "data1.loc[data1['scaled_perc_nans'] < -10000, 'scaled_perc_nans'] = -10000\n",
    "column_names_to_normalize = ['scaled_mean_token_count', 'scaled_std_dev_token_count','scaled_perc_nans']\n",
    "x = data1[column_names_to_normalize].values\n",
    "x = np.nan_to_num(x)\n",
    "x_scaled = StandardScaler().fit_transform(x)\n",
    "df_temp = pd.DataFrame(x_scaled, columns=column_names_to_normalize, index = data1.index)\n",
    "data1[column_names_to_normalize] = df_temp\n",
    "\n",
    "y.y_act = y.y_act.astype(float)\n",
    "\n",
    "print(f\"> Data mean: \\n{data1.mean()}\")\n",
    "print(f\"> Data median: \\n{data1.median()}\")\n",
    "print(f\"> Data stdev: \\n{data1.std()}\")\n",
    "\n",
    "# data1.to_csv('before.csv')\n",
    "# f = open('current.txt','w')\n",
    "# d = enchant.Dict(\"en_US\")\n",
    "\n",
    "# for i in data.index:\n",
    "#     ival = data.at[i,'Attribute_name']\n",
    "#     if ival != 'id' and d.check(ivadf_tempdata1)\n",
    "#         print(f,ival)\n",
    "#         print(f,y.at[i,'y_act'])\n",
    "#         data1.at[i,'dictionary_item'] = 1\n",
    "#     else:\n",
    "#         data1.at[i,'dictionary_item'] = 0\n",
    "\n",
    "# data1.to_csv('after.csv')\n",
    "# f.close()\n",
    "# print(data1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===[VECTORIZATION]===\n",
      "> Length of vectorized feature_names: 8528\n",
      "X_train preview:      scaled_perc_nans  scaled_mean_token_count  scaled_std_dev_token_count  \\\n",
      "453         -0.653097                 0.686283                    3.364514   \n",
      "43          -0.653120                 0.162079                   -0.054513   \n",
      "133          1.978459                -0.148544                   -0.167108   \n",
      "205         -0.653120                -0.141062                   -0.175870   \n",
      "282         -0.653120                -0.148960                   -0.175870   \n",
      "\n",
      "     has_delimiters  0  1  2  3  4  5  ...   8518  8519  8520  8521  8522  \\\n",
      "453            True  0  0  0  0  0  0  ...      0     0     0     0     0   \n",
      "43             True  0  0  0  0  0  0  ...      0     0     0     0     0   \n",
      "133            True  0  0  0  0  0  0  ...      0     0     0     0     0   \n",
      "205           False  0  0  0  0  0  0  ...      0     0     0     0     0   \n",
      "282           False  0  0  0  0  0  0  ...      0     0     0     0     0   \n",
      "\n",
      "     8523  8524  8525  8526  8527  \n",
      "453     0     0     0     0     0  \n",
      "43      0     0     0     0     0  \n",
      "133     0     0     0     0     0  \n",
      "205     0     0     0     0     0  \n",
      "282     0     0     0     0     0  \n",
      "\n",
      "[5 rows x 18519 columns]\n",
      "y_train preview:      y_act\n",
      "453    1.0\n",
      "43     1.0\n",
      "133    2.0\n",
      "205    0.0\n",
      "282    0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"===[VECTORIZATION]===\")\n",
    "arr = data['Attribute_name'].values\n",
    "data = data.fillna(0)\n",
    "arr1 = data['sample_1'].values\n",
    "arr1 = [str(x) for x in arr1]\n",
    "arr2 = data['sample_2'].values\n",
    "arr2 = [str(x) for x in arr2]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(3, 3), analyzer='char')\n",
    "X = vectorizer.fit_transform(arr)\n",
    "X1 = vectorizer.fit_transform(arr1)\n",
    "X2 = vectorizer.fit_transform(arr2)\n",
    "\n",
    "print(f\"> Length of vectorized feature_names: {len(vectorizer.get_feature_names())}\")\n",
    "\n",
    "data1.to_csv('data/preprocessing/before.csv')\n",
    "tempdf = pd.DataFrame(X.toarray())\n",
    "tempdf1 = pd.DataFrame(X1.toarray())\n",
    "tempdf2 = pd.DataFrame(X2.toarray())\n",
    "\n",
    "data2 = pd.concat([data1, tempdf, tempdf1, tempdf2], axis=1, sort=False)\n",
    "data2.to_csv('data/preprocessing/after.csv')\n",
    "data2.head()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data2, y, test_size=0.2, random_state=100)\n",
    "atr_train,atr_test = train_test_split(data2, test_size=0.2,random_state=100)\n",
    "\n",
    "# X_train_train, X_test_train,y_train_train,y_test_train = train_test_split(X_train,y_train, test_size=0.25)\n",
    "# print(X_train.head())\n",
    "# print(y_train.head())\n",
    "\n",
    "X_train_new = X_train.reset_index(drop=True)\n",
    "y_train_new = y_train.reset_index(drop=True)\n",
    "print(f\"X_train preview: {X_train.head()}\")\n",
    "print(f\"y_train preview: {y_train.head()}\")\n",
    "\n",
    "X_train_new = X_train_new.values\n",
    "y_train_new = y_train_new.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "> Best training score: 0.9507246376811594\n",
      "> Best test score: 0.8045977011494253\n",
      "> Best held score: 0.8165137614678899\n",
      "==========\n",
      "> Best training score: 0.9333333333333333\n",
      "> Best test score: 0.8275862068965517\n",
      "> Best held score: 0.8256880733944955\n",
      "==========\n",
      "> Best training score: 0.9335260115606936\n",
      "> Best test score: 0.813953488372093\n",
      "> Best held score: 0.8073394495412844\n",
      "==========\n",
      "> Best training score: 0.9479768786127167\n",
      "> Best test score: 0.7906976744186046\n",
      "> Best held score: 0.8165137614678899\n",
      "==========\n",
      "> Best training score: 0.9248554913294798\n",
      "> Best test score: 0.8023255813953488\n",
      "> Best held score: 0.7706422018348624\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "kf = KFold(n_splits=k)\n",
    "avg_train_acc,avg_test_acc = 0,0\n",
    "    \n",
    "n_estimators_grid = [5,25,50,75,100]\n",
    "max_depth_grid = [5,10,25,50,100]\n",
    "\n",
    "avgsc_lst,avgsc_train_lst,avgsc_hld_lst = [],[],[]\n",
    "avgsc,avgsc_train,avgsc_hld = 0,0,0\n",
    "\n",
    "for train_index, test_index in kf.split(X_train_new):\n",
    "    X_train_cur, X_test_cur = X_train_new[train_index], X_train_new[test_index]\n",
    "    y_train_cur, y_test_cur = y_train_new[train_index], y_train_new[test_index]\n",
    "    X_train_train, X_val,y_train_train,y_val = train_test_split(X_train_cur,y_train_cur, test_size=0.25,random_state=100)\n",
    "    \n",
    "    bestPerformingModel = RandomForestClassifier(n_estimators=10,max_depth=5)\n",
    "    bestscore = 0\n",
    "    for ne in n_estimators_grid:\n",
    "        for md in max_depth_grid:\n",
    "            clf = RandomForestClassifier(n_estimators=ne,max_depth=md)\n",
    "            clf.fit(X_train_train, y_train_train.ravel())\n",
    "            sc = clf.score(X_val, y_val)\n",
    "            print(f\"[n_estimator: {ne}, max_depth: {md}, accuracy: {sc}]\")\n",
    "            if bestscore < sc:\n",
    "                bestscore = sc\n",
    "                bestPerformingModel = clf\n",
    "#                 print(bestPerformingModel)\n",
    "\n",
    "    bscr_train = bestPerformingModel.score(X_train_cur, y_train_cur)\n",
    "    bscr = bestPerformingModel.score(X_test_cur, y_test_cur)\n",
    "    bscr_hld = bestPerformingModel.score(X_test, y_test)\n",
    "\n",
    "    avgsc_train_lst.append(bscr_train)\n",
    "    avgsc_lst.append(bscr)\n",
    "    avgsc_hld_lst.append(bscr_hld)\n",
    "    \n",
    "    avgsc_train = avgsc_train + bscr_train    \n",
    "    avgsc = avgsc + bscr\n",
    "    avgsc_hld = avgsc_hld + bscr_hld\n",
    "\n",
    "    print('='*10)\n",
    "    print(f\"> Best training score: {bscr_train}\")\n",
    "    print(f\"> Best test score: {bscr}\")\n",
    "    print(f\"> Best held score: {bscr_hld}\")\n",
    "print('='*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Average training score list: [0.9507246376811594, 0.9333333333333333, 0.9335260115606936, 0.9479768786127167, 0.9248554913294798]\n",
      "> Average testing score list: [0.8045977011494253, 0.8275862068965517, 0.813953488372093, 0.7906976744186046, 0.8023255813953488]\n",
      "> Average held score list: [0.8165137614678899, 0.8256880733944955, 0.8073394495412844, 0.8165137614678899, 0.7706422018348624]\n",
      "\n",
      "> Average training score list/k: 0.9380832705034766\n",
      "> Average testing score list/k: 0.8078321304464048\n",
      "> Average held score list/k: 0.8073394495412843\n",
      "\n",
      "Confusion Matrix: Actual (Row) vs Predicted (Column)\n",
      "[[24  0  2  0  1  0]\n",
      " [ 2 13  7  0  0  0]\n",
      " [ 4  2 45  0  1  0]\n",
      " [ 0  0  0  2  0  0]\n",
      " [ 0  0  1  0  0  0]\n",
      " [ 0  0  5  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"> Average training score list: {avgsc_train_lst}\")\n",
    "print(f\"> Average testing score list: {avgsc_lst}\")\n",
    "print(f\"> Average held score list: {avgsc_hld_lst}\")\n",
    "print()\n",
    "print(f\"> Average training score list/k: {avgsc_train/k}\")\n",
    "print(f\"> Average testing score list/k: {avgsc/k}\")\n",
    "print(f\"> Average held score list/k: {avgsc_hld/k}\")\n",
    "print()\n",
    "y_pred = bestPerformingModel.predict(X_test)\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix: Actual (Row) vs Predicted (Column)')\n",
    "print(cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.20363636 0.         0.78909091 0.         0.00727273 0.        ]\n",
      " [0.00363636 0.4        0.58909091 0.         0.00727273 0.        ]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.8        0.         0.2        0.         0.         0.        ]\n",
      " [0.00363636 0.4        0.58909091 0.         0.00727273 0.        ]\n",
      " [0.21276596 0.         0.78297872 0.         0.00425532 0.        ]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.4        0.6        0.         0.         0.        ]\n",
      " [0.21640232 0.         0.77206963 0.         0.01152805 0.        ]\n",
      " [0.         0.8        0.         0.         0.         0.2       ]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.01640232 0.         0.97206963 0.         0.01152805 0.        ]\n",
      " [0.4        0.         0.6        0.         0.         0.        ]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         0.2        0.6        0.         0.         0.2       ]\n",
      " [0.         0.2        0.8        0.         0.         0.        ]\n",
      " [0.01276596 0.2        0.78297872 0.         0.00425532 0.        ]\n",
      " [0.01640232 0.         0.97206963 0.         0.01152805 0.        ]\n",
      " [0.21640232 0.         0.37206963 0.         0.41152805 0.        ]\n",
      " [0.         0.8        0.2        0.         0.         0.        ]\n",
      " [0.81276596 0.         0.18297872 0.         0.00425532 0.        ]\n",
      " [0.21276596 0.         0.78297872 0.         0.00425532 0.        ]\n",
      " [0.01640232 0.         0.97206963 0.         0.01152805 0.        ]\n",
      " [0.         0.6        0.4        0.         0.         0.        ]\n",
      " [0.01640232 0.         0.77206963 0.         0.21152805 0.        ]\n",
      " [0.6        0.         0.4        0.         0.         0.        ]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.2        0.4        0.4        0.         0.         0.        ]\n",
      " [0.4        0.         0.6        0.         0.         0.        ]\n",
      " [0.         0.6        0.2        0.         0.         0.2       ]\n",
      " [0.01640232 0.2        0.77206963 0.         0.01152805 0.        ]\n",
      " [0.4        0.         0.4        0.         0.2        0.        ]\n",
      " [0.00363636 0.         0.98909091 0.         0.00727273 0.        ]\n",
      " [0.01640232 0.         0.97206963 0.         0.01152805 0.        ]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.00363636 0.         0.98909091 0.         0.00727273 0.        ]\n",
      " [0.01640232 0.2        0.77206963 0.         0.01152805 0.        ]\n",
      " [0.01276596 0.         0.98297872 0.         0.00425532 0.        ]\n",
      " [0.21640232 0.         0.57206963 0.         0.21152805 0.        ]\n",
      " [0.8        0.         0.2        0.         0.         0.        ]\n",
      " [0.01640232 0.         0.97206963 0.         0.01152805 0.        ]\n",
      " [0.         0.2        0.6        0.         0.         0.2       ]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.41640232 0.         0.37206963 0.         0.21152805 0.        ]\n",
      " [0.01640232 0.2        0.77206963 0.         0.01152805 0.        ]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.01640232 0.         0.97206963 0.         0.01152805 0.        ]\n",
      " [0.         0.         0.         1.         0.         0.        ]\n",
      " [0.         0.6        0.2        0.         0.         0.2       ]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.00363636 0.         0.98909091 0.         0.00727273 0.        ]\n",
      " [0.21640232 0.2        0.37206963 0.         0.21152805 0.        ]\n",
      " [0.61640232 0.         0.37206963 0.         0.01152805 0.        ]\n",
      " [0.00363636 0.         0.98909091 0.         0.00727273 0.        ]\n",
      " [0.01276596 0.         0.98297872 0.         0.00425532 0.        ]\n",
      " [0.8        0.         0.2        0.         0.         0.        ]\n",
      " [0.01640232 0.2        0.77206963 0.         0.01152805 0.        ]\n",
      " [0.80363636 0.         0.18909091 0.         0.00727273 0.        ]\n",
      " [0.         0.6        0.4        0.         0.         0.        ]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.21276596 0.         0.78297872 0.         0.00425532 0.        ]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.00363636 0.         0.98909091 0.         0.00727273 0.        ]\n",
      " [0.01640232 0.         0.97206963 0.         0.01152805 0.        ]\n",
      " [0.01640232 0.         0.97206963 0.         0.01152805 0.        ]\n",
      " [0.4        0.4        0.         0.2        0.         0.        ]\n",
      " [0.81276596 0.         0.18297872 0.         0.00425532 0.        ]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.81276596 0.         0.18297872 0.         0.00425532 0.        ]\n",
      " [0.01640232 0.         0.97206963 0.         0.01152805 0.        ]\n",
      " [0.00363636 0.2        0.58909091 0.         0.20727273 0.        ]\n",
      " [0.00363636 0.4        0.58909091 0.         0.00727273 0.        ]\n",
      " [0.00363636 0.4        0.58909091 0.         0.00727273 0.        ]\n",
      " [0.00363636 0.2        0.58909091 0.         0.00727273 0.2       ]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.01640232 0.         0.97206963 0.         0.01152805 0.        ]\n",
      " [0.00363636 0.         0.98909091 0.         0.00727273 0.        ]\n",
      " [0.00363636 0.2        0.78909091 0.         0.00727273 0.        ]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.20363636 0.         0.38909091 0.2        0.20727273 0.        ]\n",
      " [0.         0.2        0.8        0.         0.         0.        ]\n",
      " [0.4        0.4        0.2        0.         0.         0.        ]\n",
      " [0.00363636 0.         0.78909091 0.2        0.00727273 0.        ]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         0.2        0.6        0.         0.         0.2       ]\n",
      " [0.8        0.         0.2        0.         0.         0.        ]\n",
      " [0.81276596 0.         0.18297872 0.         0.00425532 0.        ]\n",
      " [0.         0.4        0.2        0.         0.         0.4       ]\n",
      " [0.01640232 0.2        0.77206963 0.         0.01152805 0.        ]\n",
      " [0.01640232 0.         0.97206963 0.         0.01152805 0.        ]\n",
      " [0.40363636 0.         0.18909091 0.         0.40727273 0.        ]\n",
      " [0.01640232 0.         0.97206963 0.         0.01152805 0.        ]\n",
      " [0.         0.2        0.4        0.2        0.         0.2       ]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.2        0.         0.6        0.2        0.        ]\n",
      " [0.00363636 0.         0.98909091 0.         0.00727273 0.        ]\n",
      " [0.01640232 0.         0.97206963 0.         0.01152805 0.        ]\n",
      " [0.6        0.2        0.2        0.         0.         0.        ]\n",
      " [0.         0.4        0.6        0.         0.         0.        ]\n",
      " [0.01640232 0.         0.97206963 0.         0.01152805 0.        ]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.00363636 0.         0.98909091 0.         0.00727273 0.        ]]\n",
      "            0    1         2    3         4    5\n",
      "0    0.203636  0.0  0.789091  0.0  0.007273  0.0\n",
      "1    0.003636  0.4  0.589091  0.0  0.007273  0.0\n",
      "2    1.000000  0.0  0.000000  0.0  0.000000  0.0\n",
      "3    0.800000  0.0  0.200000  0.0  0.000000  0.0\n",
      "4    0.003636  0.4  0.589091  0.0  0.007273  0.0\n",
      "5    0.212766  0.0  0.782979  0.0  0.004255  0.0\n",
      "6    0.000000  0.0  1.000000  0.0  0.000000  0.0\n",
      "7    1.000000  0.0  0.000000  0.0  0.000000  0.0\n",
      "8    1.000000  0.0  0.000000  0.0  0.000000  0.0\n",
      "9    0.000000  0.4  0.600000  0.0  0.000000  0.0\n",
      "10   0.216402  0.0  0.772070  0.0  0.011528  0.0\n",
      "11   0.000000  0.8  0.000000  0.0  0.000000  0.2\n",
      "12   1.000000  0.0  0.000000  0.0  0.000000  0.0\n",
      "13   1.000000  0.0  0.000000  0.0  0.000000  0.0\n",
      "14   0.016402  0.0  0.972070  0.0  0.011528  0.0\n",
      "15   0.400000  0.0  0.600000  0.0  0.000000  0.0\n",
      "16   0.000000  1.0  0.000000  0.0  0.000000  0.0\n",
      "17   0.000000  0.2  0.600000  0.0  0.000000  0.2\n",
      "18   0.000000  0.2  0.800000  0.0  0.000000  0.0\n",
      "19   0.012766  0.2  0.782979  0.0  0.004255  0.0\n",
      "20   0.016402  0.0  0.972070  0.0  0.011528  0.0\n",
      "21   0.216402  0.0  0.372070  0.0  0.411528  0.0\n",
      "22   0.000000  0.8  0.200000  0.0  0.000000  0.0\n",
      "23   0.812766  0.0  0.182979  0.0  0.004255  0.0\n",
      "24   0.212766  0.0  0.782979  0.0  0.004255  0.0\n",
      "25   0.016402  0.0  0.972070  0.0  0.011528  0.0\n",
      "26   0.000000  0.6  0.400000  0.0  0.000000  0.0\n",
      "27   0.016402  0.0  0.772070  0.0  0.211528  0.0\n",
      "28   0.600000  0.0  0.400000  0.0  0.000000  0.0\n",
      "29   1.000000  0.0  0.000000  0.0  0.000000  0.0\n",
      "..        ...  ...       ...  ...       ...  ...\n",
      "79   1.000000  0.0  0.000000  0.0  0.000000  0.0\n",
      "80   0.016402  0.0  0.972070  0.0  0.011528  0.0\n",
      "81   0.003636  0.0  0.989091  0.0  0.007273  0.0\n",
      "82   0.003636  0.2  0.789091  0.0  0.007273  0.0\n",
      "83   1.000000  0.0  0.000000  0.0  0.000000  0.0\n",
      "84   0.203636  0.0  0.389091  0.2  0.207273  0.0\n",
      "85   0.000000  0.2  0.800000  0.0  0.000000  0.0\n",
      "86   0.400000  0.4  0.200000  0.0  0.000000  0.0\n",
      "87   0.003636  0.0  0.789091  0.2  0.007273  0.0\n",
      "88   0.000000  1.0  0.000000  0.0  0.000000  0.0\n",
      "89   0.000000  0.2  0.600000  0.0  0.000000  0.2\n",
      "90   0.800000  0.0  0.200000  0.0  0.000000  0.0\n",
      "91   0.812766  0.0  0.182979  0.0  0.004255  0.0\n",
      "92   0.000000  0.4  0.200000  0.0  0.000000  0.4\n",
      "93   0.016402  0.2  0.772070  0.0  0.011528  0.0\n",
      "94   0.016402  0.0  0.972070  0.0  0.011528  0.0\n",
      "95   0.403636  0.0  0.189091  0.0  0.407273  0.0\n",
      "96   0.016402  0.0  0.972070  0.0  0.011528  0.0\n",
      "97   0.000000  0.2  0.400000  0.2  0.000000  0.2\n",
      "98   0.000000  0.0  1.000000  0.0  0.000000  0.0\n",
      "99   0.000000  1.0  0.000000  0.0  0.000000  0.0\n",
      "100  1.000000  0.0  0.000000  0.0  0.000000  0.0\n",
      "101  0.000000  0.2  0.000000  0.6  0.200000  0.0\n",
      "102  0.003636  0.0  0.989091  0.0  0.007273  0.0\n",
      "103  0.016402  0.0  0.972070  0.0  0.011528  0.0\n",
      "104  0.600000  0.2  0.200000  0.0  0.000000  0.0\n",
      "105  0.000000  0.4  0.600000  0.0  0.000000  0.0\n",
      "106  0.016402  0.0  0.972070  0.0  0.011528  0.0\n",
      "107  1.000000  0.0  0.000000  0.0  0.000000  0.0\n",
      "108  0.003636  0.0  0.989091  0.0  0.007273  0.0\n",
      "\n",
      "[109 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# save the model to disk\n",
    "filename = 'data/pretrained/rf_finalized_model.sav'\n",
    "pickle.dump(bestPerformingModel, open(filename, 'wb'))\n",
    "\n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(X_test, y_test)\n",
    "y_prob = bestPerformingModel.predict_proba(X_test)\n",
    "\n",
    "df = pd.DataFrame.from_records(y_prob)\n",
    "print(df)\n",
    "df.to_csv('data/model_predictions/rf_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
