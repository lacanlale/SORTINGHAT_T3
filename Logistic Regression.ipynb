{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import linear_model\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv\n",
    "dict_label = {\n",
    "    'Datetime':0, \n",
    "    'Sentence':1, \n",
    "    'Custom Object': 2, \n",
    "    'URL': 3, \n",
    "    'Numbers': 4, \n",
    "    'List': 5}\n",
    "# data = pd.read_csv('data/needs_extraction_data/labelled_data.csv')\n",
    "data = pd.read_csv('data/needs_extraction_data/labelled_added.csv')\n",
    "data['y_act'] = [dict_label[i] for i in data['y_act']]\n",
    "y = data.loc[:,['y_act']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Data mean: scaled_perc_nans                -2.745801e-16\n",
      "scaled_mean_token_count         -1.117919e-16\n",
      "scaled_std_dev_token_count      -2.236863e-17\n",
      "has_delimiters                   3.105360e-01\n",
      "scaled_mean_stopword_total       8.619107e-18\n",
      "scaled_mean_whitespace_count    -1.126127e-16\n",
      "scaled_mean_char_count           5.130421e-17\n",
      "scaled_mean_delim_count         -5.915375e-17\n",
      "scaled_stdev_stopword_total      6.413026e-17\n",
      "scaled_stdev_whitespace_count   -2.236863e-17\n",
      "scaled_stdev_char_count         -3.488686e-18\n",
      "scaled_stdev_delim_count         9.516930e-17\n",
      "has_url                          8.687616e-02\n",
      "has_date                         7.560074e-01\n",
      "dtype: float64\n",
      "\n",
      "> Data median: scaled_perc_nans                -0.653046\n",
      "scaled_mean_token_count         -0.144106\n",
      "scaled_std_dev_token_count      -0.171320\n",
      "has_delimiters                   0.000000\n",
      "scaled_mean_stopword_total      -0.178121\n",
      "scaled_mean_whitespace_count    -0.144106\n",
      "scaled_mean_char_count          -0.166657\n",
      "scaled_mean_delim_count         -0.169718\n",
      "scaled_stdev_stopword_total     -0.204091\n",
      "scaled_stdev_whitespace_count   -0.171320\n",
      "scaled_stdev_char_count         -0.188179\n",
      "scaled_stdev_delim_count        -0.220007\n",
      "has_url                          0.000000\n",
      "has_date                         1.000000\n",
      "dtype: float64\n",
      "\n",
      "> Data stdev: scaled_perc_nans                 1.000925\n",
      "scaled_mean_token_count          1.000925\n",
      "scaled_std_dev_token_count       1.000925\n",
      "has_delimiters                   0.463141\n",
      "scaled_mean_stopword_total       1.000925\n",
      "scaled_mean_whitespace_count     1.000925\n",
      "scaled_mean_char_count           1.000925\n",
      "scaled_mean_delim_count          1.000925\n",
      "scaled_stdev_stopword_total      1.000925\n",
      "scaled_stdev_whitespace_count    1.000925\n",
      "scaled_stdev_char_count          1.000925\n",
      "scaled_stdev_delim_count         1.000925\n",
      "has_url                          0.281914\n",
      "has_date                         0.429886\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data1 = data[['%_nans', 'mean_word_count',\n",
    "              'std_dev_word_count', 'has_delimiters', 'mean_stopword_total',\n",
    "              'mean_whitespace_count', 'mean_char_count', 'mean_delim_count',\n",
    "              'stdev_stopword_total', 'stdev_whitespace_count', 'stdev_char_count',\n",
    "              'stdev_delim_count', 'has_url', 'has_date']]\n",
    "data1 = data1.fillna(0)\n",
    "\n",
    "data1 = data1.rename(columns={\n",
    "    'mean_word_count': 'scaled_mean_token_count',\n",
    "    'std_dev_word_count': 'scaled_std_dev_token_count',\n",
    "    '%_nans': 'scaled_perc_nans',\n",
    "    'mean_stopword_total': 'scaled_mean_stopword_total',\n",
    "    'mean_whitespace_count': 'scaled_mean_whitespace_count',\n",
    "    'mean_char_count': 'scaled_mean_char_count',\n",
    "    'mean_delim_count': 'scaled_mean_delim_count',\n",
    "    'stdev_stopword_total': 'scaled_stdev_stopword_total',\n",
    "    'stdev_whitespace_count': 'scaled_stdev_whitespace_count',\n",
    "    'stdev_char_count': 'scaled_stdev_char_count',\n",
    "    'stdev_delim_count': 'scaled_stdev_delim_count'\n",
    "})\n",
    "data1.loc[data1['scaled_mean_token_count'] >\n",
    "          10000, 'scaled_mean_token_count'] = 10000\n",
    "data1.loc[data1['scaled_mean_token_count'] < -\n",
    "          10000, 'scaled_mean_token_count'] = -10000\n",
    "\n",
    "data1.loc[data1['scaled_std_dev_token_count'] >\n",
    "          10000, 'scaled_std_dev_token_count'] = 10000\n",
    "data1.loc[data1['scaled_std_dev_token_count'] < -\n",
    "          10000, 'scaled_std_dev_token_count'] = -10000\n",
    "\n",
    "data1.loc[data1['scaled_perc_nans'] > 10000, 'scaled_perc_nans'] = 10000\n",
    "data1.loc[data1['scaled_perc_nans'] < -10000, 'scaled_perc_nans'] = -10000\n",
    "\n",
    "data1.loc[data1['scaled_mean_stopword_total'] >\n",
    "          10000, 'scaled_mean_stopword_total'] = 10000\n",
    "data1.loc[data1['scaled_mean_stopword_total'] < -\n",
    "          10000, 'scaled_mean_stopword_total'] = -10000\n",
    "\n",
    "data1.loc[data1['scaled_mean_whitespace_count'] >\n",
    "          10000, 'scaled_mean_whitespace_count'] = 10000\n",
    "data1.loc[data1['scaled_mean_whitespace_count'] < -\n",
    "          10000, 'scaled_mean_whitespace_count'] = -10000\n",
    "\n",
    "data1.loc[data1['scaled_mean_char_count'] >\n",
    "          10000, 'scaled_mean_char_count'] = 10000\n",
    "data1.loc[data1['scaled_mean_char_count'] < -\n",
    "          10000, 'scaled_mean_char_count'] = -10000\n",
    "\n",
    "data1.loc[data1['scaled_mean_delim_count'] >\n",
    "          10000, 'scaled_mean_delim_count'] = 10000\n",
    "data1.loc[data1['scaled_mean_delim_count'] < -\n",
    "          10000, 'scaled_mean_delim_count'] = -10000\n",
    "\n",
    "data1.loc[data1['scaled_stdev_stopword_total'] >\n",
    "          10000, 'scaled_stdev_stopword_total'] = 10000\n",
    "data1.loc[data1['scaled_stdev_stopword_total'] < -\n",
    "          10000, 'scaled_stdev_stopword_total'] = -10000\n",
    "\n",
    "data1.loc[data1['scaled_stdev_whitespace_count'] >\n",
    "          10000, 'scaled_stdev_whitespace_count'] = 10000\n",
    "data1.loc[data1['scaled_stdev_whitespace_count'] < -\n",
    "          10000, 'scaled_stdev_whitespace_count'] = -10000\n",
    "\n",
    "data1.loc[data1['scaled_stdev_char_count'] >\n",
    "          10000, 'scaled_stdev_char_count'] = 10000\n",
    "data1.loc[data1['scaled_stdev_char_count'] < -\n",
    "          10000, 'scaled_stdev_char_count'] = -10000\n",
    "\n",
    "data1.loc[data1['scaled_stdev_delim_count'] >\n",
    "          10000, 'scaled_stdev_delim_count'] = 10000\n",
    "data1.loc[data1['scaled_stdev_delim_count'] < -\n",
    "          10000, 'scaled_stdev_delim_count'] = -10000\n",
    "\n",
    "column_names_to_normalize = ['scaled_mean_token_count',\n",
    "                             'scaled_std_dev_token_count',\n",
    "                             'scaled_perc_nans',\n",
    "                             'scaled_mean_stopword_total',\n",
    "                             'scaled_mean_whitespace_count',\n",
    "                             'scaled_mean_char_count',\n",
    "                             'scaled_mean_delim_count',\n",
    "                             'scaled_stdev_stopword_total',\n",
    "                             'scaled_stdev_whitespace_count',\n",
    "                             'scaled_stdev_char_count',\n",
    "                             'scaled_stdev_delim_count']\n",
    "x = data1[column_names_to_normalize].values\n",
    "x = np.nan_to_num(x)\n",
    "x_scaled = StandardScaler().fit_transform(x)\n",
    "df_temp = pd.DataFrame(\n",
    "    x_scaled, columns=column_names_to_normalize, index=data1.index)\n",
    "data1[column_names_to_normalize] = df_temp\n",
    "\n",
    "y.y_act = y.y_act.astype(float)\n",
    "\n",
    "print(f\"> Data mean: {data1.mean()}\\n\")\n",
    "print(f\"> Data median: {data1.median()}\\n\")\n",
    "print(f\"> Data stdev: {data1.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===[VECTORIZATION]===\n",
      "> Length of vectorized feature_names: 8528\n",
      "X_train preview:      scaled_perc_nans  scaled_mean_token_count  scaled_std_dev_token_count  \\\n",
      "453         -0.653097                 0.686283                    3.364514   \n",
      "43          -0.653120                 0.162079                   -0.054513   \n",
      "133          1.978459                -0.148544                   -0.167108   \n",
      "205         -0.653120                -0.141062                   -0.175870   \n",
      "282         -0.653120                -0.148960                   -0.175870   \n",
      "\n",
      "     has_delimiters  scaled_mean_stopword_total  scaled_mean_whitespace_count  \\\n",
      "453            True                    0.945220                      0.686283   \n",
      "43             True                    0.126161                      0.162079   \n",
      "133            True                   -0.187400                     -0.148544   \n",
      "205           False                   -0.178121                     -0.141062   \n",
      "282           False                   -0.187845                     -0.148960   \n",
      "\n",
      "     scaled_mean_char_count  scaled_mean_delim_count  \\\n",
      "453                1.142812                 1.401002   \n",
      "43                 0.161438                 0.026371   \n",
      "133               -0.183377                -0.191491   \n",
      "205               -0.160778                -0.062569   \n",
      "282               -0.176629                -0.127171   \n",
      "\n",
      "     scaled_stdev_stopword_total  scaled_stdev_whitespace_count  ...   8518  \\\n",
      "453                     3.066931                       3.364514  ...      0   \n",
      "43                     -0.088312                      -0.054513  ...      0   \n",
      "133                    -0.202753                      -0.167108  ...      0   \n",
      "205                    -0.210742                      -0.175870  ...      0   \n",
      "282                    -0.210742                      -0.175870  ...      0   \n",
      "\n",
      "     8519  8520  8521  8522  8523  8524  8525  8526  8527  \n",
      "453     0     0     0     0     0     0     0     0     0  \n",
      "43      0     0     0     0     0     0     0     0     0  \n",
      "133     0     0     0     0     0     0     0     0     0  \n",
      "205     0     0     0     0     0     0     0     0     0  \n",
      "282     0     0     0     0     0     0     0     0     0  \n",
      "\n",
      "[5 rows x 18529 columns]\n",
      "y_train preview:      y_act\n",
      "453    1.0\n",
      "43     1.0\n",
      "133    2.0\n",
      "205    0.0\n",
      "282    0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"===[VECTORIZATION]===\")\n",
    "arr = data['Attribute_name'].values\n",
    "data = data.fillna(0)\n",
    "arr1 = data['sample_1'].values\n",
    "arr1 = [str(x) for x in arr1]\n",
    "arr2 = data['sample_2'].values\n",
    "arr2 = [str(x) for x in arr2]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(3, 3), analyzer='char')\n",
    "X = vectorizer.fit_transform(arr)\n",
    "X1 = vectorizer.fit_transform(arr1)\n",
    "X2 = vectorizer.fit_transform(arr2)\n",
    "\n",
    "print(f\"> Length of vectorized feature_names: {len(vectorizer.get_feature_names())}\")\n",
    "\n",
    "data1.to_csv('data/preprocessing/before.csv')\n",
    "attr_df = pd.DataFrame(X.toarray())\n",
    "sample1_df = pd.DataFrame(X1.toarray())\n",
    "sample2_df = pd.DataFrame(X2.toarray())\n",
    "\n",
    "data2 = pd.concat([data1, attr_df, sample1_df, sample2_df], axis=1, sort=False)\n",
    "data2.to_csv('data/preprocessing/after.csv')\n",
    "data2.head()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data2, y, test_size=0.2, random_state=100)\n",
    "\n",
    "# X_train_train, X_test_train,y_train_train,y_test_train = train_test_split(X_train,y_train, test_size=0.25)\n",
    "# print(X_train.head())\n",
    "# print(y_train.head())\n",
    "\n",
    "X_train_new = X_train.reset_index(drop=True)\n",
    "y_train_new = y_train.reset_index(drop=True)\n",
    "print(f\"X_train preview: {X_train.head()}\")\n",
    "print(f\"y_train preview: {y_train.head()}\")\n",
    "\n",
    "X_train_new = X_train_new.values\n",
    "y_train_new = y_train_new.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    acc_df = pd.read_csv('data/model_data.csv')\n",
    "    index = len(acc_df)\n",
    "except FileNotFoundError:\n",
    "    acc_df = pd.DataFrame(columns=['Model', 'Params', 'Feats', 'Train', 'Validation', 'Test', 'Precision'])\n",
    "    index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C: 0.0001, accuracy: 0.47126436781609193]\n",
      "[C: 0.001, accuracy: 0.4827586206896552]\n",
      "[C: 0.01, accuracy: 0.6896551724137931]\n",
      "[C: 0.1, accuracy: 0.7701149425287356]\n",
      "[C: 1, accuracy: 0.8045977011494253]\n",
      "[C: 10, accuracy: 0.8160919540229885]\n",
      "[C: 100, accuracy: 0.8160919540229885]\n",
      "[C: 1000, accuracy: 0.8160919540229885]\n",
      "[C: 10000, accuracy: 0.8275862068965517]\n",
      "[C: 100000, accuracy: 0.8390804597701149]\n",
      "\n",
      "> Best C: 100000\n",
      "> Best training score: 0.9594202898550724\n",
      "> Best test score: 0.7931034482758621\n",
      "> Best held score: 0.7889908256880734\n",
      "==========\n",
      "[C: 0.0001, accuracy: 0.4482758620689655]\n",
      "[C: 0.001, accuracy: 0.47126436781609193]\n",
      "[C: 0.01, accuracy: 0.6436781609195402]\n",
      "[C: 0.1, accuracy: 0.7011494252873564]\n",
      "[C: 1, accuracy: 0.7126436781609196]\n",
      "[C: 10, accuracy: 0.7241379310344828]\n",
      "[C: 100, accuracy: 0.7241379310344828]\n",
      "[C: 1000, accuracy: 0.7241379310344828]\n",
      "[C: 10000, accuracy: 0.7241379310344828]\n",
      "[C: 100000, accuracy: 0.735632183908046]\n",
      "\n",
      "> Best C: 100000\n",
      "> Best training score: 0.9333333333333333\n",
      "> Best test score: 0.8390804597701149\n",
      "> Best held score: 0.8440366972477065\n",
      "==========\n",
      "[C: 0.0001, accuracy: 0.45977011494252873]\n",
      "[C: 0.001, accuracy: 0.47126436781609193]\n",
      "[C: 0.01, accuracy: 0.6781609195402298]\n",
      "[C: 0.1, accuracy: 0.7586206896551724]\n",
      "[C: 1, accuracy: 0.7241379310344828]\n",
      "[C: 10, accuracy: 0.6781609195402298]\n",
      "[C: 100, accuracy: 0.6781609195402298]\n",
      "[C: 1000, accuracy: 0.6781609195402298]\n",
      "[C: 10000, accuracy: 0.6781609195402298]\n",
      "[C: 100000, accuracy: 0.6436781609195402]\n",
      "\n",
      "> Best C: 0.1\n",
      "> Best training score: 0.9046242774566474\n",
      "> Best test score: 0.813953488372093\n",
      "> Best held score: 0.8165137614678899\n",
      "==========\n",
      "[C: 0.0001, accuracy: 0.5287356321839081]\n",
      "[C: 0.001, accuracy: 0.5402298850574713]\n",
      "[C: 0.01, accuracy: 0.7586206896551724]\n",
      "[C: 0.1, accuracy: 0.7816091954022989]\n",
      "[C: 1, accuracy: 0.8045977011494253]\n",
      "[C: 10, accuracy: 0.7701149425287356]\n",
      "[C: 100, accuracy: 0.735632183908046]\n",
      "[C: 1000, accuracy: 0.7241379310344828]\n",
      "[C: 10000, accuracy: 0.735632183908046]\n",
      "[C: 100000, accuracy: 0.735632183908046]\n",
      "\n",
      "> Best C: 1\n",
      "> Best training score: 0.9450867052023122\n",
      "> Best test score: 0.7674418604651163\n",
      "> Best held score: 0.8165137614678899\n",
      "==========\n",
      "[C: 0.0001, accuracy: 0.5632183908045977]\n",
      "[C: 0.001, accuracy: 0.5977011494252874]\n",
      "[C: 0.01, accuracy: 0.7471264367816092]\n",
      "[C: 0.1, accuracy: 0.7931034482758621]\n",
      "[C: 1, accuracy: 0.8045977011494253]\n",
      "[C: 10, accuracy: 0.7586206896551724]\n",
      "[C: 100, accuracy: 0.7471264367816092]\n",
      "[C: 1000, accuracy: 0.7241379310344828]\n",
      "[C: 10000, accuracy: 0.7241379310344828]\n",
      "[C: 100000, accuracy: 0.735632183908046]\n",
      "\n",
      "> Best C: 1\n",
      "> Best training score: 0.9450867052023122\n",
      "> Best test score: 0.813953488372093\n",
      "> Best held score: 0.8073394495412844\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "kf = KFold(n_splits=k)\n",
    "avg_train_acc, avg_test_acc = 0, 0\n",
    "\n",
    "val_arr = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]\n",
    "\n",
    "avgsc_lst, avgsc_train_lst, avgsc_hld_lst = [], [], []\n",
    "avgsc, avgsc_train, avgsc_hld = 0, 0, 0\n",
    "\n",
    "best_param_count = {'cval': {}}\n",
    "for train_index, test_index in kf.split(X_train_new):\n",
    "    X_train_cur, X_test_cur = X_train_new[train_index], X_train_new[test_index]\n",
    "    y_train_cur, y_test_cur = y_train_new[train_index], y_train_new[test_index]\n",
    "    X_train_train, X_val, y_train_train, y_val = train_test_split(\n",
    "        X_train_cur, y_train_cur, test_size=0.25, random_state=100)\n",
    "\n",
    "    bestPerformingModel = LogisticRegression(\n",
    "        penalty='l2', multi_class='multinomial', solver='lbfgs', C=1)\n",
    "    bestscore = 0\n",
    "    print('='*10)\n",
    "    for val in val_arr:\n",
    "        clf = LogisticRegression(\n",
    "            penalty='l2', multi_class='multinomial', solver='lbfgs', C=val)\n",
    "        clf.fit(X_train_train, y_train_train)\n",
    "        sc = clf.score(X_val, y_val)\n",
    "        print(f\"[C: {val}, accuracy: {sc}]\")\n",
    "        if bestscore < sc:\n",
    "            bestcval = val\n",
    "            bestscore = sc\n",
    "            bestPerformingModel = clf\n",
    "    \n",
    "    if str(bestcval) in best_param_count['cval']:\n",
    "        best_param_count['cval'][str(bestcval)] += 1\n",
    "    else:\n",
    "        best_param_count['cval'][str(bestcval)] = 1\n",
    "        \n",
    "    bscr_train = bestPerformingModel.score(X_train_cur, y_train_cur)\n",
    "    bscr = bestPerformingModel.score(X_test_cur, y_test_cur)\n",
    "    bscr_hld = bestPerformingModel.score(X_test, y_test)\n",
    "\n",
    "    avgsc_train_lst.append(bscr_train)\n",
    "    avgsc_lst.append(bscr)\n",
    "    avgsc_hld_lst.append(bscr_hld)\n",
    "\n",
    "    avgsc_train = avgsc_train + bscr_train\n",
    "    avgsc = avgsc + bscr\n",
    "    avgsc_hld = avgsc_hld + bscr_hld\n",
    "    print()\n",
    "    print(f\"> Best C: {bestcval}\")\n",
    "    print(f\"> Best training score: {bscr_train}\")\n",
    "    print(f\"> Best test score: {bscr}\")\n",
    "    print(f\"> Best held score: {bscr_hld}\")\n",
    "print('='*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = bestPerformingModel.predict(X_test)\n",
    "prec = metrics.precision_score(y_test, y_pred, average=None)\n",
    "cat_prec = {\n",
    "    'Datetime': prec[0],\n",
    "    'Sentence': prec[1],\n",
    "    'Custom Object': prec[2],\n",
    "    'URL': prec[3],\n",
    "    'Numbers': prec[4],\n",
    "    'List': prec[5],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Best C param : 100000\n",
      "> Average training score list: [0.9594202898550724, 0.9333333333333333, 0.9046242774566474, 0.9450867052023122, 0.9450867052023122]\n",
      "> Average testing score list: [0.7931034482758621, 0.8390804597701149, 0.813953488372093, 0.7674418604651163, 0.813953488372093]\n",
      "> Average held score list: [0.7889908256880734, 0.8440366972477065, 0.8165137614678899, 0.8165137614678899, 0.8073394495412844]\n",
      "\n",
      "> Average training score list: 0.9375102622099355\n",
      "> Average testing score list: 0.8055065490510559\n",
      "> Average held score list: 0.8146788990825687\n",
      "\n",
      "Confusion Matrix: Actual (Row) vs Predicted (Column)\n",
      "[[25  0  2  0  0  0]\n",
      " [ 0 13  9  0  0  0]\n",
      " [ 0  3 48  0  0  1]\n",
      " [ 0  0  0  2  0  0]\n",
      " [ 0  0  1  0  0  0]\n",
      " [ 0  1  3  1  0  0]]\n"
     ]
    }
   ],
   "source": [
    "bestcval = max(best_param_count['cval'], key=lambda i: best_param_count['cval'][i])\n",
    "bestparams = {'C': bestcval}\n",
    "print(f\"> Best C param : {bestcval}\")\n",
    "print(f\"> Average training score list: {avgsc_train_lst}\")\n",
    "print(f\"> Average testing score list: {avgsc_lst}\")\n",
    "print(f\"> Average held score list: {avgsc_hld_lst}\")\n",
    "print()\n",
    "avgsc_train = avgsc_train/k\n",
    "avgsc = avgsc/k\n",
    "avgsc_hld = avgsc_hld/k\n",
    "print(f\"> Average training score list: {avgsc_train}\")\n",
    "print(f\"> Average testing score list: {avgsc}\")\n",
    "print(f\"> Average held score list: {avgsc_hld}\")\n",
    "acc_df.loc[index] = ['logistic_regression', str(bestparams),\"X_stats, X_name, X_sample1, X_sample2\", avgsc_train, avgsc, avgsc_hld, str(cat_prec)]\n",
    "index += 1\n",
    "print()\n",
    "\n",
    "y_pred = bestPerformingModel.predict(X_test)\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix: Actual (Row) vs Predicted (Column)')\n",
    "print(cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0         1             2             3             4  \\\n",
      "0    1.181673e-01  0.098610  6.360260e-01  1.721076e-02  9.800843e-02   \n",
      "1    2.843411e-05  0.000286  2.303777e-05  9.995600e-01  3.307245e-05   \n",
      "2    9.951963e-01  0.000876  1.986406e-03  2.920283e-04  9.047534e-04   \n",
      "3    9.752192e-01  0.003140  1.214961e-02  1.024544e-03  5.788319e-03   \n",
      "4    9.655696e-02  0.138211  5.584255e-01  1.899833e-02  1.077863e-01   \n",
      "5    2.364505e-01  0.022936  7.192846e-01  1.775745e-03  1.641101e-02   \n",
      "6    5.600111e-03  0.004756  9.805510e-01  4.999110e-04  7.631279e-03   \n",
      "7    9.961313e-01  0.000734  1.554019e-03  2.393487e-04  7.868153e-04   \n",
      "8    9.053435e-01  0.013995  3.639003e-02  3.457646e-03  3.012343e-02   \n",
      "9    6.181004e-02  0.404586  4.803388e-01  7.306844e-03  1.942451e-02   \n",
      "10   2.022077e-01  0.038689  6.240293e-01  1.280323e-02  7.173208e-02   \n",
      "11   1.684020e-02  0.795409  1.631445e-01  3.185398e-03  4.122382e-03   \n",
      "12   9.989208e-01  0.000137  4.222169e-04  7.612582e-05  2.924298e-04   \n",
      "13   9.832340e-01  0.002228  8.048023e-03  8.748409e-04  3.457892e-03   \n",
      "14   2.480461e-01  0.091889  4.589313e-01  1.502911e-02  1.508737e-01   \n",
      "15   2.900520e-01  0.041358  5.834866e-01  6.535585e-03  5.868814e-02   \n",
      "16   5.912740e-17  1.000000  9.912550e-10  1.349881e-11  1.420360e-15   \n",
      "17   9.423091e-02  0.064615  7.195399e-01  1.009703e-02  8.108634e-02   \n",
      "18   1.416603e-01  0.140063  5.713530e-01  9.256342e-03  1.067420e-01   \n",
      "19   1.106221e-01  0.291152  4.246935e-01  1.587704e-02  1.149839e-01   \n",
      "20   1.233586e-01  0.054265  6.765875e-01  1.172043e-02  1.099366e-01   \n",
      "21   3.718049e-01  0.057670  3.698214e-01  1.172118e-02  1.562433e-01   \n",
      "22   4.672803e-18  1.000000  4.027960e-09  1.272897e-11  5.653513e-17   \n",
      "23   9.750592e-01  0.002205  1.669737e-02  7.567342e-04  3.036512e-03   \n",
      "24   1.526321e-01  0.050408  7.623776e-01  2.553602e-03  2.677066e-02   \n",
      "25   1.256382e-01  0.035235  7.373995e-01  2.214619e-02  5.971401e-02   \n",
      "26   2.373650e-01  0.407053  2.654754e-01  8.127485e-03  5.679835e-02   \n",
      "27   3.390456e-02  0.028990  8.839264e-01  1.991485e-03  4.713130e-02   \n",
      "28   9.995315e-01  0.000050  2.448312e-04  3.099277e-05  4.864867e-05   \n",
      "29   9.556891e-01  0.004841  1.434280e-02  1.497312e-03  2.026292e-02   \n",
      "..            ...       ...           ...           ...           ...   \n",
      "79   8.761416e-01  0.010301  5.148351e-02  2.985646e-03  5.036201e-02   \n",
      "80   1.125248e-01  0.089588  6.603926e-01  1.308518e-02  9.670170e-02   \n",
      "81   1.205019e-01  0.048533  5.662027e-01  6.819645e-03  2.417928e-01   \n",
      "82   2.097949e-02  0.007815  9.543995e-01  1.608302e-03  9.578317e-03   \n",
      "83   9.619343e-01  0.004272  1.223467e-02  1.332986e-03  1.724301e-02   \n",
      "84   2.445331e-01  0.077775  5.188160e-01  2.497215e-02  8.964562e-02   \n",
      "85   1.175567e-01  0.204385  6.218012e-01  6.290401e-03  2.667349e-02   \n",
      "86   1.505451e-01  0.348876  3.824334e-01  8.256694e-03  3.527312e-02   \n",
      "87   1.136273e-01  0.031257  7.789066e-01  5.201367e-03  5.494454e-02   \n",
      "88   1.142783e-02  0.970290  8.618023e-03  1.424253e-03  4.631601e-03   \n",
      "89   1.442552e-02  0.084694  8.717210e-01  4.447931e-03  1.833957e-02   \n",
      "90   3.770575e-01  0.513476  4.964103e-02  1.113504e-02  1.913618e-02   \n",
      "91   9.175776e-01  0.008189  5.504576e-02  1.647047e-03  1.374838e-02   \n",
      "92   4.398599e-02  0.059412  7.994509e-01  6.831743e-03  1.960904e-02   \n",
      "93   7.056203e-02  0.073560  7.394546e-01  5.933261e-03  8.414754e-02   \n",
      "94   2.063990e-01  0.075145  6.150754e-01  1.177838e-02  6.636976e-02   \n",
      "95   1.757560e-01  0.052544  5.402422e-01  1.390797e-02  1.851922e-01   \n",
      "96   1.434334e-01  0.055577  6.581079e-01  7.220098e-03  1.106342e-01   \n",
      "97   1.501725e-02  0.093819  8.019792e-01  4.929077e-02  1.721425e-02   \n",
      "98   3.981770e-03  0.001644  9.879650e-01  5.483033e-04  3.812958e-03   \n",
      "99   1.893987e-02  0.909414  5.852926e-02  1.496752e-03  3.589844e-03   \n",
      "100  9.791497e-01  0.003961  8.357757e-03  9.871381e-04  5.240895e-03   \n",
      "101  1.205543e-02  0.012530  5.243526e-02  9.008215e-01  1.236951e-02   \n",
      "102  5.753057e-03  0.004725  9.802574e-01  5.066570e-04  7.783673e-03   \n",
      "103  8.550856e-03  0.004443  9.782562e-01  4.799358e-04  7.146189e-03   \n",
      "104  7.138774e-02  0.307477  5.438142e-01  1.024768e-02  1.846471e-02   \n",
      "105  6.264985e-02  0.501852  3.446713e-01  1.237677e-02  2.234782e-02   \n",
      "106  4.621268e-03  0.002635  9.874941e-01  3.599615e-04  4.227240e-03   \n",
      "107  9.746489e-01  0.004077  1.151574e-02  1.188998e-03  5.527459e-03   \n",
      "108  5.004887e-03  0.004182  9.827297e-01  4.539884e-04  6.760193e-03   \n",
      "\n",
      "                5  \n",
      "0    3.197734e-02  \n",
      "1    6.988535e-05  \n",
      "2    7.444306e-04  \n",
      "3    2.678255e-03  \n",
      "4    8.002153e-02  \n",
      "5    3.142356e-03  \n",
      "6    9.612520e-04  \n",
      "7    5.540809e-04  \n",
      "8    1.069056e-02  \n",
      "9    2.653413e-02  \n",
      "10   5.053860e-02  \n",
      "11   1.729829e-02  \n",
      "12   1.517040e-04  \n",
      "13   2.157294e-03  \n",
      "14   3.523090e-02  \n",
      "15   1.987932e-02  \n",
      "16   4.992770e-11  \n",
      "17   3.043039e-02  \n",
      "18   3.092506e-02  \n",
      "19   4.267154e-02  \n",
      "20   2.413181e-02  \n",
      "21   3.273956e-02  \n",
      "22   1.213978e-10  \n",
      "23   2.245269e-03  \n",
      "24   5.257883e-03  \n",
      "25   1.986743e-02  \n",
      "26   2.518129e-02  \n",
      "27   4.055989e-03  \n",
      "28   9.422561e-05  \n",
      "29   3.366858e-03  \n",
      "..            ...  \n",
      "79   8.726179e-03  \n",
      "80   2.770788e-02  \n",
      "81   1.615023e-02  \n",
      "82   5.619706e-03  \n",
      "83   2.983377e-03  \n",
      "84   4.425794e-02  \n",
      "85   2.329366e-02  \n",
      "86   7.461612e-02  \n",
      "87   1.606328e-02  \n",
      "88   3.608267e-03  \n",
      "89   6.372135e-03  \n",
      "90   2.955416e-02  \n",
      "91   3.792233e-03  \n",
      "92   7.070985e-02  \n",
      "93   2.634296e-02  \n",
      "94   2.523258e-02  \n",
      "95   3.235720e-02  \n",
      "96   2.502723e-02  \n",
      "97   2.267930e-02  \n",
      "98   2.047710e-03  \n",
      "99   8.030244e-03  \n",
      "100  2.303229e-03  \n",
      "101  9.787853e-03  \n",
      "102  9.744915e-04  \n",
      "103  1.123978e-03  \n",
      "104  4.860890e-02  \n",
      "105  5.610198e-02  \n",
      "106  6.620633e-04  \n",
      "107  3.042239e-03  \n",
      "108  8.690809e-04  \n",
      "\n",
      "[109 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# save the model to disk\n",
    "filename = 'data/pretrained/lr_finalized_model.pickle'\n",
    "pickle.dump(bestPerformingModel, open(filename, 'wb+'))\n",
    "\n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb+'))\n",
    "result = loaded_model.score(X_test, y_test)\n",
    "y_prob = bestPerformingModel.predict_proba(X_test)\n",
    "\n",
    "df = pd.DataFrame.from_records(y_prob)\n",
    "print(df)\n",
    "df.to_csv('data/model_predictions/lr_predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature combination testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_feat_combos(index):\n",
    "    combos = {\n",
    "        \"X_stats\": data1,\n",
    "        \"X_name\": attr_df,\n",
    "        \"X_stats, X_name\": pd.concat([data1, attr_df], axis=1, sort=False),\n",
    "        \"X_sample1\":  pd.concat([sample1_df], axis=1, sort=False),\n",
    "        \"X_name, X_sample1\":  pd.concat([attr_df, sample1_df], axis=1, sort=False),\n",
    "        \"X_stats, X_sample1\":  pd.concat([data1, sample1_df], axis=1, sort=False),\n",
    "        \"X_stats, X_name, X_sample1\":  pd.concat([data1, attr_df, sample1_df], axis=1, sort=False)\n",
    "    }\n",
    "    \n",
    "\n",
    "    for combo in combos:\n",
    "        print(\"=\"*50, combo, \"=\"*50)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            combos[combo], y, test_size=0.2, random_state=100)\n",
    "\n",
    "        X_train_new = X_train.reset_index(drop=True)\n",
    "        y_train_new = y_train.reset_index(drop=True)\n",
    "        X_train_new = X_train_new.values\n",
    "        y_train_new = y_train_new.values\n",
    "        best_param_count = {'cval': {}}\n",
    "        k = 5\n",
    "        kf = KFold(n_splits=k)\n",
    "        avg_train_acc, avg_test_acc = 0, 0\n",
    "\n",
    "        val_arr = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]\n",
    "\n",
    "        avgsc_lst, avgsc_train_lst, avgsc_hld_lst = [], [], []\n",
    "        avgsc, avgsc_train, avgsc_hld = 0, 0, 0\n",
    "\n",
    "        best_param_count = {'cval': {}}\n",
    "        for train_index, test_index in kf.split(X_train_new):\n",
    "            X_train_cur, X_test_cur = X_train_new[train_index], X_train_new[test_index]\n",
    "            y_train_cur, y_test_cur = y_train_new[train_index], y_train_new[test_index]\n",
    "            X_train_train, X_val, y_train_train, y_val = train_test_split(\n",
    "                X_train_cur, y_train_cur, test_size=0.25, random_state=100)\n",
    "\n",
    "            bestPerformingModel = LogisticRegression(\n",
    "                penalty='l2', multi_class='multinomial', solver='lbfgs', C=1)\n",
    "            bestscore = 0\n",
    "            print('\\t', '-'*10)\n",
    "            for val in val_arr:\n",
    "                clf = LogisticRegression(\n",
    "                    penalty='l2', multi_class='multinomial', solver='lbfgs', C=val)\n",
    "                clf.fit(X_train_train, y_train_train)\n",
    "                sc = clf.score(X_val, y_val)\n",
    "                print(f\"\\t[C: {val}, accuracy: {sc}]\")\n",
    "                if bestscore < sc:\n",
    "                    bestcval = val\n",
    "                    bestscore = sc\n",
    "                    bestPerformingModel = clf\n",
    "\n",
    "            if str(bestcval) in best_param_count['cval']:\n",
    "                best_param_count['cval'][str(bestcval)] += 1\n",
    "            else:\n",
    "                best_param_count['cval'][str(bestcval)] = 1\n",
    "            bscr_train = bestPerformingModel.score(X_train_cur, y_train_cur)\n",
    "            bscr = bestPerformingModel.score(X_test_cur, y_test_cur)\n",
    "            bscr_hld = bestPerformingModel.score(X_test, y_test)\n",
    "\n",
    "            avgsc_train_lst.append(bscr_train)\n",
    "            avgsc_lst.append(bscr)\n",
    "            avgsc_hld_lst.append(bscr_hld)\n",
    "\n",
    "            avgsc_train = avgsc_train + bscr_train\n",
    "            avgsc = avgsc + bscr\n",
    "            avgsc_hld = avgsc_hld + bscr_hld\n",
    "            print()\n",
    "            print(f\"\\t> Best C: {bestcval}\")\n",
    "            print(f\"\\t> Best training score: {bscr_train}\")\n",
    "            print(f\"\\t> Best test score: {bscr}\")\n",
    "            print(f\"\\t> Best held score: {bscr_hld}\")\n",
    "        print('\\t', '-'*10)\n",
    "        \n",
    "        y_pred = bestPerformingModel.predict(X_test)\n",
    "        prec = metrics.precision_score(y_test, y_pred, average=None)\n",
    "        cat_prec = {\n",
    "            'Datetime': prec[0],\n",
    "            'Sentence': prec[1],\n",
    "            'Custom Object': prec[2],\n",
    "            'URL': prec[3],\n",
    "            'Numbers': prec[4],\n",
    "            'List': prec[5],\n",
    "        }    \n",
    "        bestcval = max(best_param_count['cval'], key=lambda i: best_param_count['cval'][i])\n",
    "        bestparams = {'C': bestcval}\n",
    "        print(f\"\\t> Best C param : {bestcval}\")\n",
    "        print(f\"\\t> Average training score list: {avgsc_train_lst}\")\n",
    "        print(f\"\\t> Average testing score list: {avgsc_lst}\")\n",
    "        print(f\"\\t> Average held score list: {avgsc_hld_lst}\")\n",
    "        print()\n",
    "        avgsc_train = avgsc_train/k\n",
    "        avgsc = avgsc/k\n",
    "        avgsc_hld = avgsc_hld/k\n",
    "        print(f\"\\t> Average training score list: {avgsc_train}\")\n",
    "        print(f\"\\t> Average testing score list: {avgsc}\")\n",
    "        print(f\"\\t> Average held score list: {avgsc_hld}\")\n",
    "        acc_df.loc[index] = ['logistic_regression', str(bestparams), combo, avgsc_train, avgsc, avgsc_hld, str(cat_prec)]\n",
    "        index += 1\n",
    "        print()\n",
    "\n",
    "        y_pred = bestPerformingModel.predict(X_test)\n",
    "        cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "        print('\\tConfusion Matrix: Actual (Row) vs Predicted (Column)')\n",
    "        print('\\t', cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== X_stats ==================================================\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.4367816091954023]\n",
      "\t[C: 0.001, accuracy: 0.45977011494252873]\n",
      "\t[C: 0.01, accuracy: 0.47126436781609193]\n",
      "\t[C: 0.1, accuracy: 0.5977011494252874]\n",
      "\t[C: 1, accuracy: 0.6206896551724138]\n",
      "\t[C: 10, accuracy: 0.6781609195402298]\n",
      "\t[C: 100, accuracy: 0.7471264367816092]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[C: 1000, accuracy: 0.8160919540229885]\n",
      "\t[C: 10000, accuracy: 0.8160919540229885]\n",
      "\t[C: 100000, accuracy: 0.8505747126436781]\n",
      "\n",
      "\t> Best C: 100000\n",
      "\t> Best training score: 0.8782608695652174\n",
      "\t> Best test score: 0.7586206896551724\n",
      "\t> Best held score: 0.8256880733944955\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.41379310344827586]\n",
      "\t[C: 0.001, accuracy: 0.4367816091954023]\n",
      "\t[C: 0.01, accuracy: 0.4482758620689655]\n",
      "\t[C: 0.1, accuracy: 0.5517241379310345]\n",
      "\t[C: 1, accuracy: 0.5632183908045977]\n",
      "\t[C: 10, accuracy: 0.6091954022988506]\n",
      "\t[C: 100, accuracy: 0.6436781609195402]\n",
      "\t[C: 1000, accuracy: 0.7011494252873564]\n",
      "\t[C: 10000, accuracy: 0.6896551724137931]\n",
      "\t[C: 100000, accuracy: 0.7126436781609196]\n",
      "\n",
      "\t> Best C: 100000\n",
      "\t> Best training score: 0.8\n",
      "\t> Best test score: 0.8045977011494253\n",
      "\t> Best held score: 0.7889908256880734\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.4482758620689655]\n",
      "\t[C: 0.001, accuracy: 0.45977011494252873]\n",
      "\t[C: 0.01, accuracy: 0.45977011494252873]\n",
      "\t[C: 0.1, accuracy: 0.5747126436781609]\n",
      "\t[C: 1, accuracy: 0.5862068965517241]\n",
      "\t[C: 10, accuracy: 0.6551724137931034]\n",
      "\t[C: 100, accuracy: 0.7126436781609196]\n",
      "\t[C: 1000, accuracy: 0.7816091954022989]\n",
      "\t[C: 10000, accuracy: 0.8045977011494253]\n",
      "\t[C: 100000, accuracy: 0.7931034482758621]\n",
      "\n",
      "\t> Best C: 10000\n",
      "\t> Best training score: 0.846820809248555\n",
      "\t> Best test score: 0.8255813953488372\n",
      "\t> Best held score: 0.8715596330275229\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.4827586206896552]\n",
      "\t[C: 0.001, accuracy: 0.5172413793103449]\n",
      "\t[C: 0.01, accuracy: 0.5057471264367817]\n",
      "\t[C: 0.1, accuracy: 0.6551724137931034]\n",
      "\t[C: 1, accuracy: 0.6551724137931034]\n",
      "\t[C: 10, accuracy: 0.7011494252873564]\n",
      "\t[C: 100, accuracy: 0.7701149425287356]\n",
      "\t[C: 1000, accuracy: 0.8045977011494253]\n",
      "\t[C: 10000, accuracy: 0.8045977011494253]\n",
      "\t[C: 100000, accuracy: 0.8275862068965517]\n",
      "\n",
      "\t> Best C: 100000\n",
      "\t> Best training score: 0.8497109826589595\n",
      "\t> Best test score: 0.813953488372093\n",
      "\t> Best held score: 0.8440366972477065\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.5287356321839081]\n",
      "\t[C: 0.001, accuracy: 0.5517241379310345]\n",
      "\t[C: 0.01, accuracy: 0.5402298850574713]\n",
      "\t[C: 0.1, accuracy: 0.6091954022988506]\n",
      "\t[C: 1, accuracy: 0.6551724137931034]\n",
      "\t[C: 10, accuracy: 0.6666666666666666]\n",
      "\t[C: 100, accuracy: 0.7126436781609196]\n",
      "\t[C: 1000, accuracy: 0.7816091954022989]\n",
      "\t[C: 10000, accuracy: 0.8160919540229885]\n",
      "\t[C: 100000, accuracy: 0.8275862068965517]\n",
      "\n",
      "\t> Best C: 100000\n",
      "\t> Best training score: 0.8641618497109826\n",
      "\t> Best test score: 0.8837209302325582\n",
      "\t> Best held score: 0.8348623853211009\n",
      "\t ----------\n",
      "\t> Best C param : 100000\n",
      "\t> Average training score list: [0.8782608695652174, 0.8, 0.846820809248555, 0.8497109826589595, 0.8641618497109826]\n",
      "\t> Average testing score list: [0.7586206896551724, 0.8045977011494253, 0.8255813953488372, 0.813953488372093, 0.8837209302325582]\n",
      "\t> Average held score list: [0.8256880733944955, 0.7889908256880734, 0.8715596330275229, 0.8440366972477065, 0.8348623853211009]\n",
      "\n",
      "\t> Average training score list: 0.8477909022367429\n",
      "\t> Average testing score list: 0.8172948409516172\n",
      "\t> Average held score list: 0.8330275229357798\n",
      "\n",
      "\tConfusion Matrix: Actual (Row) vs Predicted (Column)\n",
      "\t [[25  0  2  0  0  0]\n",
      " [ 0 19  2  1  0  0]\n",
      " [ 4  1 44  0  0  3]\n",
      " [ 0  0  0  2  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [ 0  0  3  1  0  1]]\n",
      "================================================== X_name ==================================================\n",
      "\t ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[C: 0.0001, accuracy: 0.4367816091954023]\n",
      "\t[C: 0.001, accuracy: 0.4367816091954023]\n",
      "\t[C: 0.01, accuracy: 0.5747126436781609]\n",
      "\t[C: 0.1, accuracy: 0.7241379310344828]\n",
      "\t[C: 1, accuracy: 0.7816091954022989]\n",
      "\t[C: 10, accuracy: 0.7701149425287356]\n",
      "\t[C: 100, accuracy: 0.7586206896551724]\n",
      "\t[C: 1000, accuracy: 0.7586206896551724]\n",
      "\t[C: 10000, accuracy: 0.7471264367816092]\n",
      "\t[C: 100000, accuracy: 0.7471264367816092]\n",
      "\n",
      "\t> Best C: 1\n",
      "\t> Best training score: 0.9072463768115943\n",
      "\t> Best test score: 0.8620689655172413\n",
      "\t> Best held score: 0.8073394495412844\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.41379310344827586]\n",
      "\t[C: 0.001, accuracy: 0.41379310344827586]\n",
      "\t[C: 0.01, accuracy: 0.5172413793103449]\n",
      "\t[C: 0.1, accuracy: 0.6896551724137931]\n",
      "\t[C: 1, accuracy: 0.7931034482758621]\n",
      "\t[C: 10, accuracy: 0.8160919540229885]\n",
      "\t[C: 100, accuracy: 0.7701149425287356]\n",
      "\t[C: 1000, accuracy: 0.7701149425287356]\n",
      "\t[C: 10000, accuracy: 0.7586206896551724]\n",
      "\t[C: 100000, accuracy: 0.7471264367816092]\n",
      "\n",
      "\t> Best C: 10\n",
      "\t> Best training score: 0.927536231884058\n",
      "\t> Best test score: 0.7586206896551724\n",
      "\t> Best held score: 0.7981651376146789\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.4482758620689655]\n",
      "\t[C: 0.001, accuracy: 0.4482758620689655]\n",
      "\t[C: 0.01, accuracy: 0.5517241379310345]\n",
      "\t[C: 0.1, accuracy: 0.735632183908046]\n",
      "\t[C: 1, accuracy: 0.8045977011494253]\n",
      "\t[C: 10, accuracy: 0.8160919540229885]\n",
      "\t[C: 100, accuracy: 0.8045977011494253]\n",
      "\t[C: 1000, accuracy: 0.7816091954022989]\n",
      "\t[C: 10000, accuracy: 0.7586206896551724]\n",
      "\t[C: 100000, accuracy: 0.7816091954022989]\n",
      "\n",
      "\t> Best C: 10\n",
      "\t> Best training score: 0.9364161849710982\n",
      "\t> Best test score: 0.7790697674418605\n",
      "\t> Best held score: 0.7798165137614679\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.4827586206896552]\n",
      "\t[C: 0.001, accuracy: 0.4827586206896552]\n",
      "\t[C: 0.01, accuracy: 0.5402298850574713]\n",
      "\t[C: 0.1, accuracy: 0.7126436781609196]\n",
      "\t[C: 1, accuracy: 0.8045977011494253]\n",
      "\t[C: 10, accuracy: 0.8045977011494253]\n",
      "\t[C: 100, accuracy: 0.7816091954022989]\n",
      "\t[C: 1000, accuracy: 0.7701149425287356]\n",
      "\t[C: 10000, accuracy: 0.735632183908046]\n",
      "\t[C: 100000, accuracy: 0.7471264367816092]\n",
      "\n",
      "\t> Best C: 1\n",
      "\t> Best training score: 0.9104046242774566\n",
      "\t> Best test score: 0.8023255813953488\n",
      "\t> Best held score: 0.7798165137614679\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.5287356321839081]\n",
      "\t[C: 0.001, accuracy: 0.5287356321839081]\n",
      "\t[C: 0.01, accuracy: 0.6551724137931034]\n",
      "\t[C: 0.1, accuracy: 0.735632183908046]\n",
      "\t[C: 1, accuracy: 0.7931034482758621]\n",
      "\t[C: 10, accuracy: 0.7931034482758621]\n",
      "\t[C: 100, accuracy: 0.7471264367816092]\n",
      "\t[C: 1000, accuracy: 0.7011494252873564]\n",
      "\t[C: 10000, accuracy: 0.6896551724137931]\n",
      "\t[C: 100000, accuracy: 0.6666666666666666]\n",
      "\n",
      "\t> Best C: 1\n",
      "\t> Best training score: 0.9190751445086706\n",
      "\t> Best test score: 0.7674418604651163\n",
      "\t> Best held score: 0.7889908256880734\n",
      "\t ----------\n",
      "\t> Best C param : 1\n",
      "\t> Average training score list: [0.9072463768115943, 0.927536231884058, 0.9364161849710982, 0.9104046242774566, 0.9190751445086706]\n",
      "\t> Average testing score list: [0.8620689655172413, 0.7586206896551724, 0.7790697674418605, 0.8023255813953488, 0.7674418604651163]\n",
      "\t> Average held score list: [0.8073394495412844, 0.7981651376146789, 0.7798165137614679, 0.7798165137614679, 0.7889908256880734]\n",
      "\n",
      "\t> Average training score list: 0.9201357124905755\n",
      "\t> Average testing score list: 0.7939053728949479\n",
      "\t> Average held score list: 0.7908256880733945\n",
      "\n",
      "\tConfusion Matrix: Actual (Row) vs Predicted (Column)\n",
      "\t [[24  0  3  0  0  0]\n",
      " [ 1 14  6  0  0  1]\n",
      " [ 2  1 46  1  1  1]\n",
      " [ 0  0  0  2  0  0]\n",
      " [ 0  0  1  0  0  0]\n",
      " [ 0  1  4  0  0  0]]\n",
      "================================================== X_stats, X_name ==================================================\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.4367816091954023]\n",
      "\t[C: 0.001, accuracy: 0.45977011494252873]\n",
      "\t[C: 0.01, accuracy: 0.6781609195402298]\n",
      "\t[C: 0.1, accuracy: 0.7586206896551724]\n",
      "\t[C: 1, accuracy: 0.8160919540229885]\n",
      "\t[C: 10, accuracy: 0.7931034482758621]\n",
      "\t[C: 100, accuracy: 0.7701149425287356]\n",
      "\t[C: 1000, accuracy: 0.7816091954022989]\n",
      "\t[C: 10000, accuracy: 0.735632183908046]\n",
      "\t[C: 100000, accuracy: 0.7586206896551724]\n",
      "\n",
      "\t> Best C: 1\n",
      "\t> Best training score: 0.9333333333333333\n",
      "\t> Best test score: 0.8275862068965517\n",
      "\t> Best held score: 0.8623853211009175\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.41379310344827586]\n",
      "\t[C: 0.001, accuracy: 0.4367816091954023]\n",
      "\t[C: 0.01, accuracy: 0.6206896551724138]\n",
      "\t[C: 0.1, accuracy: 0.7701149425287356]\n",
      "\t[C: 1, accuracy: 0.8160919540229885]\n",
      "\t[C: 10, accuracy: 0.7701149425287356]\n",
      "\t[C: 100, accuracy: 0.7586206896551724]\n",
      "\t[C: 1000, accuracy: 0.7701149425287356]\n",
      "\t[C: 10000, accuracy: 0.7701149425287356]\n",
      "\t[C: 100000, accuracy: 0.7471264367816092]\n",
      "\n",
      "\t> Best C: 1\n",
      "\t> Best training score: 0.9246376811594202\n",
      "\t> Best test score: 0.7816091954022989\n",
      "\t> Best held score: 0.8532110091743119\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.4482758620689655]\n",
      "\t[C: 0.001, accuracy: 0.45977011494252873]\n",
      "\t[C: 0.01, accuracy: 0.632183908045977]\n",
      "\t[C: 0.1, accuracy: 0.7816091954022989]\n",
      "\t[C: 1, accuracy: 0.8160919540229885]\n",
      "\t[C: 10, accuracy: 0.7931034482758621]\n",
      "\t[C: 100, accuracy: 0.735632183908046]\n",
      "\t[C: 1000, accuracy: 0.735632183908046]\n",
      "\t[C: 10000, accuracy: 0.7241379310344828]\n",
      "\t[C: 100000, accuracy: 0.7471264367816092]\n",
      "\n",
      "\t> Best C: 1\n",
      "\t> Best training score: 0.930635838150289\n",
      "\t> Best test score: 0.8255813953488372\n",
      "\t> Best held score: 0.8348623853211009\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.4827586206896552]\n",
      "\t[C: 0.001, accuracy: 0.5172413793103449]\n",
      "\t[C: 0.01, accuracy: 0.6436781609195402]\n",
      "\t[C: 0.1, accuracy: 0.7931034482758621]\n",
      "\t[C: 1, accuracy: 0.8505747126436781]\n",
      "\t[C: 10, accuracy: 0.8275862068965517]\n",
      "\t[C: 100, accuracy: 0.8045977011494253]\n",
      "\t[C: 1000, accuracy: 0.7816091954022989]\n",
      "\t[C: 10000, accuracy: 0.7586206896551724]\n",
      "\t[C: 100000, accuracy: 0.735632183908046]\n",
      "\n",
      "\t> Best C: 1\n",
      "\t> Best training score: 0.9335260115606936\n",
      "\t> Best test score: 0.813953488372093\n",
      "\t> Best held score: 0.8348623853211009\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.5287356321839081]\n",
      "\t[C: 0.001, accuracy: 0.5517241379310345]\n",
      "\t[C: 0.01, accuracy: 0.6896551724137931]\n",
      "\t[C: 0.1, accuracy: 0.7816091954022989]\n",
      "\t[C: 1, accuracy: 0.7931034482758621]\n",
      "\t[C: 10, accuracy: 0.7816091954022989]\n",
      "\t[C: 100, accuracy: 0.7931034482758621]\n",
      "\t[C: 1000, accuracy: 0.7701149425287356]\n",
      "\t[C: 10000, accuracy: 0.8045977011494253]\n",
      "\t[C: 100000, accuracy: 0.8045977011494253]\n",
      "\n",
      "\t> Best C: 10000\n",
      "\t> Best training score: 0.9508670520231214\n",
      "\t> Best test score: 0.7674418604651163\n",
      "\t> Best held score: 0.7614678899082569\n",
      "\t ----------\n",
      "\t> Best C param : 1\n",
      "\t> Average training score list: [0.9333333333333333, 0.9246376811594202, 0.930635838150289, 0.9335260115606936, 0.9508670520231214]\n",
      "\t> Average testing score list: [0.8275862068965517, 0.7816091954022989, 0.8255813953488372, 0.813953488372093, 0.7674418604651163]\n",
      "\t> Average held score list: [0.8623853211009175, 0.8532110091743119, 0.8348623853211009, 0.8348623853211009, 0.7614678899082569]\n",
      "\n",
      "\t> Average training score list: 0.9345999832453715\n",
      "\t> Average testing score list: 0.8032344292969794\n",
      "\t> Average held score list: 0.8293577981651377\n",
      "\n",
      "\tConfusion Matrix: Actual (Row) vs Predicted (Column)\n",
      "\t [[26  0  1  0  0  0]\n",
      " [ 1 15  4  1  0  1]\n",
      " [ 3  8 40  0  1  0]\n",
      " [ 0  0  0  2  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [ 0  2  3  0  0  0]]\n",
      "================================================== X_sample1 ==================================================\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.45977011494252873]\n",
      "\t[C: 0.001, accuracy: 0.47126436781609193]\n",
      "\t[C: 0.01, accuracy: 0.5632183908045977]\n",
      "\t[C: 0.1, accuracy: 0.7471264367816092]\n",
      "\t[C: 1, accuracy: 0.7701149425287356]\n",
      "\t[C: 10, accuracy: 0.7701149425287356]\n",
      "\t[C: 100, accuracy: 0.7586206896551724]\n",
      "\t[C: 1000, accuracy: 0.7586206896551724]\n",
      "\t[C: 10000, accuracy: 0.7816091954022989]\n",
      "\t[C: 100000, accuracy: 0.7816091954022989]\n",
      "\n",
      "\t> Best C: 10000\n",
      "\t> Best training score: 0.8869565217391304\n",
      "\t> Best test score: 0.6666666666666666\n",
      "\t> Best held score: 0.8073394495412844\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.4367816091954023]\n",
      "\t[C: 0.001, accuracy: 0.45977011494252873]\n",
      "\t[C: 0.01, accuracy: 0.5287356321839081]\n",
      "\t[C: 0.1, accuracy: 0.6666666666666666]\n",
      "\t[C: 1, accuracy: 0.6781609195402298]\n",
      "\t[C: 10, accuracy: 0.6896551724137931]\n",
      "\t[C: 100, accuracy: 0.6896551724137931]\n",
      "\t[C: 1000, accuracy: 0.6781609195402298]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[C: 10000, accuracy: 0.6551724137931034]\n",
      "\t[C: 100000, accuracy: 0.6781609195402298]\n",
      "\n",
      "\t> Best C: 10\n",
      "\t> Best training score: 0.8782608695652174\n",
      "\t> Best test score: 0.7816091954022989\n",
      "\t> Best held score: 0.8165137614678899\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.45977011494252873]\n",
      "\t[C: 0.001, accuracy: 0.47126436781609193]\n",
      "\t[C: 0.01, accuracy: 0.5057471264367817]\n",
      "\t[C: 0.1, accuracy: 0.6666666666666666]\n",
      "\t[C: 1, accuracy: 0.6896551724137931]\n",
      "\t[C: 10, accuracy: 0.7011494252873564]\n",
      "\t[C: 100, accuracy: 0.7011494252873564]\n",
      "\t[C: 1000, accuracy: 0.7241379310344828]\n",
      "\t[C: 10000, accuracy: 0.7241379310344828]\n",
      "\t[C: 100000, accuracy: 0.7471264367816092]\n",
      "\n",
      "\t> Best C: 100000\n",
      "\t> Best training score: 0.8641618497109826\n",
      "\t> Best test score: 0.7674418604651163\n",
      "\t> Best held score: 0.8256880733944955\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.5287356321839081]\n",
      "\t[C: 0.001, accuracy: 0.5402298850574713]\n",
      "\t[C: 0.01, accuracy: 0.5862068965517241]\n",
      "\t[C: 0.1, accuracy: 0.7241379310344828]\n",
      "\t[C: 1, accuracy: 0.7241379310344828]\n",
      "\t[C: 10, accuracy: 0.7701149425287356]\n",
      "\t[C: 100, accuracy: 0.735632183908046]\n",
      "\t[C: 1000, accuracy: 0.7471264367816092]\n",
      "\t[C: 10000, accuracy: 0.735632183908046]\n",
      "\t[C: 100000, accuracy: 0.7011494252873564]\n",
      "\n",
      "\t> Best C: 10\n",
      "\t> Best training score: 0.8757225433526011\n",
      "\t> Best test score: 0.7093023255813954\n",
      "\t> Best held score: 0.7798165137614679\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.5632183908045977]\n",
      "\t[C: 0.001, accuracy: 0.5862068965517241]\n",
      "\t[C: 0.01, accuracy: 0.6206896551724138]\n",
      "\t[C: 0.1, accuracy: 0.7701149425287356]\n",
      "\t[C: 1, accuracy: 0.7701149425287356]\n",
      "\t[C: 10, accuracy: 0.7931034482758621]\n",
      "\t[C: 100, accuracy: 0.7931034482758621]\n",
      "\t[C: 1000, accuracy: 0.7931034482758621]\n",
      "\t[C: 10000, accuracy: 0.7586206896551724]\n",
      "\t[C: 100000, accuracy: 0.735632183908046]\n",
      "\n",
      "\t> Best C: 10\n",
      "\t> Best training score: 0.884393063583815\n",
      "\t> Best test score: 0.7906976744186046\n",
      "\t> Best held score: 0.8348623853211009\n",
      "\t ----------\n",
      "\t> Best C param : 10\n",
      "\t> Average training score list: [0.8869565217391304, 0.8782608695652174, 0.8641618497109826, 0.8757225433526011, 0.884393063583815]\n",
      "\t> Average testing score list: [0.6666666666666666, 0.7816091954022989, 0.7674418604651163, 0.7093023255813954, 0.7906976744186046]\n",
      "\t> Average held score list: [0.8073394495412844, 0.8165137614678899, 0.8256880733944955, 0.7798165137614679, 0.8348623853211009]\n",
      "\n",
      "\t> Average training score list: 0.8778989695903494\n",
      "\t> Average testing score list: 0.7431435445068164\n",
      "\t> Average held score list: 0.8128440366972477\n",
      "\n",
      "\tConfusion Matrix: Actual (Row) vs Predicted (Column)\n",
      "\t [[24  0  3  0  0  0]\n",
      " [ 0 16  6  0  0  0]\n",
      " [ 2  0 49  0  0  1]\n",
      " [ 0  0  0  2  0  0]\n",
      " [ 0  0  1  0  0  0]\n",
      " [ 0  1  4  0  0  0]]\n",
      "================================================== X_name, X_sample1 ==================================================\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.45977011494252873]\n",
      "\t[C: 0.001, accuracy: 0.47126436781609193]\n",
      "\t[C: 0.01, accuracy: 0.6896551724137931]\n",
      "\t[C: 0.1, accuracy: 0.7701149425287356]\n",
      "\t[C: 1, accuracy: 0.7816091954022989]\n",
      "\t[C: 10, accuracy: 0.7471264367816092]\n",
      "\t[C: 100, accuracy: 0.7241379310344828]\n",
      "\t[C: 1000, accuracy: 0.7241379310344828]\n",
      "\t[C: 10000, accuracy: 0.7241379310344828]\n",
      "\t[C: 100000, accuracy: 0.7586206896551724]\n",
      "\n",
      "\t> Best C: 1\n",
      "\t> Best training score: 0.936231884057971\n",
      "\t> Best test score: 0.7816091954022989\n",
      "\t> Best held score: 0.8532110091743119\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.4367816091954023]\n",
      "\t[C: 0.001, accuracy: 0.45977011494252873]\n",
      "\t[C: 0.01, accuracy: 0.6436781609195402]\n",
      "\t[C: 0.1, accuracy: 0.6781609195402298]\n",
      "\t[C: 1, accuracy: 0.735632183908046]\n",
      "\t[C: 10, accuracy: 0.7471264367816092]\n",
      "\t[C: 100, accuracy: 0.7471264367816092]\n",
      "\t[C: 1000, accuracy: 0.7126436781609196]\n",
      "\t[C: 10000, accuracy: 0.7126436781609196]\n",
      "\t[C: 100000, accuracy: 0.6896551724137931]\n",
      "\n",
      "\t> Best C: 10\n",
      "\t> Best training score: 0.9304347826086956\n",
      "\t> Best test score: 0.8045977011494253\n",
      "\t> Best held score: 0.8165137614678899\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.45977011494252873]\n",
      "\t[C: 0.001, accuracy: 0.47126436781609193]\n",
      "\t[C: 0.01, accuracy: 0.632183908045977]\n",
      "\t[C: 0.1, accuracy: 0.735632183908046]\n",
      "\t[C: 1, accuracy: 0.7816091954022989]\n",
      "\t[C: 10, accuracy: 0.7586206896551724]\n",
      "\t[C: 100, accuracy: 0.7011494252873564]\n",
      "\t[C: 1000, accuracy: 0.6781609195402298]\n",
      "\t[C: 10000, accuracy: 0.6896551724137931]\n",
      "\t[C: 100000, accuracy: 0.7241379310344828]\n",
      "\n",
      "\t> Best C: 1\n",
      "\t> Best training score: 0.930635838150289\n",
      "\t> Best test score: 0.813953488372093\n",
      "\t> Best held score: 0.8348623853211009\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.5287356321839081]\n",
      "\t[C: 0.001, accuracy: 0.5402298850574713]\n",
      "\t[C: 0.01, accuracy: 0.6896551724137931]\n",
      "\t[C: 0.1, accuracy: 0.7816091954022989]\n",
      "\t[C: 1, accuracy: 0.8045977011494253]\n",
      "\t[C: 10, accuracy: 0.8045977011494253]\n",
      "\t[C: 100, accuracy: 0.735632183908046]\n",
      "\t[C: 1000, accuracy: 0.7126436781609196]\n",
      "\t[C: 10000, accuracy: 0.7241379310344828]\n",
      "\t[C: 100000, accuracy: 0.7126436781609196]\n",
      "\n",
      "\t> Best C: 1\n",
      "\t> Best training score: 0.9393063583815029\n",
      "\t> Best test score: 0.8372093023255814\n",
      "\t> Best held score: 0.8256880733944955\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.5632183908045977]\n",
      "\t[C: 0.001, accuracy: 0.5862068965517241]\n",
      "\t[C: 0.01, accuracy: 0.735632183908046]\n",
      "\t[C: 0.1, accuracy: 0.8160919540229885]\n",
      "\t[C: 1, accuracy: 0.8620689655172413]\n",
      "\t[C: 10, accuracy: 0.8505747126436781]\n",
      "\t[C: 100, accuracy: 0.7586206896551724]\n",
      "\t[C: 1000, accuracy: 0.7586206896551724]\n",
      "\t[C: 10000, accuracy: 0.7241379310344828]\n",
      "\t[C: 100000, accuracy: 0.735632183908046]\n",
      "\n",
      "\t> Best C: 1\n",
      "\t> Best training score: 0.953757225433526\n",
      "\t> Best test score: 0.813953488372093\n",
      "\t> Best held score: 0.8256880733944955\n",
      "\t ----------\n",
      "\t> Best C param : 1\n",
      "\t> Average training score list: [0.936231884057971, 0.9304347826086956, 0.930635838150289, 0.9393063583815029, 0.953757225433526]\n",
      "\t> Average testing score list: [0.7816091954022989, 0.8045977011494253, 0.813953488372093, 0.8372093023255814, 0.813953488372093]\n",
      "\t> Average held score list: [0.8532110091743119, 0.8165137614678899, 0.8348623853211009, 0.8256880733944955, 0.8256880733944955]\n",
      "\n",
      "\t> Average training score list: 0.9380732177263968\n",
      "\t> Average testing score list: 0.8102646351242984\n",
      "\t> Average held score list: 0.8311926605504587\n",
      "\n",
      "\tConfusion Matrix: Actual (Row) vs Predicted (Column)\n",
      "\t [[25  0  2  0  0  0]\n",
      " [ 0 15  7  0  0  0]\n",
      " [ 2  1 48  0  0  1]\n",
      " [ 0  0  0  2  0  0]\n",
      " [ 0  0  1  0  0  0]\n",
      " [ 0  1  4  0  0  0]]\n",
      "================================================== X_stats, X_sample1 ==================================================\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.45977011494252873]\n",
      "\t[C: 0.001, accuracy: 0.47126436781609193]\n",
      "\t[C: 0.01, accuracy: 0.6781609195402298]\n",
      "\t[C: 0.1, accuracy: 0.7126436781609196]\n",
      "\t[C: 1, accuracy: 0.735632183908046]\n",
      "\t[C: 10, accuracy: 0.7471264367816092]\n",
      "\t[C: 100, accuracy: 0.735632183908046]\n",
      "\t[C: 1000, accuracy: 0.8045977011494253]\n",
      "\t[C: 10000, accuracy: 0.7931034482758621]\n",
      "\t[C: 100000, accuracy: 0.7816091954022989]\n",
      "\n",
      "\t> Best C: 1000\n",
      "\t> Best training score: 0.9304347826086956\n",
      "\t> Best test score: 0.7586206896551724\n",
      "\t> Best held score: 0.7522935779816514\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.4367816091954023]\n",
      "\t[C: 0.001, accuracy: 0.45977011494252873]\n",
      "\t[C: 0.01, accuracy: 0.6206896551724138]\n",
      "\t[C: 0.1, accuracy: 0.632183908045977]\n",
      "\t[C: 1, accuracy: 0.6436781609195402]\n",
      "\t[C: 10, accuracy: 0.6666666666666666]\n",
      "\t[C: 100, accuracy: 0.6896551724137931]\n",
      "\t[C: 1000, accuracy: 0.6896551724137931]\n",
      "\t[C: 10000, accuracy: 0.7126436781609196]\n",
      "\t[C: 100000, accuracy: 0.7126436781609196]\n",
      "\n",
      "\t> Best C: 10000\n",
      "\t> Best training score: 0.9130434782608695\n",
      "\t> Best test score: 0.7701149425287356\n",
      "\t> Best held score: 0.7339449541284404\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.45977011494252873]\n",
      "\t[C: 0.001, accuracy: 0.47126436781609193]\n",
      "\t[C: 0.01, accuracy: 0.632183908045977]\n",
      "\t[C: 0.1, accuracy: 0.6781609195402298]\n",
      "\t[C: 1, accuracy: 0.6896551724137931]\n",
      "\t[C: 10, accuracy: 0.7011494252873564]\n",
      "\t[C: 100, accuracy: 0.6896551724137931]\n",
      "\t[C: 1000, accuracy: 0.735632183908046]\n",
      "\t[C: 10000, accuracy: 0.7586206896551724]\n",
      "\t[C: 100000, accuracy: 0.7126436781609196]\n",
      "\n",
      "\t> Best C: 10000\n",
      "\t> Best training score: 0.9219653179190751\n",
      "\t> Best test score: 0.7674418604651163\n",
      "\t> Best held score: 0.7889908256880734\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.5287356321839081]\n",
      "\t[C: 0.001, accuracy: 0.5402298850574713]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[C: 0.01, accuracy: 0.6781609195402298]\n",
      "\t[C: 0.1, accuracy: 0.7241379310344828]\n",
      "\t[C: 1, accuracy: 0.7241379310344828]\n",
      "\t[C: 10, accuracy: 0.7126436781609196]\n",
      "\t[C: 100, accuracy: 0.7011494252873564]\n",
      "\t[C: 1000, accuracy: 0.7471264367816092]\n",
      "\t[C: 10000, accuracy: 0.6781609195402298]\n",
      "\t[C: 100000, accuracy: 0.7241379310344828]\n",
      "\n",
      "\t> Best C: 1000\n",
      "\t> Best training score: 0.9104046242774566\n",
      "\t> Best test score: 0.8023255813953488\n",
      "\t> Best held score: 0.7339449541284404\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.5632183908045977]\n",
      "\t[C: 0.001, accuracy: 0.5862068965517241]\n",
      "\t[C: 0.01, accuracy: 0.7011494252873564]\n",
      "\t[C: 0.1, accuracy: 0.7471264367816092]\n",
      "\t[C: 1, accuracy: 0.7471264367816092]\n",
      "\t[C: 10, accuracy: 0.735632183908046]\n",
      "\t[C: 100, accuracy: 0.735632183908046]\n",
      "\t[C: 1000, accuracy: 0.7816091954022989]\n",
      "\t[C: 10000, accuracy: 0.735632183908046]\n",
      "\t[C: 100000, accuracy: 0.7586206896551724]\n",
      "\n",
      "\t> Best C: 1000\n",
      "\t> Best training score: 0.9190751445086706\n",
      "\t> Best test score: 0.813953488372093\n",
      "\t> Best held score: 0.7155963302752294\n",
      "\t ----------\n",
      "\t> Best C param : 1000\n",
      "\t> Average training score list: [0.9304347826086956, 0.9130434782608695, 0.9219653179190751, 0.9104046242774566, 0.9190751445086706]\n",
      "\t> Average testing score list: [0.7586206896551724, 0.7701149425287356, 0.7674418604651163, 0.8023255813953488, 0.813953488372093]\n",
      "\t> Average held score list: [0.7522935779816514, 0.7339449541284404, 0.7889908256880734, 0.7339449541284404, 0.7155963302752294]\n",
      "\n",
      "\t> Average training score list: 0.9189846695149535\n",
      "\t> Average testing score list: 0.7824913124832933\n",
      "\t> Average held score list: 0.744954128440367\n",
      "\n",
      "\tConfusion Matrix: Actual (Row) vs Predicted (Column)\n",
      "\t [[27  0  0  0  0  0]\n",
      " [ 0 16  6  0  0  0]\n",
      " [13  6 33  0  0  0]\n",
      " [ 0  0  0  2  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [ 0  1  2  1  1  0]]\n",
      "================================================== X_stats, X_name, X_sample1 ==================================================\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.45977011494252873]\n",
      "\t[C: 0.001, accuracy: 0.47126436781609193]\n",
      "\t[C: 0.01, accuracy: 0.7011494252873564]\n",
      "\t[C: 0.1, accuracy: 0.7701149425287356]\n",
      "\t[C: 1, accuracy: 0.7816091954022989]\n",
      "\t[C: 10, accuracy: 0.7931034482758621]\n",
      "\t[C: 100, accuracy: 0.8160919540229885]\n",
      "\t[C: 1000, accuracy: 0.8045977011494253]\n",
      "\t[C: 10000, accuracy: 0.8045977011494253]\n",
      "\t[C: 100000, accuracy: 0.8160919540229885]\n",
      "\n",
      "\t> Best C: 100\n",
      "\t> Best training score: 0.9536231884057971\n",
      "\t> Best test score: 0.7586206896551724\n",
      "\t> Best held score: 0.8256880733944955\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.4367816091954023]\n",
      "\t[C: 0.001, accuracy: 0.45977011494252873]\n",
      "\t[C: 0.01, accuracy: 0.6436781609195402]\n",
      "\t[C: 0.1, accuracy: 0.6896551724137931]\n",
      "\t[C: 1, accuracy: 0.7586206896551724]\n",
      "\t[C: 10, accuracy: 0.7586206896551724]\n",
      "\t[C: 100, accuracy: 0.735632183908046]\n",
      "\t[C: 1000, accuracy: 0.7126436781609196]\n",
      "\t[C: 10000, accuracy: 0.5862068965517241]\n",
      "\t[C: 100000, accuracy: 0.5632183908045977]\n",
      "\n",
      "\t> Best C: 1\n",
      "\t> Best training score: 0.9333333333333333\n",
      "\t> Best test score: 0.8275862068965517\n",
      "\t> Best held score: 0.8256880733944955\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.45977011494252873]\n",
      "\t[C: 0.001, accuracy: 0.47126436781609193]\n",
      "\t[C: 0.01, accuracy: 0.6436781609195402]\n",
      "\t[C: 0.1, accuracy: 0.7471264367816092]\n",
      "\t[C: 1, accuracy: 0.7471264367816092]\n",
      "\t[C: 10, accuracy: 0.7126436781609196]\n",
      "\t[C: 100, accuracy: 0.7126436781609196]\n",
      "\t[C: 1000, accuracy: 0.7011494252873564]\n",
      "\t[C: 10000, accuracy: 0.632183908045977]\n",
      "\t[C: 100000, accuracy: 0.632183908045977]\n",
      "\n",
      "\t> Best C: 0.1\n",
      "\t> Best training score: 0.8872832369942196\n",
      "\t> Best test score: 0.7906976744186046\n",
      "\t> Best held score: 0.8165137614678899\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.5287356321839081]\n",
      "\t[C: 0.001, accuracy: 0.5402298850574713]\n",
      "\t[C: 0.01, accuracy: 0.7126436781609196]\n",
      "\t[C: 0.1, accuracy: 0.7471264367816092]\n",
      "\t[C: 1, accuracy: 0.8045977011494253]\n",
      "\t[C: 10, accuracy: 0.7586206896551724]\n",
      "\t[C: 100, accuracy: 0.735632183908046]\n",
      "\t[C: 1000, accuracy: 0.7241379310344828]\n",
      "\t[C: 10000, accuracy: 0.632183908045977]\n",
      "\t[C: 100000, accuracy: 0.632183908045977]\n",
      "\n",
      "\t> Best C: 1\n",
      "\t> Best training score: 0.9393063583815029\n",
      "\t> Best test score: 0.813953488372093\n",
      "\t> Best held score: 0.7981651376146789\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.5632183908045977]\n",
      "\t[C: 0.001, accuracy: 0.5862068965517241]\n",
      "\t[C: 0.01, accuracy: 0.735632183908046]\n",
      "\t[C: 0.1, accuracy: 0.8045977011494253]\n",
      "\t[C: 1, accuracy: 0.8505747126436781]\n",
      "\t[C: 10, accuracy: 0.8160919540229885]\n",
      "\t[C: 100, accuracy: 0.7816091954022989]\n",
      "\t[C: 1000, accuracy: 0.7701149425287356]\n",
      "\t[C: 10000, accuracy: 0.7701149425287356]\n",
      "\t[C: 100000, accuracy: 0.7816091954022989]\n",
      "\n",
      "\t> Best C: 1\n",
      "\t> Best training score: 0.953757225433526\n",
      "\t> Best test score: 0.8023255813953488\n",
      "\t> Best held score: 0.8256880733944955\n",
      "\t ----------\n",
      "\t> Best C param : 1\n",
      "\t> Average training score list: [0.9536231884057971, 0.9333333333333333, 0.8872832369942196, 0.9393063583815029, 0.953757225433526]\n",
      "\t> Average testing score list: [0.7586206896551724, 0.8275862068965517, 0.7906976744186046, 0.813953488372093, 0.8023255813953488]\n",
      "\t> Average held score list: [0.8256880733944955, 0.8256880733944955, 0.8165137614678899, 0.7981651376146789, 0.8256880733944955]\n",
      "\n",
      "\t> Average training score list: 0.9334606685096759\n",
      "\t> Average testing score list: 0.7986367281475542\n",
      "\t> Average held score list: 0.8183486238532109\n",
      "\n",
      "\tConfusion Matrix: Actual (Row) vs Predicted (Column)\n",
      "\t [[24  0  3  0  0  0]\n",
      " [ 0 17  5  0  0  0]\n",
      " [ 0  4 47  0  0  1]\n",
      " [ 0  0  0  2  0  0]\n",
      " [ 0  0  1  0  0  0]\n",
      " [ 0  2  3  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "test_feat_combos(index)\n",
    "acc_df.to_csv('data/model_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
