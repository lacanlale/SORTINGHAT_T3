{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import linear_model\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv\n",
    "dict_label = {\n",
    "    'Datetime':0, \n",
    "    'Sentence':1, \n",
    "    'Custom Object': 2, \n",
    "    'URL': 3, \n",
    "    'Numbers': 4, \n",
    "    'List': 5}\n",
    "# data = pd.read_csv('data/needs_extraction_data/labelled_data.csv')\n",
    "# data = pd.read_csv('data/needs_extraction_data/labelled_added.csv')\n",
    "data = pd.read_csv('labelled_added.csv')\n",
    "data['y_act'] = [dict_label[i] for i in data['y_act']]\n",
    "y = data.loc[:,['y_act']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Data mean: scaled_perc_nans                -2.745801e-16\n",
      "scaled_mean_token_count         -1.117919e-16\n",
      "scaled_std_dev_token_count      -2.236863e-17\n",
      "has_delimiters                   3.105360e-01\n",
      "scaled_mean_stopword_total      -1.918777e-17\n",
      "scaled_mean_whitespace_count    -1.126127e-16\n",
      "scaled_mean_char_count           5.130421e-17\n",
      "scaled_mean_delim_count         -4.186423e-17\n",
      "scaled_stdev_stopword_total      6.741373e-17\n",
      "scaled_stdev_whitespace_count   -2.236863e-17\n",
      "scaled_stdev_char_count         -3.488686e-18\n",
      "scaled_stdev_delim_count         9.516930e-17\n",
      "has_url                          8.687616e-02\n",
      "has_date                         7.560074e-01\n",
      "has_email                        1.000000e+00\n",
      "dtype: float64\n",
      "\n",
      "> Data median: scaled_perc_nans                -0.653046\n",
      "scaled_mean_token_count         -0.144106\n",
      "scaled_std_dev_token_count      -0.171320\n",
      "has_delimiters                   0.000000\n",
      "scaled_mean_stopword_total      -0.178121\n",
      "scaled_mean_whitespace_count    -0.144106\n",
      "scaled_mean_char_count          -0.166657\n",
      "scaled_mean_delim_count         -0.169718\n",
      "scaled_stdev_stopword_total     -0.204091\n",
      "scaled_stdev_whitespace_count   -0.171320\n",
      "scaled_stdev_char_count         -0.188179\n",
      "scaled_stdev_delim_count        -0.220007\n",
      "has_url                          0.000000\n",
      "has_date                         1.000000\n",
      "has_email                        1.000000\n",
      "dtype: float64\n",
      "\n",
      "> Data stdev: scaled_perc_nans                 1.000925\n",
      "scaled_mean_token_count          1.000925\n",
      "scaled_std_dev_token_count       1.000925\n",
      "has_delimiters                   0.463141\n",
      "scaled_mean_stopword_total       1.000925\n",
      "scaled_mean_whitespace_count     1.000925\n",
      "scaled_mean_char_count           1.000925\n",
      "scaled_mean_delim_count          1.000925\n",
      "scaled_stdev_stopword_total      1.000925\n",
      "scaled_stdev_whitespace_count    1.000925\n",
      "scaled_stdev_char_count          1.000925\n",
      "scaled_stdev_delim_count         1.000925\n",
      "has_url                          0.281914\n",
      "has_date                         0.429886\n",
      "has_email                        0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data1 = data[['%_nans', 'mean_word_count',\n",
    "              'std_dev_word_count', 'has_delimiters', 'mean_stopword_total',\n",
    "              'mean_whitespace_count', 'mean_char_count', 'mean_delim_count',\n",
    "              'stdev_stopword_total', 'stdev_whitespace_count', 'stdev_char_count',\n",
    "              'stdev_delim_count', 'has_url', 'has_date', 'has_email']]\n",
    "data1 = data1.fillna(0)\n",
    "\n",
    "data1 = data1.rename(columns={\n",
    "    'mean_word_count': 'scaled_mean_token_count',\n",
    "    'std_dev_word_count': 'scaled_std_dev_token_count',\n",
    "    '%_nans': 'scaled_perc_nans',\n",
    "    'mean_stopword_total': 'scaled_mean_stopword_total',\n",
    "    'mean_whitespace_count': 'scaled_mean_whitespace_count',\n",
    "    'mean_char_count': 'scaled_mean_char_count',\n",
    "    'mean_delim_count': 'scaled_mean_delim_count',\n",
    "    'stdev_stopword_total': 'scaled_stdev_stopword_total',\n",
    "    'stdev_whitespace_count': 'scaled_stdev_whitespace_count',\n",
    "    'stdev_char_count': 'scaled_stdev_char_count',\n",
    "    'stdev_delim_count': 'scaled_stdev_delim_count'\n",
    "})\n",
    "data1.loc[data1['scaled_mean_token_count'] >\n",
    "          10000, 'scaled_mean_token_count'] = 10000\n",
    "data1.loc[data1['scaled_mean_token_count'] < -\n",
    "          10000, 'scaled_mean_token_count'] = -10000\n",
    "\n",
    "data1.loc[data1['scaled_std_dev_token_count'] >\n",
    "          10000, 'scaled_std_dev_token_count'] = 10000\n",
    "data1.loc[data1['scaled_std_dev_token_count'] < -\n",
    "          10000, 'scaled_std_dev_token_count'] = -10000\n",
    "\n",
    "data1.loc[data1['scaled_perc_nans'] > 10000, 'scaled_perc_nans'] = 10000\n",
    "data1.loc[data1['scaled_perc_nans'] < -10000, 'scaled_perc_nans'] = -10000\n",
    "\n",
    "data1.loc[data1['scaled_mean_stopword_total'] >\n",
    "          10000, 'scaled_mean_stopword_total'] = 10000\n",
    "data1.loc[data1['scaled_mean_stopword_total'] < -\n",
    "          10000, 'scaled_mean_stopword_total'] = -10000\n",
    "\n",
    "data1.loc[data1['scaled_mean_whitespace_count'] >\n",
    "          10000, 'scaled_mean_whitespace_count'] = 10000\n",
    "data1.loc[data1['scaled_mean_whitespace_count'] < -\n",
    "          10000, 'scaled_mean_whitespace_count'] = -10000\n",
    "\n",
    "data1.loc[data1['scaled_mean_char_count'] >\n",
    "          10000, 'scaled_mean_char_count'] = 10000\n",
    "data1.loc[data1['scaled_mean_char_count'] < -\n",
    "          10000, 'scaled_mean_char_count'] = -10000\n",
    "\n",
    "data1.loc[data1['scaled_mean_delim_count'] >\n",
    "          10000, 'scaled_mean_delim_count'] = 10000\n",
    "data1.loc[data1['scaled_mean_delim_count'] < -\n",
    "          10000, 'scaled_mean_delim_count'] = -10000\n",
    "\n",
    "data1.loc[data1['scaled_stdev_stopword_total'] >\n",
    "          10000, 'scaled_stdev_stopword_total'] = 10000\n",
    "data1.loc[data1['scaled_stdev_stopword_total'] < -\n",
    "          10000, 'scaled_stdev_stopword_total'] = -10000\n",
    "\n",
    "data1.loc[data1['scaled_stdev_whitespace_count'] >\n",
    "          10000, 'scaled_stdev_whitespace_count'] = 10000\n",
    "data1.loc[data1['scaled_stdev_whitespace_count'] < -\n",
    "          10000, 'scaled_stdev_whitespace_count'] = -10000\n",
    "\n",
    "data1.loc[data1['scaled_stdev_char_count'] >\n",
    "          10000, 'scaled_stdev_char_count'] = 10000\n",
    "data1.loc[data1['scaled_stdev_char_count'] < -\n",
    "          10000, 'scaled_stdev_char_count'] = -10000\n",
    "\n",
    "data1.loc[data1['scaled_stdev_delim_count'] >\n",
    "          10000, 'scaled_stdev_delim_count'] = 10000\n",
    "data1.loc[data1['scaled_stdev_delim_count'] < -\n",
    "          10000, 'scaled_stdev_delim_count'] = -10000\n",
    "\n",
    "column_names_to_normalize = ['scaled_mean_token_count',\n",
    "                             'scaled_std_dev_token_count',\n",
    "                             'scaled_perc_nans',\n",
    "                             'scaled_mean_stopword_total',\n",
    "                             'scaled_mean_whitespace_count',\n",
    "                             'scaled_mean_char_count',\n",
    "                             'scaled_mean_delim_count',\n",
    "                             'scaled_stdev_stopword_total',\n",
    "                             'scaled_stdev_whitespace_count',\n",
    "                             'scaled_stdev_char_count',\n",
    "                             'scaled_stdev_delim_count']\n",
    "x = data1[column_names_to_normalize].values\n",
    "x = np.nan_to_num(x)\n",
    "x_scaled = StandardScaler().fit_transform(x)\n",
    "df_temp = pd.DataFrame(\n",
    "    x_scaled, columns=column_names_to_normalize, index=data1.index)\n",
    "data1[column_names_to_normalize] = df_temp\n",
    "\n",
    "y.y_act = y.y_act.astype(float)\n",
    "\n",
    "print(f\"> Data mean: {data1.mean()}\\n\")\n",
    "print(f\"> Data median: {data1.median()}\\n\")\n",
    "print(f\"> Data stdev: {data1.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===[VECTORIZATION]===\n",
      "> Length of vectorized feature_names: 8528\n",
      "X_train preview:      scaled_perc_nans  scaled_mean_token_count  scaled_std_dev_token_count  \\\n",
      "453         -0.653097                 0.686283                    3.364514   \n",
      "43          -0.653120                 0.162079                   -0.054513   \n",
      "133          1.978459                -0.148544                   -0.167108   \n",
      "205         -0.653120                -0.141062                   -0.175870   \n",
      "282         -0.653120                -0.148960                   -0.175870   \n",
      "\n",
      "     has_delimiters  scaled_mean_stopword_total  scaled_mean_whitespace_count  \\\n",
      "453            True                    0.945220                      0.686283   \n",
      "43             True                    0.126161                      0.162079   \n",
      "133            True                   -0.187400                     -0.148544   \n",
      "205           False                   -0.178121                     -0.141062   \n",
      "282           False                   -0.187845                     -0.148960   \n",
      "\n",
      "     scaled_mean_char_count  scaled_mean_delim_count  \\\n",
      "453                1.142812                 1.401002   \n",
      "43                 0.161438                 0.026371   \n",
      "133               -0.183377                -0.191491   \n",
      "205               -0.160778                -0.062569   \n",
      "282               -0.176629                -0.127171   \n",
      "\n",
      "     scaled_stdev_stopword_total  scaled_stdev_whitespace_count  ...   8518  \\\n",
      "453                     3.066931                       3.364514  ...      0   \n",
      "43                     -0.088312                      -0.054513  ...      0   \n",
      "133                    -0.202753                      -0.167108  ...      0   \n",
      "205                    -0.210742                      -0.175870  ...      0   \n",
      "282                    -0.210742                      -0.175870  ...      0   \n",
      "\n",
      "     8519  8520  8521  8522  8523  8524  8525  8526  8527  \n",
      "453     0     0     0     0     0     0     0     0     0  \n",
      "43      0     0     0     0     0     0     0     0     0  \n",
      "133     0     0     0     0     0     0     0     0     0  \n",
      "205     0     0     0     0     0     0     0     0     0  \n",
      "282     0     0     0     0     0     0     0     0     0  \n",
      "\n",
      "[5 rows x 18530 columns]\n",
      "y_train preview:      y_act\n",
      "453    1.0\n",
      "43     1.0\n",
      "133    2.0\n",
      "205    0.0\n",
      "282    0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"===[VECTORIZATION]===\")\n",
    "arr = data['Attribute_name'].values\n",
    "data = data.fillna(0)\n",
    "arr1 = data['sample_1'].values\n",
    "arr1 = [str(x) for x in arr1]\n",
    "arr2 = data['sample_2'].values\n",
    "arr2 = [str(x) for x in arr2]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(3, 3), analyzer='char')\n",
    "X = vectorizer.fit_transform(arr)\n",
    "X1 = vectorizer.fit_transform(arr1)\n",
    "X2 = vectorizer.fit_transform(arr2)\n",
    "\n",
    "print(f\"> Length of vectorized feature_names: {len(vectorizer.get_feature_names())}\")\n",
    "\n",
    "data1.to_csv('data/preprocessing/before.csv')\n",
    "attr_df = pd.DataFrame(X.toarray())\n",
    "sample1_df = pd.DataFrame(X1.toarray())\n",
    "sample2_df = pd.DataFrame(X2.toarray())\n",
    "\n",
    "data2 = pd.concat([data1, attr_df, sample1_df, sample2_df], axis=1, sort=False)\n",
    "data2.to_csv('data/preprocessing/after.csv')\n",
    "data2.head()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data2, y, test_size=0.2, random_state=100)\n",
    "\n",
    "# X_train_train, X_test_train,y_train_train,y_test_train = train_test_split(X_train,y_train, test_size=0.25)\n",
    "# print(X_train.head())\n",
    "# print(y_train.head())\n",
    "\n",
    "X_train_new = X_train.reset_index(drop=True)\n",
    "y_train_new = y_train.reset_index(drop=True)\n",
    "print(f\"X_train preview: {X_train.head()}\")\n",
    "print(f\"y_train preview: {y_train.head()}\")\n",
    "\n",
    "X_train_new = X_train_new.values\n",
    "y_train_new = y_train_new.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    acc_df = pd.read_csv('data/model_data.csv')\n",
    "    index = len(acc_df)\n",
    "except FileNotFoundError:\n",
    "    acc_df = pd.DataFrame(columns=['Model', 'Params', 'Feats', 'Train', 'Validation', 'Test', 'Precision'])\n",
    "    index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C: 0.0001, accuracy: 0.47126436781609193]\n",
      "[C: 0.001, accuracy: 0.4827586206896552]\n",
      "[C: 0.01, accuracy: 0.6896551724137931]\n",
      "[C: 0.1, accuracy: 0.7701149425287356]\n",
      "[C: 1, accuracy: 0.8045977011494253]\n",
      "[C: 10, accuracy: 0.8160919540229885]\n",
      "[C: 100, accuracy: 0.8160919540229885]\n",
      "[C: 1000, accuracy: 0.8160919540229885]\n",
      "[C: 10000, accuracy: 0.8390804597701149]\n",
      "[C: 100000, accuracy: 0.8390804597701149]\n",
      "\n",
      "> Best C: 10000\n",
      "> Best training score: 0.9594202898550724\n",
      "> Best test score: 0.7701149425287356\n",
      "> Best held score: 0.7981651376146789\n",
      "==========\n",
      "[C: 0.0001, accuracy: 0.4482758620689655]\n",
      "[C: 0.001, accuracy: 0.47126436781609193]\n",
      "[C: 0.01, accuracy: 0.6436781609195402]\n",
      "[C: 0.1, accuracy: 0.7011494252873564]\n",
      "[C: 1, accuracy: 0.7126436781609196]\n",
      "[C: 10, accuracy: 0.7241379310344828]\n",
      "[C: 100, accuracy: 0.7241379310344828]\n",
      "[C: 1000, accuracy: 0.7241379310344828]\n",
      "[C: 10000, accuracy: 0.7241379310344828]\n",
      "[C: 100000, accuracy: 0.7241379310344828]\n",
      "\n",
      "> Best C: 10\n",
      "> Best training score: 0.9304347826086956\n",
      "> Best test score: 0.8160919540229885\n",
      "> Best held score: 0.8532110091743119\n",
      "==========\n",
      "[C: 0.0001, accuracy: 0.45977011494252873]\n",
      "[C: 0.001, accuracy: 0.47126436781609193]\n",
      "[C: 0.01, accuracy: 0.6781609195402298]\n",
      "[C: 0.1, accuracy: 0.7586206896551724]\n",
      "[C: 1, accuracy: 0.7241379310344828]\n",
      "[C: 10, accuracy: 0.6781609195402298]\n",
      "[C: 100, accuracy: 0.6781609195402298]\n",
      "[C: 1000, accuracy: 0.6781609195402298]\n",
      "[C: 10000, accuracy: 0.6781609195402298]\n",
      "[C: 100000, accuracy: 0.6436781609195402]\n",
      "\n",
      "> Best C: 0.1\n",
      "> Best training score: 0.9046242774566474\n",
      "> Best test score: 0.813953488372093\n",
      "> Best held score: 0.8165137614678899\n",
      "==========\n",
      "[C: 0.0001, accuracy: 0.5287356321839081]\n",
      "[C: 0.001, accuracy: 0.5402298850574713]\n",
      "[C: 0.01, accuracy: 0.7586206896551724]\n",
      "[C: 0.1, accuracy: 0.7816091954022989]\n",
      "[C: 1, accuracy: 0.8045977011494253]\n",
      "[C: 10, accuracy: 0.7701149425287356]\n",
      "[C: 100, accuracy: 0.735632183908046]\n",
      "[C: 1000, accuracy: 0.7241379310344828]\n",
      "[C: 10000, accuracy: 0.735632183908046]\n",
      "[C: 100000, accuracy: 0.7241379310344828]\n",
      "\n",
      "> Best C: 1\n",
      "> Best training score: 0.9450867052023122\n",
      "> Best test score: 0.7674418604651163\n",
      "> Best held score: 0.8165137614678899\n",
      "==========\n",
      "[C: 0.0001, accuracy: 0.5632183908045977]\n",
      "[C: 0.001, accuracy: 0.5977011494252874]\n",
      "[C: 0.01, accuracy: 0.7471264367816092]\n",
      "[C: 0.1, accuracy: 0.7931034482758621]\n",
      "[C: 1, accuracy: 0.8045977011494253]\n",
      "[C: 10, accuracy: 0.7586206896551724]\n",
      "[C: 100, accuracy: 0.7471264367816092]\n",
      "[C: 1000, accuracy: 0.7241379310344828]\n",
      "[C: 10000, accuracy: 0.7241379310344828]\n",
      "[C: 100000, accuracy: 0.735632183908046]\n",
      "\n",
      "> Best C: 1\n",
      "> Best training score: 0.9450867052023122\n",
      "> Best test score: 0.813953488372093\n",
      "> Best held score: 0.8073394495412844\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "kf = KFold(n_splits=k)\n",
    "avg_train_acc, avg_test_acc = 0, 0\n",
    "\n",
    "val_arr = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]\n",
    "\n",
    "avgsc_lst, avgsc_train_lst, avgsc_hld_lst = [], [], []\n",
    "avgsc, avgsc_train, avgsc_hld = 0, 0, 0\n",
    "\n",
    "best_param_count = {'cval': {}}\n",
    "for train_index, test_index in kf.split(X_train_new):\n",
    "    X_train_cur, X_test_cur = X_train_new[train_index], X_train_new[test_index]\n",
    "    y_train_cur, y_test_cur = y_train_new[train_index], y_train_new[test_index]\n",
    "    X_train_train, X_val, y_train_train, y_val = train_test_split(\n",
    "        X_train_cur, y_train_cur, test_size=0.25, random_state=100)\n",
    "\n",
    "    bestPerformingModel = LogisticRegression(\n",
    "        penalty='l2', multi_class='multinomial', solver='lbfgs', C=1)\n",
    "    bestscore = 0\n",
    "    print('='*10)\n",
    "    for val in val_arr:\n",
    "        clf = LogisticRegression(\n",
    "            penalty='l2', multi_class='multinomial', solver='lbfgs', C=val)\n",
    "        clf.fit(X_train_train, y_train_train)\n",
    "        sc = clf.score(X_val, y_val)\n",
    "        print(f\"[C: {val}, accuracy: {sc}]\")\n",
    "        if bestscore < sc:\n",
    "            bestcval = val\n",
    "            bestscore = sc\n",
    "            bestPerformingModel = clf\n",
    "    \n",
    "    if str(bestcval) in best_param_count['cval']:\n",
    "        best_param_count['cval'][str(bestcval)] += 1\n",
    "    else:\n",
    "        best_param_count['cval'][str(bestcval)] = 1\n",
    "        \n",
    "    bscr_train = bestPerformingModel.score(X_train_cur, y_train_cur)\n",
    "    bscr = bestPerformingModel.score(X_test_cur, y_test_cur)\n",
    "    bscr_hld = bestPerformingModel.score(X_test, y_test)\n",
    "\n",
    "    avgsc_train_lst.append(bscr_train)\n",
    "    avgsc_lst.append(bscr)\n",
    "    avgsc_hld_lst.append(bscr_hld)\n",
    "\n",
    "    avgsc_train = avgsc_train + bscr_train\n",
    "    avgsc = avgsc + bscr\n",
    "    avgsc_hld = avgsc_hld + bscr_hld\n",
    "    print()\n",
    "    print(f\"> Best C: {bestcval}\")\n",
    "    print(f\"> Best training score: {bscr_train}\")\n",
    "    print(f\"> Best test score: {bscr}\")\n",
    "    print(f\"> Best held score: {bscr_hld}\")\n",
    "print('='*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = bestPerformingModel.predict(X_test)\n",
    "prec = metrics.precision_score(y_test, y_pred, average=None)\n",
    "cat_prec = {\n",
    "    'Datetime': prec[0],\n",
    "    'Sentence': prec[1],\n",
    "    'Custom Object': prec[2],\n",
    "    'URL': prec[3],\n",
    "    'Numbers': prec[4],\n",
    "    'List': prec[5],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Best C param : 1\n",
      "> Average training score list: [0.9594202898550724, 0.9304347826086956, 0.9046242774566474, 0.9450867052023122, 0.9450867052023122]\n",
      "> Average testing score list: [0.7701149425287356, 0.8160919540229885, 0.813953488372093, 0.7674418604651163, 0.813953488372093]\n",
      "> Average held score list: [0.7981651376146789, 0.8532110091743119, 0.8165137614678899, 0.8165137614678899, 0.8073394495412844]\n",
      "\n",
      "> Average training score list: 0.936930552065008\n",
      "> Average testing score list: 0.7963111467522054\n",
      "> Average held score list: 0.8183486238532112\n",
      "\n",
      "Confusion Matrix: Actual (Row) vs Predicted (Column)\n",
      "[[25  0  2  0  0  0]\n",
      " [ 0 13  9  0  0  0]\n",
      " [ 0  3 48  0  0  1]\n",
      " [ 0  0  0  2  0  0]\n",
      " [ 0  0  1  0  0  0]\n",
      " [ 0  1  3  1  0  0]]\n"
     ]
    }
   ],
   "source": [
    "bestcval = max(best_param_count['cval'], key=lambda i: best_param_count['cval'][i])\n",
    "bestparams = {'C': bestcval}\n",
    "print(f\"> Best C param : {bestcval}\")\n",
    "print(f\"> Average training score list: {avgsc_train_lst}\")\n",
    "print(f\"> Average testing score list: {avgsc_lst}\")\n",
    "print(f\"> Average held score list: {avgsc_hld_lst}\")\n",
    "print()\n",
    "avgsc_train = avgsc_train/k\n",
    "avgsc = avgsc/k\n",
    "avgsc_hld = avgsc_hld/k\n",
    "print(f\"> Average training score list: {avgsc_train}\")\n",
    "print(f\"> Average testing score list: {avgsc}\")\n",
    "print(f\"> Average held score list: {avgsc_hld}\")\n",
    "acc_df.loc[index] = ['logistic_regression', str(bestparams),\"X_stats, X_name, X_sample1, X_sample2\", avgsc_train, avgsc, avgsc_hld, str(cat_prec)]\n",
    "index += 1\n",
    "print()\n",
    "\n",
    "y_pred = bestPerformingModel.predict(X_test)\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix: Actual (Row) vs Predicted (Column)')\n",
    "print(cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'data/pretrained/lr_finalized_model.pickle'\n",
    "pickle.dump(bestPerformingModel, open(filename, 'wb+'))\n",
    "\n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb+'))\n",
    "result = loaded_model.score(X_test, y_test)\n",
    "y_prob = bestPerformingModel.predict_proba(X_test)\n",
    "\n",
    "df = pd.DataFrame.from_records(y_prob)\n",
    "df.to_csv('data/model_predictions/lr_predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature combination testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_feat_combos(index):\n",
    "    combos = {\n",
    "        \"X_stats\": data1,\n",
    "        \"X_name\": attr_df,\n",
    "        \"X_stats, X_name\": pd.concat([data1, attr_df], axis=1, sort=False),\n",
    "        \"X_sample1\":  pd.concat([sample1_df], axis=1, sort=False),\n",
    "        \"X_name, X_sample1\":  pd.concat([attr_df, sample1_df], axis=1, sort=False),\n",
    "        \"X_stats, X_sample1\":  pd.concat([data1, sample1_df], axis=1, sort=False),\n",
    "        \"X_stats, X_name, X_sample1\":  pd.concat([data1, attr_df, sample1_df], axis=1, sort=False)\n",
    "    }\n",
    "    \n",
    "\n",
    "    for combo in combos:\n",
    "        print(\"=\"*50, combo, \"=\"*50)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            combos[combo], y, test_size=0.2, random_state=100)\n",
    "\n",
    "        X_train_new = X_train.reset_index(drop=True)\n",
    "        y_train_new = y_train.reset_index(drop=True)\n",
    "        X_train_new = X_train_new.values\n",
    "        y_train_new = y_train_new.values\n",
    "        best_param_count = {'cval': {}}\n",
    "        k = 5\n",
    "        kf = KFold(n_splits=k)\n",
    "        avg_train_acc, avg_test_acc = 0, 0\n",
    "\n",
    "        val_arr = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]\n",
    "\n",
    "        avgsc_lst, avgsc_train_lst, avgsc_hld_lst = [], [], []\n",
    "        avgsc, avgsc_train, avgsc_hld = 0, 0, 0\n",
    "\n",
    "        best_param_count = {'cval': {}}\n",
    "        for train_index, test_index in kf.split(X_train_new):\n",
    "            X_train_cur, X_test_cur = X_train_new[train_index], X_train_new[test_index]\n",
    "            y_train_cur, y_test_cur = y_train_new[train_index], y_train_new[test_index]\n",
    "            X_train_train, X_val, y_train_train, y_val = train_test_split(\n",
    "                X_train_cur, y_train_cur, test_size=0.25, random_state=100)\n",
    "\n",
    "            bestPerformingModel = LogisticRegression(\n",
    "                penalty='l2', multi_class='multinomial', solver='lbfgs', C=1)\n",
    "            bestscore = 0\n",
    "            print('\\t', '-'*10)\n",
    "            for val in val_arr:\n",
    "                clf = LogisticRegression(\n",
    "                    penalty='l2', multi_class='multinomial', solver='lbfgs', C=val)\n",
    "                clf.fit(X_train_train, y_train_train)\n",
    "                sc = clf.score(X_val, y_val)\n",
    "                print(f\"\\t[C: {val}, accuracy: {sc}]\")\n",
    "                if bestscore < sc:\n",
    "                    bestcval = val\n",
    "                    bestscore = sc\n",
    "                    bestPerformingModel = clf\n",
    "\n",
    "            if str(bestcval) in best_param_count['cval']:\n",
    "                best_param_count['cval'][str(bestcval)] += 1\n",
    "            else:\n",
    "                best_param_count['cval'][str(bestcval)] = 1\n",
    "            bscr_train = bestPerformingModel.score(X_train_cur, y_train_cur)\n",
    "            bscr = bestPerformingModel.score(X_test_cur, y_test_cur)\n",
    "            bscr_hld = bestPerformingModel.score(X_test, y_test)\n",
    "\n",
    "            avgsc_train_lst.append(bscr_train)\n",
    "            avgsc_lst.append(bscr)\n",
    "            avgsc_hld_lst.append(bscr_hld)\n",
    "\n",
    "            avgsc_train = avgsc_train + bscr_train\n",
    "            avgsc = avgsc + bscr\n",
    "            avgsc_hld = avgsc_hld + bscr_hld\n",
    "            print()\n",
    "            print(f\"\\t> Best C: {bestcval}\")\n",
    "            print(f\"\\t> Best training score: {bscr_train}\")\n",
    "            print(f\"\\t> Best test score: {bscr}\")\n",
    "            print(f\"\\t> Best held score: {bscr_hld}\")\n",
    "        print('\\t', '-'*10)\n",
    "        \n",
    "        y_pred = bestPerformingModel.predict(X_test)\n",
    "        prec = metrics.precision_score(y_test, y_pred, average=None)\n",
    "        cat_prec = {\n",
    "            'Datetime': prec[0],\n",
    "            'Sentence': prec[1],\n",
    "            'Custom Object': prec[2],\n",
    "            'URL': prec[3],\n",
    "            'Numbers': prec[4],\n",
    "            'List': prec[5],\n",
    "        }    \n",
    "        bestcval = max(best_param_count['cval'], key=lambda i: best_param_count['cval'][i])\n",
    "        bestparams = {'C': bestcval}\n",
    "        print(f\"\\t> Best C param : {bestcval}\")\n",
    "        print(f\"\\t> Average training score list: {avgsc_train_lst}\")\n",
    "        print(f\"\\t> Average testing score list: {avgsc_lst}\")\n",
    "        print(f\"\\t> Average held score list: {avgsc_hld_lst}\")\n",
    "        print()\n",
    "        avgsc_train = avgsc_train/k\n",
    "        avgsc = avgsc/k\n",
    "        avgsc_hld = avgsc_hld/k\n",
    "        print(f\"\\t> Average training score list: {avgsc_train}\")\n",
    "        print(f\"\\t> Average testing score list: {avgsc}\")\n",
    "        print(f\"\\t> Average held score list: {avgsc_hld}\")\n",
    "        acc_df.loc[index] = ['logistic_regression', str(bestparams), combo, avgsc_train, avgsc, avgsc_hld, str(cat_prec)]\n",
    "        index += 1\n",
    "        print()\n",
    "\n",
    "        y_pred = bestPerformingModel.predict(X_test)\n",
    "        cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "        print('\\tConfusion Matrix: Actual (Row) vs Predicted (Column)')\n",
    "        print('\\t', cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== X_stats ==================================================\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.4367816091954023]\n",
      "\t[C: 0.001, accuracy: 0.45977011494252873]\n",
      "\t[C: 0.01, accuracy: 0.47126436781609193]\n",
      "\t[C: 0.1, accuracy: 0.5977011494252874]\n",
      "\t[C: 1, accuracy: 0.6206896551724138]\n",
      "\t[C: 10, accuracy: 0.6781609195402298]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[C: 100, accuracy: 0.7471264367816092]\n",
      "\t[C: 1000, accuracy: 0.8390804597701149]\n",
      "\t[C: 10000, accuracy: 0.8160919540229885]\n",
      "\t[C: 100000, accuracy: 0.8390804597701149]\n",
      "\n",
      "\t> Best C: 1000\n",
      "\t> Best training score: 0.863768115942029\n",
      "\t> Best test score: 0.7701149425287356\n",
      "\t> Best held score: 0.8440366972477065\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.41379310344827586]\n",
      "\t[C: 0.001, accuracy: 0.4367816091954023]\n",
      "\t[C: 0.01, accuracy: 0.4482758620689655]\n",
      "\t[C: 0.1, accuracy: 0.5517241379310345]\n",
      "\t[C: 1, accuracy: 0.5632183908045977]\n",
      "\t[C: 10, accuracy: 0.6091954022988506]\n",
      "\t[C: 100, accuracy: 0.632183908045977]\n",
      "\t[C: 1000, accuracy: 0.6781609195402298]\n",
      "\t[C: 10000, accuracy: 0.7126436781609196]\n",
      "\t[C: 100000, accuracy: 0.7126436781609196]\n",
      "\n",
      "\t> Best C: 10000\n",
      "\t> Best training score: 0.7913043478260869\n",
      "\t> Best test score: 0.7816091954022989\n",
      "\t> Best held score: 0.7614678899082569\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.4482758620689655]\n",
      "\t[C: 0.001, accuracy: 0.45977011494252873]\n",
      "\t[C: 0.01, accuracy: 0.45977011494252873]\n",
      "\t[C: 0.1, accuracy: 0.5747126436781609]\n",
      "\t[C: 1, accuracy: 0.5862068965517241]\n",
      "\t[C: 10, accuracy: 0.6551724137931034]\n",
      "\t[C: 100, accuracy: 0.7241379310344828]\n",
      "\t[C: 1000, accuracy: 0.7931034482758621]\n",
      "\t[C: 10000, accuracy: 0.8160919540229885]\n",
      "\t[C: 100000, accuracy: 0.7931034482758621]\n",
      "\n",
      "\t> Best C: 10000\n",
      "\t> Best training score: 0.8497109826589595\n",
      "\t> Best test score: 0.7906976744186046\n",
      "\t> Best held score: 0.8532110091743119\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.4827586206896552]\n",
      "\t[C: 0.001, accuracy: 0.5172413793103449]\n",
      "\t[C: 0.01, accuracy: 0.5057471264367817]\n",
      "\t[C: 0.1, accuracy: 0.6551724137931034]\n",
      "\t[C: 1, accuracy: 0.6551724137931034]\n",
      "\t[C: 10, accuracy: 0.7011494252873564]\n",
      "\t[C: 100, accuracy: 0.7586206896551724]\n",
      "\t[C: 1000, accuracy: 0.8275862068965517]\n",
      "\t[C: 10000, accuracy: 0.7931034482758621]\n",
      "\t[C: 100000, accuracy: 0.8045977011494253]\n",
      "\n",
      "\t> Best C: 1000\n",
      "\t> Best training score: 0.8497109826589595\n",
      "\t> Best test score: 0.7906976744186046\n",
      "\t> Best held score: 0.8440366972477065\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.5287356321839081]\n",
      "\t[C: 0.001, accuracy: 0.5517241379310345]\n",
      "\t[C: 0.01, accuracy: 0.5402298850574713]\n",
      "\t[C: 0.1, accuracy: 0.6091954022988506]\n",
      "\t[C: 1, accuracy: 0.6551724137931034]\n",
      "\t[C: 10, accuracy: 0.6666666666666666]\n",
      "\t[C: 100, accuracy: 0.7126436781609196]\n",
      "\t[C: 1000, accuracy: 0.8160919540229885]\n",
      "\t[C: 10000, accuracy: 0.7701149425287356]\n",
      "\t[C: 100000, accuracy: 0.7701149425287356]\n",
      "\n",
      "\t> Best C: 1000\n",
      "\t> Best training score: 0.838150289017341\n",
      "\t> Best test score: 0.8604651162790697\n",
      "\t> Best held score: 0.8532110091743119\n",
      "\t ----------\n",
      "\t> Best C param : 1000\n",
      "\t> Average training score list: [0.863768115942029, 0.7913043478260869, 0.8497109826589595, 0.8497109826589595, 0.838150289017341]\n",
      "\t> Average testing score list: [0.7701149425287356, 0.7816091954022989, 0.7906976744186046, 0.7906976744186046, 0.8604651162790697]\n",
      "\t> Average held score list: [0.8440366972477065, 0.7614678899082569, 0.8532110091743119, 0.8440366972477065, 0.8532110091743119]\n",
      "\n",
      "\t> Average training score list: 0.8385289436206753\n",
      "\t> Average testing score list: 0.7987169206094628\n",
      "\t> Average held score list: 0.8311926605504587\n",
      "\n",
      "\tConfusion Matrix: Actual (Row) vs Predicted (Column)\n",
      "\t [[25  0  2  0  0  0]\n",
      " [ 0 20  2  0  0  0]\n",
      " [ 4  0 45  0  0  3]\n",
      " [ 0  0  0  2  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [ 0  0  3  1  0  1]]\n",
      "================================================== X_name ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.4367816091954023]\n",
      "\t[C: 0.001, accuracy: 0.4367816091954023]\n",
      "\t[C: 0.01, accuracy: 0.5747126436781609]\n",
      "\t[C: 0.1, accuracy: 0.7241379310344828]\n",
      "\t[C: 1, accuracy: 0.7816091954022989]\n",
      "\t[C: 10, accuracy: 0.7701149425287356]\n",
      "\t[C: 100, accuracy: 0.7586206896551724]\n",
      "\t[C: 1000, accuracy: 0.7586206896551724]\n",
      "\t[C: 10000, accuracy: 0.7471264367816092]\n",
      "\t[C: 100000, accuracy: 0.7471264367816092]\n",
      "\n",
      "\t> Best C: 1\n",
      "\t> Best training score: 0.9072463768115943\n",
      "\t> Best test score: 0.8620689655172413\n",
      "\t> Best held score: 0.8073394495412844\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.41379310344827586]\n",
      "\t[C: 0.001, accuracy: 0.41379310344827586]\n",
      "\t[C: 0.01, accuracy: 0.5172413793103449]\n",
      "\t[C: 0.1, accuracy: 0.6896551724137931]\n",
      "\t[C: 1, accuracy: 0.7931034482758621]\n",
      "\t[C: 10, accuracy: 0.8160919540229885]\n",
      "\t[C: 100, accuracy: 0.7701149425287356]\n",
      "\t[C: 1000, accuracy: 0.7701149425287356]\n",
      "\t[C: 10000, accuracy: 0.7586206896551724]\n",
      "\t[C: 100000, accuracy: 0.7471264367816092]\n",
      "\n",
      "\t> Best C: 10\n",
      "\t> Best training score: 0.927536231884058\n",
      "\t> Best test score: 0.7586206896551724\n",
      "\t> Best held score: 0.7981651376146789\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.4482758620689655]\n",
      "\t[C: 0.001, accuracy: 0.4482758620689655]\n",
      "\t[C: 0.01, accuracy: 0.5517241379310345]\n",
      "\t[C: 0.1, accuracy: 0.735632183908046]\n",
      "\t[C: 1, accuracy: 0.8045977011494253]\n",
      "\t[C: 10, accuracy: 0.8160919540229885]\n",
      "\t[C: 100, accuracy: 0.8045977011494253]\n",
      "\t[C: 1000, accuracy: 0.7816091954022989]\n",
      "\t[C: 10000, accuracy: 0.7586206896551724]\n",
      "\t[C: 100000, accuracy: 0.7816091954022989]\n",
      "\n",
      "\t> Best C: 10\n",
      "\t> Best training score: 0.9364161849710982\n",
      "\t> Best test score: 0.7790697674418605\n",
      "\t> Best held score: 0.7798165137614679\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.4827586206896552]\n",
      "\t[C: 0.001, accuracy: 0.4827586206896552]\n",
      "\t[C: 0.01, accuracy: 0.5402298850574713]\n",
      "\t[C: 0.1, accuracy: 0.7126436781609196]\n",
      "\t[C: 1, accuracy: 0.8045977011494253]\n",
      "\t[C: 10, accuracy: 0.8045977011494253]\n",
      "\t[C: 100, accuracy: 0.7816091954022989]\n",
      "\t[C: 1000, accuracy: 0.7701149425287356]\n",
      "\t[C: 10000, accuracy: 0.735632183908046]\n",
      "\t[C: 100000, accuracy: 0.7471264367816092]\n",
      "\n",
      "\t> Best C: 1\n",
      "\t> Best training score: 0.9104046242774566\n",
      "\t> Best test score: 0.8023255813953488\n",
      "\t> Best held score: 0.7798165137614679\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.5287356321839081]\n",
      "\t[C: 0.001, accuracy: 0.5287356321839081]\n",
      "\t[C: 0.01, accuracy: 0.6551724137931034]\n",
      "\t[C: 0.1, accuracy: 0.735632183908046]\n",
      "\t[C: 1, accuracy: 0.7931034482758621]\n",
      "\t[C: 10, accuracy: 0.7931034482758621]\n",
      "\t[C: 100, accuracy: 0.7471264367816092]\n",
      "\t[C: 1000, accuracy: 0.7011494252873564]\n",
      "\t[C: 10000, accuracy: 0.6896551724137931]\n",
      "\t[C: 100000, accuracy: 0.6666666666666666]\n",
      "\n",
      "\t> Best C: 1\n",
      "\t> Best training score: 0.9190751445086706\n",
      "\t> Best test score: 0.7674418604651163\n",
      "\t> Best held score: 0.7889908256880734\n",
      "\t ----------\n",
      "\t> Best C param : 1\n",
      "\t> Average training score list: [0.9072463768115943, 0.927536231884058, 0.9364161849710982, 0.9104046242774566, 0.9190751445086706]\n",
      "\t> Average testing score list: [0.8620689655172413, 0.7586206896551724, 0.7790697674418605, 0.8023255813953488, 0.7674418604651163]\n",
      "\t> Average held score list: [0.8073394495412844, 0.7981651376146789, 0.7798165137614679, 0.7798165137614679, 0.7889908256880734]\n",
      "\n",
      "\t> Average training score list: 0.9201357124905755\n",
      "\t> Average testing score list: 0.7939053728949479\n",
      "\t> Average held score list: 0.7908256880733945\n",
      "\n",
      "\tConfusion Matrix: Actual (Row) vs Predicted (Column)\n",
      "\t [[24  0  3  0  0  0]\n",
      " [ 1 14  6  0  0  1]\n",
      " [ 2  1 46  1  1  1]\n",
      " [ 0  0  0  2  0  0]\n",
      " [ 0  0  1  0  0  0]\n",
      " [ 0  1  4  0  0  0]]\n",
      "================================================== X_stats, X_name ==================================================\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.4367816091954023]\n",
      "\t[C: 0.001, accuracy: 0.45977011494252873]\n",
      "\t[C: 0.01, accuracy: 0.6781609195402298]\n",
      "\t[C: 0.1, accuracy: 0.7586206896551724]\n",
      "\t[C: 1, accuracy: 0.8160919540229885]\n",
      "\t[C: 10, accuracy: 0.7931034482758621]\n",
      "\t[C: 100, accuracy: 0.7701149425287356]\n",
      "\t[C: 1000, accuracy: 0.7816091954022989]\n",
      "\t[C: 10000, accuracy: 0.735632183908046]\n",
      "\t[C: 100000, accuracy: 0.7816091954022989]\n",
      "\n",
      "\t> Best C: 1\n",
      "\t> Best training score: 0.9333333333333333\n",
      "\t> Best test score: 0.8275862068965517\n",
      "\t> Best held score: 0.8623853211009175\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.41379310344827586]\n",
      "\t[C: 0.001, accuracy: 0.4367816091954023]\n",
      "\t[C: 0.01, accuracy: 0.6206896551724138]\n",
      "\t[C: 0.1, accuracy: 0.7701149425287356]\n",
      "\t[C: 1, accuracy: 0.8160919540229885]\n",
      "\t[C: 10, accuracy: 0.7701149425287356]\n",
      "\t[C: 100, accuracy: 0.7586206896551724]\n",
      "\t[C: 1000, accuracy: 0.7701149425287356]\n",
      "\t[C: 10000, accuracy: 0.7816091954022989]\n",
      "\t[C: 100000, accuracy: 0.7701149425287356]\n",
      "\n",
      "\t> Best C: 1\n",
      "\t> Best training score: 0.9246376811594202\n",
      "\t> Best test score: 0.7816091954022989\n",
      "\t> Best held score: 0.8532110091743119\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.4482758620689655]\n",
      "\t[C: 0.001, accuracy: 0.45977011494252873]\n",
      "\t[C: 0.01, accuracy: 0.632183908045977]\n",
      "\t[C: 0.1, accuracy: 0.7816091954022989]\n",
      "\t[C: 1, accuracy: 0.8160919540229885]\n",
      "\t[C: 10, accuracy: 0.7931034482758621]\n",
      "\t[C: 100, accuracy: 0.735632183908046]\n",
      "\t[C: 1000, accuracy: 0.735632183908046]\n",
      "\t[C: 10000, accuracy: 0.7126436781609196]\n",
      "\t[C: 100000, accuracy: 0.7241379310344828]\n",
      "\n",
      "\t> Best C: 1\n",
      "\t> Best training score: 0.930635838150289\n",
      "\t> Best test score: 0.8255813953488372\n",
      "\t> Best held score: 0.8348623853211009\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.4827586206896552]\n",
      "\t[C: 0.001, accuracy: 0.5172413793103449]\n",
      "\t[C: 0.01, accuracy: 0.6436781609195402]\n",
      "\t[C: 0.1, accuracy: 0.7931034482758621]\n",
      "\t[C: 1, accuracy: 0.8505747126436781]\n",
      "\t[C: 10, accuracy: 0.8275862068965517]\n",
      "\t[C: 100, accuracy: 0.8045977011494253]\n",
      "\t[C: 1000, accuracy: 0.7816091954022989]\n",
      "\t[C: 10000, accuracy: 0.8045977011494253]\n",
      "\t[C: 100000, accuracy: 0.735632183908046]\n",
      "\n",
      "\t> Best C: 1\n",
      "\t> Best training score: 0.9335260115606936\n",
      "\t> Best test score: 0.813953488372093\n",
      "\t> Best held score: 0.8348623853211009\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.5287356321839081]\n",
      "\t[C: 0.001, accuracy: 0.5517241379310345]\n",
      "\t[C: 0.01, accuracy: 0.6896551724137931]\n",
      "\t[C: 0.1, accuracy: 0.7816091954022989]\n",
      "\t[C: 1, accuracy: 0.7931034482758621]\n",
      "\t[C: 10, accuracy: 0.7816091954022989]\n",
      "\t[C: 100, accuracy: 0.7931034482758621]\n",
      "\t[C: 1000, accuracy: 0.7701149425287356]\n",
      "\t[C: 10000, accuracy: 0.7816091954022989]\n",
      "\t[C: 100000, accuracy: 0.8045977011494253]\n",
      "\n",
      "\t> Best C: 100000\n",
      "\t> Best training score: 0.9508670520231214\n",
      "\t> Best test score: 0.7441860465116279\n",
      "\t> Best held score: 0.7614678899082569\n",
      "\t ----------\n",
      "\t> Best C param : 1\n",
      "\t> Average training score list: [0.9333333333333333, 0.9246376811594202, 0.930635838150289, 0.9335260115606936, 0.9508670520231214]\n",
      "\t> Average testing score list: [0.8275862068965517, 0.7816091954022989, 0.8255813953488372, 0.813953488372093, 0.7441860465116279]\n",
      "\t> Average held score list: [0.8623853211009175, 0.8532110091743119, 0.8348623853211009, 0.8348623853211009, 0.7614678899082569]\n",
      "\n",
      "\t> Average training score list: 0.9345999832453715\n",
      "\t> Average testing score list: 0.7985832665062818\n",
      "\t> Average held score list: 0.8293577981651377\n",
      "\n",
      "\tConfusion Matrix: Actual (Row) vs Predicted (Column)\n",
      "\t [[25  0  2  0  0  0]\n",
      " [ 1 14  6  0  0  1]\n",
      " [ 3  6 42  0  1  0]\n",
      " [ 0  0  0  2  0  0]\n",
      " [ 0  0  1  0  0  0]\n",
      " [ 0  1  3  0  1  0]]\n",
      "================================================== X_sample1 ==================================================\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.45977011494252873]\n",
      "\t[C: 0.001, accuracy: 0.47126436781609193]\n",
      "\t[C: 0.01, accuracy: 0.5632183908045977]\n",
      "\t[C: 0.1, accuracy: 0.7471264367816092]\n",
      "\t[C: 1, accuracy: 0.7701149425287356]\n",
      "\t[C: 10, accuracy: 0.7701149425287356]\n",
      "\t[C: 100, accuracy: 0.7586206896551724]\n",
      "\t[C: 1000, accuracy: 0.7586206896551724]\n",
      "\t[C: 10000, accuracy: 0.7816091954022989]\n",
      "\t[C: 100000, accuracy: 0.7816091954022989]\n",
      "\n",
      "\t> Best C: 10000\n",
      "\t> Best training score: 0.8869565217391304\n",
      "\t> Best test score: 0.6666666666666666\n",
      "\t> Best held score: 0.8073394495412844\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.4367816091954023]\n",
      "\t[C: 0.001, accuracy: 0.45977011494252873]\n",
      "\t[C: 0.01, accuracy: 0.5287356321839081]\n",
      "\t[C: 0.1, accuracy: 0.6666666666666666]\n",
      "\t[C: 1, accuracy: 0.6781609195402298]\n",
      "\t[C: 10, accuracy: 0.6896551724137931]\n",
      "\t[C: 100, accuracy: 0.6896551724137931]\n",
      "\t[C: 1000, accuracy: 0.6781609195402298]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[C: 10000, accuracy: 0.6551724137931034]\n",
      "\t[C: 100000, accuracy: 0.6781609195402298]\n",
      "\n",
      "\t> Best C: 10\n",
      "\t> Best training score: 0.8782608695652174\n",
      "\t> Best test score: 0.7816091954022989\n",
      "\t> Best held score: 0.8165137614678899\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.45977011494252873]\n",
      "\t[C: 0.001, accuracy: 0.47126436781609193]\n",
      "\t[C: 0.01, accuracy: 0.5057471264367817]\n",
      "\t[C: 0.1, accuracy: 0.6666666666666666]\n",
      "\t[C: 1, accuracy: 0.6896551724137931]\n",
      "\t[C: 10, accuracy: 0.7011494252873564]\n",
      "\t[C: 100, accuracy: 0.7011494252873564]\n",
      "\t[C: 1000, accuracy: 0.7241379310344828]\n",
      "\t[C: 10000, accuracy: 0.7241379310344828]\n",
      "\t[C: 100000, accuracy: 0.7471264367816092]\n",
      "\n",
      "\t> Best C: 100000\n",
      "\t> Best training score: 0.8641618497109826\n",
      "\t> Best test score: 0.7674418604651163\n",
      "\t> Best held score: 0.8256880733944955\n",
      "\t ----------\n",
      "\t[C: 0.0001, accuracy: 0.5287356321839081]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5526066caca7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_feat_combos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0macc_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/model_data.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-3f1e25590787>\u001b[0m in \u001b[0;36mtest_feat_combos\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 clf = LogisticRegression(\n\u001b[1;32m     45\u001b[0m                     penalty='l2', multi_class='multinomial', solver='lbfgs', C=val)\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                 \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\t[C: {val}, accuracy: {sc}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1296\u001b[0m                       \u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                       sample_weight=sample_weight)\n\u001b[0;32m-> 1298\u001b[0;31m             for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n\u001b[0m\u001b[1;32m   1299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m         \u001b[0mfold_coefs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfold_coefs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mlogistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight)\u001b[0m\n\u001b[1;32m    708\u001b[0m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfprime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m                     iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)\n\u001b[0m\u001b[1;32m    711\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m                 \u001b[0;31m# old scipy doesn't have maxiter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfmin_l_bfgs_b\u001b[0;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n\u001b[0;32m--> 199\u001b[0;31m                            **opts)\n\u001b[0m\u001b[1;32m    200\u001b[0m     d = {'grad': res['jac'],\n\u001b[1;32m    201\u001b[0m          \u001b[0;34m'task'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0mupper_bnd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mnbd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbounds_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmaxls\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_feat_combos(index)\n",
    "acc_df.to_csv('data/model_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "try:\n",
    "    acc_df = pd.read_csv('data/model_data_desc_ab.csv')\n",
    "    index = len(acc_df)\n",
    "except FileNotFoundError:\n",
    "    acc_df = pd.DataFrame(columns=['Model', 'Params', 'Feats', 'Train', 'Validation', 'Test', 'Precision'])\n",
    "    index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def test_desc_combos(index):\n",
    "    feats = ['scaled_perc_nans', 'scaled_mean_token_count',\n",
    "       'scaled_std_dev_token_count', 'has_delimiters',\n",
    "       'scaled_mean_stopword_total', 'scaled_mean_whitespace_count',\n",
    "       'scaled_mean_char_count', 'scaled_mean_delim_count',\n",
    "       'scaled_stdev_stopword_total', 'scaled_stdev_whitespace_count',\n",
    "       'scaled_stdev_char_count', 'scaled_stdev_delim_count', 'has_url',\n",
    "       'has_date', 'has_email']\n",
    "\n",
    "    for ind in range(0, len(feats)):\n",
    "        combo = feats[ind:]\n",
    "        print(f'{\"=\"*5} {combo} {\"=\"*5}')\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            pd.concat([data1[combo], attr_df, sample1_df], axis=1, sort=False), y, test_size=0.2, random_state=100)\n",
    "\n",
    "        X_train_new = X_train.reset_index(drop=True)\n",
    "        y_train_new = y_train.reset_index(drop=True)\n",
    "        X_train_new = X_train_new.values\n",
    "        y_train_new = y_train_new.values\n",
    "        best_param_count = {'cval': {}}\n",
    "        k = 5\n",
    "        kf = KFold(n_splits=k)\n",
    "        avg_train_acc, avg_test_acc = 0, 0\n",
    "\n",
    "        val_arr = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]\n",
    "\n",
    "        avgsc_lst, avgsc_train_lst, avgsc_hld_lst = [], [], []\n",
    "        avgsc, avgsc_train, avgsc_hld = 0, 0, 0\n",
    "\n",
    "        best_param_count = {'cval': {}}\n",
    "        for train_index, test_index in kf.split(X_train_new):\n",
    "            X_train_cur, X_test_cur = X_train_new[train_index], X_train_new[test_index]\n",
    "            y_train_cur, y_test_cur = y_train_new[train_index], y_train_new[test_index]\n",
    "            X_train_train, X_val, y_train_train, y_val = train_test_split(\n",
    "                X_train_cur, y_train_cur, test_size=0.25, random_state=100)\n",
    "\n",
    "            bestPerformingModel = LogisticRegression(\n",
    "                penalty='l2', multi_class='multinomial', solver='lbfgs', C=1)\n",
    "            bestscore = 0\n",
    "            for val in val_arr:\n",
    "                clf = LogisticRegression(\n",
    "                    penalty='l2', multi_class='multinomial', solver='lbfgs', C=val)\n",
    "                clf.fit(X_train_train, y_train_train)\n",
    "                sc = clf.score(X_val, y_val)\n",
    "#                 print(f\"\\t[C: {val}, accuracy: {sc}]\")\n",
    "                if bestscore < sc:\n",
    "                    bestcval = val\n",
    "                    bestscore = sc\n",
    "                    bestPerformingModel = clf\n",
    "\n",
    "            if str(bestcval) in best_param_count['cval']:\n",
    "                best_param_count['cval'][str(bestcval)] += 1\n",
    "            else:\n",
    "                best_param_count['cval'][str(bestcval)] = 1\n",
    "            bscr_train = bestPerformingModel.score(X_train_cur, y_train_cur)\n",
    "            bscr = bestPerformingModel.score(X_test_cur, y_test_cur)\n",
    "            bscr_hld = bestPerformingModel.score(X_test, y_test)\n",
    "            avgsc_train_lst.append(bscr_train)\n",
    "            avgsc_lst.append(bscr)\n",
    "            avgsc_hld_lst.append(bscr_hld)\n",
    "\n",
    "            avgsc_train = avgsc_train + bscr_train\n",
    "            avgsc = avgsc + bscr\n",
    "            avgsc_hld = avgsc_hld + bscr_hld\n",
    "            print()\n",
    "            print(f\"\\t> Best C: {bestcval}\")\n",
    "            print(f\"\\t> Best training score: {bscr_train}\")\n",
    "            print(f\"\\t> Best test score: {bscr}\")\n",
    "            print(f\"\\t> Best held score: {bscr_hld}\")\n",
    "            print('\\t', '-'*10)\n",
    "        \n",
    "        y_pred = bestPerformingModel.predict(X_test)\n",
    "        prec = metrics.precision_score(y_test, y_pred, average=None)\n",
    "        cat_prec = {\n",
    "            'Datetime': prec[0],\n",
    "            'Sentence': prec[1],\n",
    "            'Custom Object': prec[2],\n",
    "            'URL': prec[3],\n",
    "            'Numbers': prec[4],\n",
    "            'List': prec[5],\n",
    "        }\n",
    "        \n",
    "        avgsc_train = avgsc_train/k\n",
    "        avgsc = avgsc/k\n",
    "        avgsc_hld = avgsc_hld/k\n",
    "        bestcval = max(best_param_count['cval'],\n",
    "                       key=lambda i: best_param_count['cval'][i])\n",
    "        bestparams = {'C': bestcval}\n",
    "        print(f\"\\t> Best C param : {bestcval}\")\n",
    "        print(f\"\\t> Average training score list: {avgsc_train_lst}\")\n",
    "        print(f\"\\t> Average testing score list: {avgsc_lst}\")\n",
    "        print(f\"\\t> Average held score list: {avgsc_hld_lst}\")\n",
    "        print()\n",
    "        print(f\"\\t> Average training score list: {avgsc_train}\")\n",
    "        print(f\"\\t> Average testing score list: {avgsc}\")\n",
    "        print(f\"\\t> Average held score list: {avgsc_hld}\")\n",
    "        acc_df.loc[index] = ['logistic_regression', str(\n",
    "            bestparams), combo, avgsc_train, avgsc, avgsc_hld, str(cat_prec)]\n",
    "        index += 1\n",
    "        print()\n",
    "        y_pred = bestPerformingModel.predict(X_test)\n",
    "        cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "        print('\\tConfusion Matrix: Actual (Row) vs Predicted (Column)')\n",
    "        print('\\t', cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== ['scaled_perc_nans', 'scaled_mean_token_count', 'scaled_std_dev_token_count', 'has_delimiters', 'scaled_mean_stopword_total', 'scaled_mean_whitespace_count', 'scaled_mean_char_count', 'scaled_mean_delim_count', 'scaled_stdev_stopword_total', 'scaled_stdev_whitespace_count', 'scaled_stdev_char_count', 'scaled_stdev_delim_count', 'has_url', 'has_date', 'has_email'] =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t> Best C: 100\n",
      "\t> Best training score: 0.9536231884057971\n",
      "\t> Best test score: 0.7586206896551724\n",
      "\t> Best held score: 0.8256880733944955\n",
      "\t ----------\n",
      "\n",
      "\t> Best C: 1\n",
      "\t> Best training score: 0.9333333333333333\n",
      "\t> Best test score: 0.8275862068965517\n",
      "\t> Best held score: 0.8256880733944955\n",
      "\t ----------\n",
      "\n",
      "\t> Best C: 0.1\n",
      "\t> Best training score: 0.8872832369942196\n",
      "\t> Best test score: 0.7906976744186046\n",
      "\t> Best held score: 0.8165137614678899\n",
      "\t ----------\n",
      "\n",
      "\t> Best C: 1\n",
      "\t> Best training score: 0.9393063583815029\n",
      "\t> Best test score: 0.813953488372093\n",
      "\t> Best held score: 0.7981651376146789\n",
      "\t ----------\n",
      "\n",
      "\t> Best C: 1\n",
      "\t> Best training score: 0.953757225433526\n",
      "\t> Best test score: 0.8023255813953488\n",
      "\t> Best held score: 0.8256880733944955\n",
      "\t ----------\n",
      "\t> Best C param : 1\n",
      "\t> Average training score list: [0.9536231884057971, 0.9333333333333333, 0.8872832369942196, 0.9393063583815029, 0.953757225433526]\n",
      "\t> Average testing score list: [0.7586206896551724, 0.8275862068965517, 0.7906976744186046, 0.813953488372093, 0.8023255813953488]\n",
      "\t> Average held score list: [0.8256880733944955, 0.8256880733944955, 0.8165137614678899, 0.7981651376146789, 0.8256880733944955]\n",
      "\n",
      "\t> Average training score list: 0.9334606685096759\n",
      "\t> Average testing score list: 0.7986367281475542\n",
      "\t> Average held score list: 0.8183486238532109\n",
      "\n",
      "\tConfusion Matrix: Actual (Row) vs Predicted (Column)\n",
      "\t [[24  0  3  0  0  0]\n",
      " [ 0 17  5  0  0  0]\n",
      " [ 0  4 47  0  0  1]\n",
      " [ 0  0  0  2  0  0]\n",
      " [ 0  0  1  0  0  0]\n",
      " [ 0  2  3  0  0  0]]\n",
      "===== ['scaled_mean_token_count', 'scaled_std_dev_token_count', 'has_delimiters', 'scaled_mean_stopword_total', 'scaled_mean_whitespace_count', 'scaled_mean_char_count', 'scaled_mean_delim_count', 'scaled_stdev_stopword_total', 'scaled_stdev_whitespace_count', 'scaled_stdev_char_count', 'scaled_stdev_delim_count', 'has_url', 'has_date', 'has_email'] =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t> Best C: 100000\n",
      "\t> Best training score: 0.9565217391304348\n",
      "\t> Best test score: 0.7931034482758621\n",
      "\t> Best held score: 0.7981651376146789\n",
      "\t ----------\n"
     ]
    }
   ],
   "source": [
    "test_desc_combos(index)\n",
    "acc_df.to_csv('data/model_data_desc.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
