{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "  \n",
    "from statistics import mode\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import re\n",
    "import enchant\n",
    "import pickle\n",
    "import time\n",
    "import editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Record_id', 'Attribute_name', 'y_pred', 'y_act', 'Reason',\n",
       "       'total_vals', 'num_nans', '%_nans', 'mean_word_count',\n",
       "       'std_dev_word_count', 'has_delimiters', 'sample_1', 'sample_2',\n",
       "       'sample_3', 'sample_4', 'sample_5', 'mean_stopword_total',\n",
       "       'mean_whitespace_count', 'mean_char_count', 'mean_delim_count',\n",
       "       'stdev_stopword_total', 'stdev_whitespace_count', 'stdev_char_count',\n",
       "       'stdev_delim_count', 'has_url', 'has_date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_label = {\n",
    "    'Datetime':0, \n",
    "    'Sentence':1, \n",
    "    'Custom Object': 2, \n",
    "    'URL': 3, \n",
    "    'Numbers': 4, \n",
    "    'List': 5}\n",
    "\n",
    "data = pd.read_csv('data/needs_extraction_data/labelled_added.csv')\n",
    "data['y_act'] = [dict_label[i] for i in data['y_act']]\n",
    "y = data.loc[:,['y_act']]\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/admin/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:3140: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Data mean: %_nans                   -2.745801e-16\n",
      "mean_word_count          -1.117919e-16\n",
      "std_dev_word_count       -2.236863e-17\n",
      "has_delimiters           -8.619107e-18\n",
      "mean_stopword_total       8.619107e-18\n",
      "mean_whitespace_count    -1.126127e-16\n",
      "mean_char_count          -2.385646e-17\n",
      "mean_delim_count         -5.915375e-17\n",
      "stdev_stopword_total      6.413026e-17\n",
      "stdev_whitespace_count   -2.236863e-17\n",
      "stdev_char_count         -3.488686e-18\n",
      "stdev_delim_count         9.516930e-17\n",
      "has_url                  -1.327753e-16\n",
      "has_date                  1.526813e-16\n",
      "dtype: float64\n",
      "\n",
      "> Data median: %_nans                   -0.653046\n",
      "mean_word_count          -0.144106\n",
      "std_dev_word_count       -0.171320\n",
      "has_delimiters           -0.671120\n",
      "mean_stopword_total      -0.178121\n",
      "mean_whitespace_count    -0.144106\n",
      "mean_char_count          -0.161318\n",
      "mean_delim_count         -0.169718\n",
      "stdev_stopword_total     -0.204091\n",
      "stdev_whitespace_count   -0.171320\n",
      "stdev_char_count         -0.188179\n",
      "stdev_delim_count        -0.220007\n",
      "has_url                  -0.308450\n",
      "has_date                  0.568101\n",
      "dtype: float64\n",
      "\n",
      "> Data stdev: %_nans                    1.000925\n",
      "mean_word_count           1.000925\n",
      "std_dev_word_count        1.000925\n",
      "has_delimiters            1.000925\n",
      "mean_stopword_total       1.000925\n",
      "mean_whitespace_count     1.000925\n",
      "mean_char_count           1.000925\n",
      "mean_delim_count          1.000925\n",
      "stdev_stopword_total      1.000925\n",
      "stdev_whitespace_count    1.000925\n",
      "stdev_char_count          1.000925\n",
      "stdev_delim_count         1.000925\n",
      "has_url                   1.000925\n",
      "has_date                  1.000925\n",
      "dtype: float64\n",
      "===[VECTORIZATION]===\n",
      "> Length of vectorized feature_names: 8528\n",
      "X_train preview:\n",
      "       %_nans  mean_word_count  std_dev_word_count  has_delimiters  \\\n",
      "453 -0.653097         0.686283            3.364514        1.490046   \n",
      "43  -0.653120         0.162079           -0.054513        1.490046   \n",
      "133  1.978459        -0.148544           -0.167108        1.490046   \n",
      "205 -0.653120        -0.141062           -0.175870       -0.671120   \n",
      "282 -0.653120        -0.148960           -0.175870       -0.671120   \n",
      "\n",
      "     mean_stopword_total  mean_whitespace_count  mean_char_count  \\\n",
      "453             0.945220               0.686283         1.077718   \n",
      "43              0.126161               0.162079         0.149129   \n",
      "133            -0.187400              -0.148544        -0.177140   \n",
      "205            -0.178121              -0.141062        -0.155756   \n",
      "282            -0.187845              -0.148960        -0.170754   \n",
      "\n",
      "     mean_delim_count  stdev_stopword_total  stdev_whitespace_count  ...   \\\n",
      "453          1.401002              3.066931                3.364514  ...    \n",
      "43           0.026371             -0.088312               -0.054513  ...    \n",
      "133         -0.191491             -0.202753               -0.167108  ...    \n",
      "205         -0.062569             -0.210742               -0.175870  ...    \n",
      "282         -0.127171             -0.210742               -0.175870  ...    \n",
      "\n",
      "     1335  1336  1337  1338  1339  1340  1341  1342  1343  1344  \n",
      "453     0     0     0     0     0     0     0     0     0     0  \n",
      "43      0     0     0     0     0     0     0     0     0     0  \n",
      "133     0     0     0     0     0     0     0     0     0     0  \n",
      "205     0     0     0     0     0     0     0     0     0     0  \n",
      "282     0     0     0     0     0     0     0     0     0     0  \n",
      "\n",
      "[5 rows x 1359 columns]\n",
      "y_train preview:\n",
      "     y_act\n",
      "453    1.0\n",
      "43     1.0\n",
      "133    2.0\n",
      "205    0.0\n",
      "282    0.0\n"
     ]
    }
   ],
   "source": [
    "data1 = data[['%_nans', 'mean_word_count',\n",
    "              'std_dev_word_count', 'has_delimiters', 'mean_stopword_total',\n",
    "              'mean_whitespace_count', 'mean_char_count', 'mean_delim_count',\n",
    "              'stdev_stopword_total', 'stdev_whitespace_count', 'stdev_char_count',\n",
    "              'stdev_delim_count', 'has_url', 'has_date']]\n",
    "# data1 = data1.fillna(0)\n",
    "\n",
    "# data1 = data1.rename(columns={\n",
    "#     'mean_word_count': 'scaled_mean_token_count',\n",
    "#     'std_dev_word_count': 'scaled_std_dev_token_count',\n",
    "#     '%_nans': 'scaled_perc_nans',\n",
    "#     'mean_stopword_total': 'scaled_mean_stopword_total',\n",
    "#     'mean_whitespace_count': 'scaled_mean_whitespace_count',\n",
    "#     'mean_char_count': 'scaled_mean_char_count',\n",
    "#     'mean_delim_count': 'scaled_mean_delim_count',\n",
    "#     'stdev_stopword_total': 'scaled_stdev_stopword_total',\n",
    "#     'stdev_whitespace_count': 'scaled_stdev_whitespace_count',\n",
    "#     'stdev_char_count': 'scaled_stdev_char_count',\n",
    "#     'stdev_delim_count': 'scaled_stdev_delim_count'\n",
    "# })\n",
    "# data1.loc[data1['scaled_mean_token_count'] >\n",
    "#           10000, 'scaled_mean_token_count'] = 10000\n",
    "# data1.loc[data1['scaled_mean_token_count'] < -\n",
    "#           10000, 'scaled_mean_token_count'] = -10000\n",
    "\n",
    "# data1.loc[data1['scaled_std_dev_token_count'] >\n",
    "#           10000, 'scaled_std_dev_token_count'] = 10000\n",
    "# data1.loc[data1['scaled_std_dev_token_count'] < -\n",
    "#           10000, 'scaled_std_dev_token_count'] = -10000\n",
    "\n",
    "# data1.loc[data1['scaled_perc_nans'] > 10000, 'scaled_perc_nans'] = 10000\n",
    "# data1.loc[data1['scaled_perc_nans'] < -10000, 'scaled_perc_nans'] = -10000\n",
    "\n",
    "# data1.loc[data1['scaled_mean_stopword_total'] >\n",
    "#           10000, 'scaled_mean_stopword_total'] = 10000\n",
    "# data1.loc[data1['scaled_mean_stopword_total'] < -\n",
    "#           10000, 'scaled_mean_stopword_total'] = -10000\n",
    "\n",
    "# data1.loc[data1['scaled_mean_whitespace_count'] >\n",
    "#           10000, 'scaled_mean_whitespace_count'] = 10000\n",
    "# data1.loc[data1['scaled_mean_whitespace_count'] < -\n",
    "#           10000, 'scaled_mean_whitespace_count'] = -10000\n",
    "\n",
    "# data1.loc[data1['scaled_mean_char_count'] >\n",
    "#           10000, 'scaled_mean_char_count'] = 10000\n",
    "# data1.loc[data1['scaled_mean_char_count'] < -\n",
    "#           10000, 'scaled_mean_char_count'] = -10000\n",
    "\n",
    "# data1.loc[data1['scaled_mean_delim_count'] >\n",
    "#           10000, 'scaled_mean_delim_count'] = 10000\n",
    "# data1.loc[data1['scaled_mean_delim_count'] < -\n",
    "#           10000, 'scaled_mean_delim_count'] = -10000\n",
    "\n",
    "# data1.loc[data1['scaled_stdev_stopword_total'] >\n",
    "#           10000, 'scaled_stdev_stopword_total'] = 10000\n",
    "# data1.loc[data1['scaled_stdev_stopword_total'] < -\n",
    "#           10000, 'scaled_stdev_stopword_total'] = -10000\n",
    "\n",
    "# data1.loc[data1['scaled_stdev_whitespace_count'] >\n",
    "#           10000, 'scaled_stdev_whitespace_count'] = 10000\n",
    "# data1.loc[data1['scaled_stdev_whitespace_count'] < -\n",
    "#           10000, 'scaled_stdev_whitespace_count'] = -10000\n",
    "\n",
    "# data1.loc[data1['scaled_stdev_char_count'] >\n",
    "#           10000, 'scaled_stdev_char_count'] = 10000\n",
    "# data1.loc[data1['scaled_stdev_char_count'] < -\n",
    "#           10000, 'scaled_stdev_char_count'] = -10000\n",
    "\n",
    "# data1.loc[data1['scaled_stdev_delim_count'] >\n",
    "#           10000, 'scaled_stdev_delim_count'] = 10000\n",
    "# data1.loc[data1['scaled_stdev_delim_count'] < -\n",
    "#           10000, 'scaled_stdev_delim_count'] = -10000\n",
    "\n",
    "# column_names_to_normalize = ['scaled_mean_token_count',\n",
    "#                              'scaled_std_dev_token_count',\n",
    "#                              'scaled_perc_nans',\n",
    "#                              'scaled_mean_stopword_total',\n",
    "#                              'scaled_mean_whitespace_count',\n",
    "#                              'scaled_mean_char_count',\n",
    "#                              'scaled_mean_delim_count',\n",
    "#                              'scaled_stdev_stopword_total',\n",
    "#                              'scaled_stdev_whitespace_count',\n",
    "#                              'scaled_stdev_char_count',\n",
    "#                              'scaled_stdev_delim_count']\n",
    "columns_to_use = ['%_nans', 'mean_word_count',\n",
    "              'std_dev_word_count', 'has_delimiters', 'mean_stopword_total',\n",
    "              'mean_whitespace_count', 'mean_char_count', 'mean_delim_count',\n",
    "              'stdev_stopword_total', 'stdev_whitespace_count', 'stdev_char_count',\n",
    "              'stdev_delim_count', 'has_url', 'has_date']\n",
    "x = data1[columns_to_use].values\n",
    "x = np.nan_to_num(x)\n",
    "x_scaled = StandardScaler().fit_transform(x)\n",
    "df_temp = pd.DataFrame(\n",
    "    x_scaled, columns=columns_to_use, index=data1.index)\n",
    "data1[columns_to_use] = df_temp\n",
    "\n",
    "y.y_act = y.y_act.astype(float)\n",
    "\n",
    "print(f\"> Data mean: {data1.mean()}\\n\")\n",
    "print(f\"> Data median: {data1.median()}\\n\")\n",
    "print(f\"> Data stdev: {data1.std()}\")\n",
    "\n",
    "print(\"===[VECTORIZATION]===\")\n",
    "arr = data['Attribute_name'].values\n",
    "data = data.fillna(0)\n",
    "arr1 = data['sample_1'].values\n",
    "arr1 = [str(x) for x in arr1]\n",
    "arr2 = data['sample_2'].values\n",
    "arr2 = [str(x) for x in arr2]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(3, 3), analyzer='char')\n",
    "X = vectorizer.fit_transform(arr)\n",
    "X1 = vectorizer.fit_transform(arr1)\n",
    "X2 = vectorizer.fit_transform(arr2)\n",
    "\n",
    "print(f\"> Length of vectorized feature_names: {len(vectorizer.get_feature_names())}\")\n",
    "\n",
    "# data1.to_csv('data/preprocessing/before.csv')\n",
    "attr_df = pd.DataFrame(X.toarray())\n",
    "sample1_df = pd.DataFrame(X1.toarray())\n",
    "sample2_df = pd.DataFrame(X2.toarray())\n",
    "# data2 = pd.concat([data1, attr_df, sample1_df, sample2_df], axis=1, sort=False)\n",
    "data2 = pd.concat([data1, attr_df], axis=1, sort=False)\n",
    "\n",
    "# data2.to_csv('data/preprocessing/after.csv')\n",
    "data2.head()\n",
    "\n",
    "X_train, X_test_held, y_train, y_test_held = train_test_split(\n",
    "    data2, y, test_size=0.2, random_state=100)\n",
    "\n",
    "X_train_new = X_train.reset_index(drop=True)\n",
    "y_train_new = y_train.reset_index(drop=True)\n",
    "print(f\"X_train preview:\\n{X_train.head()}\")\n",
    "print(f\"y_train preview:\\n{y_train.head()}\")\n",
    "\n",
    "X_train_new = X_train_new.values\n",
    "y_train_new = y_train_new.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_train, X_val, y_train_train, y_val = train_test_split(\n",
    "    X_train_new, y_train_new, test_size=0.25, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_failed(model, x_test, y_test):\n",
    "    data = pd.read_csv('data/needs_extraction_data/labelled_added.csv')\n",
    "    dict_label = {\n",
    "        0: 'Datetime',\n",
    "        1: 'Sentence',\n",
    "        2: 'Custom Object',\n",
    "        3: 'URL',\n",
    "        4: 'Numbers',\n",
    "        5: 'List'\n",
    "    }\n",
    "    preds = model.predict(x_test)\n",
    "    ret_df = pd.DataFrame(columns=['Index', 'Attribute_name', 'y_pred', 'y_act', 'sample_1', '%_nans', 'mean_word_count',\n",
    "              'std_dev_word_count', 'has_delimiters', 'mean_stopword_total',\n",
    "              'mean_whitespace_count', 'mean_char_count', 'mean_delim_count',\n",
    "              'stdev_stopword_total', 'stdev_whitespace_count', 'stdev_char_count',\n",
    "              'stdev_delim_count', 'has_url', 'has_date'])\n",
    "    count = 0\n",
    "    for pred in preds:\n",
    "        y_true = int(y_test.values[count])\n",
    "        if pred != y_true:\n",
    "            index = x_test.index[count]\n",
    "            row = data.loc[index]\n",
    "            samples = [\n",
    "                row.sample_1\n",
    "            ]\n",
    "            attr_name = row['Attribute_name']\n",
    "            ret_df.loc[count] = [index, attr_name, pred, y_true] + samples + list(row[['%_nans', 'mean_word_count',\n",
    "              'std_dev_word_count', 'has_delimiters', 'mean_stopword_total',\n",
    "              'mean_whitespace_count', 'mean_char_count', 'mean_delim_count',\n",
    "              'stdev_stopword_total', 'stdev_whitespace_count', 'stdev_char_count',\n",
    "              'stdev_delim_count', 'has_url', 'has_date']])\n",
    "        count += 1\n",
    "    ret_df['y_act'] = ret_df['y_act'].astype(int)\n",
    "    ret_df['y_pred'] = ret_df['y_pred'].astype(int)\n",
    "    ret_df['y_act'] = [dict_label[i] for i in ret_df['y_act']]\n",
    "    ret_df['y_pred'] = [dict_label[i] for i in ret_df['y_pred']]\n",
    "    return ret_df\n",
    "\n",
    "\n",
    "def get_false_nums(df):\n",
    "    false_labels = df.y_pred.unique()\n",
    "    true_labels = df.y_act.unique()\n",
    "    length = len(df)\n",
    "    for label in false_labels:\n",
    "        total = len(df.loc[df.y_pred == label])\n",
    "        print(f\">>> Incorrectly predicted as {label}: {total}, {'{0:.3g}'.format((total/length)*100)}%\")\n",
    "        \n",
    "        temp_df = df.loc[df.y_pred == label]\n",
    "        temp_labels = temp_df.y_act.unique()\n",
    "        t_len = len(temp_df)\n",
    "        for t_label in temp_labels:\n",
    "            t_total = len(temp_df[temp_df.y_act == t_label])\n",
    "            print(f\"\\t>>> {t_label} predicted as {label}: {t_total}, {'{0:.5g}'.format((t_total/t_len)*100)}%\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \"\"\"\n",
      "/Users/admin/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== [Random Forest] ====================\n",
      "> Train: 0.9537037037037037\n",
      "> Validate: 0.8148148148148148\n",
      "> Test: 0.8256880733944955\n",
      "> Precision Numbers: 0.7692307692307693\n",
      "> Precision Not Numbers: 0.8518518518518519\n",
      "> Recall Numbers: 0.9090909090909091\n",
      "> Recall Not Numbers: 0.8518518518518519\n",
      "\n",
      "==================== [Logistic Regression] ====================\n",
      "> Train: 0.9027777777777778\n",
      "> Validate: 0.7777777777777778\n",
      "> Test: 0.8348623853211009\n",
      "> Precision Numbers: 0.8421052631578947\n",
      "> Precision Not Numbers: 0.9230769230769231\n",
      "> Recall Numbers: 0.7272727272727273\n",
      "> Recall Not Numbers: 0.8888888888888888\n",
      "\n",
      "==================== [SVM] ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Train: 0.9166666666666666\n",
      "> Validate: 0.7314814814814815\n",
      "> Test: 0.7981651376146789\n",
      "> Precision Numbers: 1.0\n",
      "> Precision Not Numbers: 0.92\n",
      "> Recall Numbers: 0.6363636363636364\n",
      "> Recall Not Numbers: 0.8518518518518519\n"
     ]
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=50, max_depth=50, random_state=100)\n",
    "lr_clf = LogisticRegression(C=1, random_state=100)\n",
    "svm_clf = svm.SVC(C=10, gamma=0.1, random_state=100)\n",
    "\n",
    "rf_clf.fit(X_train_train, y_train_train)\n",
    "lr_clf.fit(X_train_train, y_train_train)\n",
    "svm_clf.fit(X_train_train, y_train_train)\n",
    "\n",
    "print(\"=\"*20,\"[Random Forest]\",\"=\"*20)\n",
    "trainsc = rf_clf.score(X_train, y_train)\n",
    "valsc = rf_clf.score(X_val, y_val)\n",
    "heldsc = rf_clf.score(X_test_held, y_test_held)\n",
    "held_pr = metrics.precision_recall_fscore_support(y_test_held, rf_clf.predict(X_test_held))\n",
    "print(f\"> Train: {trainsc}\")\n",
    "print(f\"> Validate: {valsc}\")\n",
    "print(f\"> Test: {heldsc}\")\n",
    "print(f\"> Precision Numbers: {held_pr[0][1]}\")\n",
    "print(f\"> Precision Not Numbers: {held_pr[0][0]}\")\n",
    "print(f\"> Recall Numbers: {held_pr[1][1]}\")\n",
    "print(f\"> Recall Not Numbers: {held_pr[1][0]}\")\n",
    "print()\n",
    "print(\"=\"*20,\"[Logistic Regression]\",\"=\"*20)\n",
    "trainsc = lr_clf.score(X_train, y_train)\n",
    "valsc = lr_clf.score(X_val, y_val)\n",
    "heldsc = lr_clf.score(X_test_held, y_test_held)\n",
    "held_pr = metrics.precision_recall_fscore_support(y_test_held, lr_clf.predict(X_test_held))\n",
    "print(f\"> Train: {trainsc}\")\n",
    "print(f\"> Validate: {valsc}\")\n",
    "print(f\"> Test: {heldsc}\")\n",
    "print(f\"> Precision Numbers: {held_pr[0][1]}\")\n",
    "print(f\"> Precision Not Numbers: {held_pr[0][0]}\")\n",
    "print(f\"> Recall Numbers: {held_pr[1][1]}\")\n",
    "print(f\"> Recall Not Numbers: {held_pr[1][0]}\")\n",
    "print()\n",
    "print(\"=\"*20,\"[SVM]\",\"=\"*20)\n",
    "trainsc = svm_clf.score(X_train, y_train)\n",
    "valsc = svm_clf.score(X_val, y_val)\n",
    "heldsc = svm_clf.score(X_test_held, y_test_held)\n",
    "held_pr = metrics.precision_recall_fscore_support(y_test_held, svm_clf.predict(X_test_held))\n",
    "print(f\"> Train: {trainsc}\")\n",
    "print(f\"> Validate: {valsc}\")\n",
    "print(f\"> Test: {heldsc}\")\n",
    "print(f\"> Precision Numbers: {held_pr[0][1]}\")\n",
    "print(f\"> Precision Not Numbers: {held_pr[0][0]}\")\n",
    "print(f\"> Recall Numbers: {held_pr[1][1]}\")\n",
    "print(f\"> Recall Not Numbers: {held_pr[1][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \"\"\"\n",
      "/Users/admin/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== [Random Forest] ====================\n",
      ">>> Total incorrect predictions: 19, 17.43%\n",
      ">>> Incorrectly predicted as Custom Object: 8, 42.1%\n",
      "\t>>> List predicted as Custom Object: 2, 25%\n",
      "\t>>> Datetime predicted as Custom Object: 4, 50%\n",
      "\t>>> Sentence predicted as Custom Object: 2, 25%\n",
      ">>> Incorrectly predicted as Sentence: 6, 31.6%\n",
      "\t>>> List predicted as Sentence: 2, 33.333%\n",
      "\t>>> Custom Object predicted as Sentence: 4, 66.667%\n",
      ">>> Incorrectly predicted as Datetime: 4, 21.1%\n",
      "\t>>> Custom Object predicted as Datetime: 3, 75%\n",
      "\t>>> Numbers predicted as Datetime: 1, 25%\n",
      ">>> Incorrectly predicted as List: 1, 5.26%\n",
      "\t>>> Custom Object predicted as List: 1, 100%\n",
      "\n",
      "\n",
      "   Index                              Attribute_name         y_pred  \\\n",
      "0    485                                   site_info  Custom Object   \n",
      "1    188  customers_who_bought_this_item_also_bought       Sentence   \n",
      "4    260                                    soloists       Sentence   \n",
      "5    344                   ri_coll_teach_select_date       Datetime   \n",
      "15   360                                    duration       Datetime   \n",
      "28   149                          WHOIS_UPDATED_DATE  Custom Object   \n",
      "33   150                               WHOIS_REGDATE  Custom Object   \n",
      "36   312                                    Location       Datetime   \n",
      "49   505                                        item           List   \n",
      "51   321                     job_info_alt_field_name  Custom Object   \n",
      "\n",
      "            y_act                                           sample_1  \\\n",
      "0            List                       Sidewalk: Curb side : Cutout   \n",
      "1            List                                                NaN   \n",
      "4   Custom Object  [{u'soloistName': u'Voigt, Deborah', u'soloist...   \n",
      "5   Custom Object                                                NaN   \n",
      "15        Numbers                                             30.178   \n",
      "28       Datetime                                    16/11/2016 0:00   \n",
      "33       Datetime                                    18/01/1994 0:00   \n",
      "36  Custom Object                            -122.316794,37.925086,0   \n",
      "49  Custom Object                                  Total Value Added   \n",
      "51           List                                                NaN   \n",
      "\n",
      "      %_nans  mean_word_count  std_dev_word_count has_delimiters  \\\n",
      "0   0.000000         4.790605            0.756768          False   \n",
      "1   0.106200         8.967800            3.792013          False   \n",
      "4   0.000000         5.659029           14.361233          False   \n",
      "5   0.984694         1.000000            0.000000          False   \n",
      "15  0.000000         1.000000            0.000000           True   \n",
      "28  0.000000         1.919147            0.272610           True   \n",
      "33  0.000000         1.924200            0.264678           True   \n",
      "36  0.000000         1.000000            0.000000          False   \n",
      "49  0.000000         4.746626            1.713252          False   \n",
      "51  0.750434         2.293630            2.649395           True   \n",
      "\n",
      "    mean_stopword_total  mean_whitespace_count  mean_char_count  \\\n",
      "0              5.700863               3.790605        26.490067   \n",
      "1             18.920100               7.967800       327.683600   \n",
      "4             17.163492               4.659029        66.198628   \n",
      "5              1.000000               0.000000         3.107142   \n",
      "15             1.000000               0.000000         6.060150   \n",
      "28             1.919147               0.919147        13.947221   \n",
      "33             1.924200               0.924200        13.982594   \n",
      "36             1.000000               0.000000        23.000000   \n",
      "49             7.243788               3.746626        35.344214   \n",
      "51             2.482576               1.293630        13.849042   \n",
      "\n",
      "    mean_delim_count  stdev_stopword_total  stdev_whitespace_count  \\\n",
      "0           1.910258              1.017474                0.756768   \n",
      "1          40.752000              8.221078                3.792013   \n",
      "4           4.445279             44.890993               14.361233   \n",
      "5           0.030612              0.000000                0.000000   \n",
      "15          1.021117              0.000000                0.000000   \n",
      "28          0.933184              0.272610                0.272610   \n",
      "33          0.938237              0.264678                0.264678   \n",
      "36          5.000000              0.000000                0.000000   \n",
      "49          1.497108              2.771545                1.713252   \n",
      "51          0.506601              3.328730                2.649395   \n",
      "\n",
      "    stdev_char_count  stdev_delim_count has_url has_date  \n",
      "0           5.342256           0.285812   False    False  \n",
      "1         150.045720          18.638463    True     True  \n",
      "4         189.722768          13.799144   False    False  \n",
      "5           0.859370           0.245534   False     True  \n",
      "15          0.412837           0.143773   False     True  \n",
      "28          2.968205           0.344239   False     True  \n",
      "33          2.886634           0.337782   False     True  \n",
      "36          0.000000           0.000000   False    False  \n",
      "49         14.443932           1.498747   False    False  \n",
      "51         21.384505           1.587924   False     True  \n",
      "==================== [Logistic Regression] ====================\n",
      ">>> Total incorrect predictions: 18, 16.51%\n",
      ">>> Incorrectly predicted as Custom Object: 10, 55.6%\n",
      "\t>>> List predicted as Custom Object: 2, 20%\n",
      "\t>>> Numbers predicted as Custom Object: 1, 10%\n",
      "\t>>> Sentence predicted as Custom Object: 4, 40%\n",
      "\t>>> Datetime predicted as Custom Object: 3, 30%\n",
      ">>> Incorrectly predicted as URL: 3, 16.7%\n",
      "\t>>> List predicted as URL: 1, 33.333%\n",
      "\t>>> Custom Object predicted as URL: 1, 33.333%\n",
      "\t>>> Sentence predicted as URL: 1, 33.333%\n",
      ">>> Incorrectly predicted as Datetime: 2, 11.1%\n",
      "\t>>> Custom Object predicted as Datetime: 1, 50%\n",
      "\t>>> Sentence predicted as Datetime: 1, 50%\n",
      ">>> Incorrectly predicted as Sentence: 3, 16.7%\n",
      "\t>>> Custom Object predicted as Sentence: 3, 100%\n",
      "\n",
      "\n",
      "   Index                              Attribute_name         y_pred  \\\n",
      "0    485                                   site_info  Custom Object   \n",
      "1    188  customers_who_bought_this_item_also_bought            URL   \n",
      "5    344                   ri_coll_teach_select_date       Datetime   \n",
      "15   360                                    duration  Custom Object   \n",
      "25   240                                    hometown            URL   \n",
      "26   414                                 description       Sentence   \n",
      "32   160                                SpecialNotes  Custom Object   \n",
      "34   398                            Short definition  Custom Object   \n",
      "51   321                     job_info_alt_field_name  Custom Object   \n",
      "54    29                                   monthyear  Custom Object   \n",
      "\n",
      "            y_act                                           sample_1  \\\n",
      "0            List                       Sidewalk: Curb side : Cutout   \n",
      "1            List                                                NaN   \n",
      "5   Custom Object                                                NaN   \n",
      "15        Numbers                                             30.178   \n",
      "25  Custom Object                                     Pittsburgh, PA   \n",
      "26  Custom Object                                      911/NO  VOICE   \n",
      "32       Sentence  Fiscal year ends on June 30; reporting period ...   \n",
      "34       Sentence  Simple mean most favored nation tariff rate is...   \n",
      "51           List                                                NaN   \n",
      "54       Datetime                                2017-08-17T00:00:00   \n",
      "\n",
      "      %_nans  mean_word_count  std_dev_word_count has_delimiters  \\\n",
      "0   0.000000         4.790605            0.756768          False   \n",
      "1   0.106200         8.967800            3.792013          False   \n",
      "5   0.984694         1.000000            0.000000          False   \n",
      "15  0.000000         1.000000            0.000000           True   \n",
      "25  0.000152         1.157980            0.440571           True   \n",
      "26  0.000000         1.910031            0.713143           True   \n",
      "32  0.441065        14.494297           23.136867          False   \n",
      "34  0.790514        13.915020           30.370874          False   \n",
      "51  0.750434         2.293630            2.649395           True   \n",
      "54  0.000000         1.000000            0.000000          False   \n",
      "\n",
      "    mean_stopword_total  mean_whitespace_count  mean_char_count  \\\n",
      "0              5.700863               3.790605        26.490067   \n",
      "1             18.920100               7.967800       327.683600   \n",
      "5              1.000000               0.000000         3.107142   \n",
      "15             1.000000               0.000000         6.060150   \n",
      "25             1.197874               0.157980         9.212062   \n",
      "26             1.739073               0.910031        12.043655   \n",
      "32            12.866920              13.494297        89.250951   \n",
      "34            10.617260              12.915020        88.127800   \n",
      "51             2.482576               1.293630        13.849042   \n",
      "54             1.000000               0.000000        19.000000   \n",
      "\n",
      "    mean_delim_count  stdev_stopword_total  stdev_whitespace_count  \\\n",
      "0           1.910258              1.017474                0.756768   \n",
      "1          40.752000              8.221078                3.792013   \n",
      "5           0.030612              0.000000                0.000000   \n",
      "15          1.021117              0.000000                0.000000   \n",
      "25          0.041443              0.581830                0.440571   \n",
      "26          0.007120              0.524211                0.713143   \n",
      "32          2.239544             18.797332               23.136867   \n",
      "34          1.667325             22.586426               30.370874   \n",
      "51          0.506601              3.328730                2.649395   \n",
      "54          4.000000              0.000000                0.000000   \n",
      "\n",
      "    stdev_char_count  stdev_delim_count has_url has_date  \n",
      "0           5.342256           0.285812   False    False  \n",
      "1         150.045720          18.638463    True     True  \n",
      "5           0.859370           0.245534   False     True  \n",
      "15          0.412837           0.143773   False     True  \n",
      "25          1.708736           0.213812    True     True  \n",
      "26          2.782283           0.089102   False     True  \n",
      "32        151.507776           3.419864   False     True  \n",
      "34        199.544879           3.901370   False     True  \n",
      "51         21.384505           1.587924   False     True  \n",
      "54          0.000000           0.000000   False     True  \n",
      "==================== [SVM] ====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Total incorrect predictions: 22, 20.18%\n",
      ">>> Incorrectly predicted as Custom Object: 17, 77.3%\n",
      "\t>>> List predicted as Custom Object: 5, 29.412%\n",
      "\t>>> Datetime predicted as Custom Object: 4, 23.529%\n",
      "\t>>> Sentence predicted as Custom Object: 8, 47.059%\n",
      ">>> Incorrectly predicted as Datetime: 2, 9.09%\n",
      "\t>>> Numbers predicted as Datetime: 1, 50%\n",
      "\t>>> Custom Object predicted as Datetime: 1, 50%\n",
      ">>> Incorrectly predicted as List: 2, 9.09%\n",
      "\t>>> Custom Object predicted as List: 2, 100%\n",
      ">>> Incorrectly predicted as Numbers: 1, 4.55%\n",
      "\t>>> Custom Object predicted as Numbers: 1, 100%\n",
      "\n",
      "\n",
      "   Index                              Attribute_name         y_pred  \\\n",
      "0    485                                   site_info  Custom Object   \n",
      "1    188  customers_who_bought_this_item_also_bought  Custom Object   \n",
      "8    477                               Period Ending  Custom Object   \n",
      "9    175                             Survey Question  Custom Object   \n",
      "15   360                                    duration       Datetime   \n",
      "32   160                                SpecialNotes  Custom Object   \n",
      "34   398                            Short definition  Custom Object   \n",
      "49   505                                        item           List   \n",
      "51   321                     job_info_alt_field_name  Custom Object   \n",
      "54    29                                   monthyear  Custom Object   \n",
      "\n",
      "            y_act                                           sample_1  \\\n",
      "0            List                       Sidewalk: Curb side : Cutout   \n",
      "1            List                                                NaN   \n",
      "8        Datetime                                         2015-12-31   \n",
      "9        Sentence  What are some of the biggest challenges you fa...   \n",
      "15        Numbers                                             30.178   \n",
      "32       Sentence  Fiscal year ends on June 30; reporting period ...   \n",
      "34       Sentence  Simple mean most favored nation tariff rate is...   \n",
      "49  Custom Object                                  Total Value Added   \n",
      "51           List                                                NaN   \n",
      "54       Datetime                                2017-08-17T00:00:00   \n",
      "\n",
      "      %_nans  mean_word_count  std_dev_word_count has_delimiters  \\\n",
      "0   0.000000         4.790605            0.756768          False   \n",
      "1   0.106200         8.967800            3.792013          False   \n",
      "8   0.000000         1.000000            0.000000          False   \n",
      "9   0.015936         8.932271            8.465503          False   \n",
      "15  0.000000         1.000000            0.000000           True   \n",
      "32  0.441065        14.494297           23.136867          False   \n",
      "34  0.790514        13.915020           30.370874          False   \n",
      "49  0.000000         4.746626            1.713252          False   \n",
      "51  0.750434         2.293630            2.649395           True   \n",
      "54  0.000000         1.000000            0.000000          False   \n",
      "\n",
      "    mean_stopword_total  mean_whitespace_count  mean_char_count  \\\n",
      "0              5.700863               3.790605        26.490067   \n",
      "1             18.920100               7.967800       327.683600   \n",
      "8              1.000000               0.000000        10.000000   \n",
      "9              6.490040               7.932271        52.155378   \n",
      "15             1.000000               0.000000         6.060150   \n",
      "32            12.866920              13.494297        89.250951   \n",
      "34            10.617260              12.915020        88.127800   \n",
      "49             7.243788               3.746626        35.344214   \n",
      "51             2.482576               1.293630        13.849042   \n",
      "54             1.000000               0.000000        19.000000   \n",
      "\n",
      "    mean_delim_count  stdev_stopword_total  stdev_whitespace_count  \\\n",
      "0           1.910258              1.017474                0.756768   \n",
      "1          40.752000              8.221078                3.792013   \n",
      "8           2.000000              0.000000                0.000000   \n",
      "9           0.426295              5.803119                8.465503   \n",
      "15          1.021117              0.000000                0.000000   \n",
      "32          2.239544             18.797332               23.136867   \n",
      "34          1.667325             22.586426               30.370874   \n",
      "49          1.497108              2.771545                1.713252   \n",
      "51          0.506601              3.328730                2.649395   \n",
      "54          4.000000              0.000000                0.000000   \n",
      "\n",
      "    stdev_char_count  stdev_delim_count has_url has_date  \n",
      "0           5.342256           0.285812   False    False  \n",
      "1         150.045720          18.638463    True     True  \n",
      "8           0.000000           0.000000   False     True  \n",
      "9          49.093087           0.723500   False     True  \n",
      "15          0.412837           0.143773   False     True  \n",
      "32        151.507776           3.419864   False     True  \n",
      "34        199.544879           3.901370   False     True  \n",
      "49         14.443932           1.498747   False    False  \n",
      "51         21.384505           1.587924   False     True  \n",
      "54          0.000000           0.000000   False     True  \n"
     ]
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=50, max_depth=50, random_state=100)\n",
    "lr_clf = LogisticRegression(C=1, random_state=100)\n",
    "svm_clf = svm.SVC(C=10, gamma=0.1, random_state=100)\n",
    "\n",
    "rf_clf.fit(X_train_train, y_train_train)\n",
    "lr_clf.fit(X_train_train, y_train_train)\n",
    "svm_clf.fit(X_train_train, y_train_train)\n",
    "\n",
    "print(\"=\"*20,\"[Random Forest]\",\"=\"*20)\n",
    "fails = record_failed(rf_clf, X_test_held, y_test_held)\n",
    "print(f\">>> Total incorrect predictions: {len(fails)}, {'{0:.4g}'.format(((len(fails)/len(y_test_held))*100))}%\")\n",
    "get_false_nums(fails)\n",
    "print(fails.head(10))\n",
    "fails.to_csv(\"rf_false.csv\", index=False)\n",
    "print(\"=\"*20,\"[Logistic Regression]\",\"=\"*20)\n",
    "fails = record_failed(lr_clf, X_test_held, y_test_held)\n",
    "print(f\">>> Total incorrect predictions: {len(fails)}, {'{0:.4g}'.format(((len(fails)/len(y_test_held))*100))}%\")\n",
    "get_false_nums(fails)\n",
    "print(fails.head(10))\n",
    "fails.to_csv(\"lr_false.csv\", index=False)\n",
    "print(\"=\"*20,\"[SVM]\",\"=\"*20)\n",
    "fails = record_failed(svm_clf, X_test_held, y_test_held)\n",
    "print(f\">>> Total incorrect predictions: {len(fails)}, {'{0:.4g}'.format(((len(fails)/len(y_test_held))*100))}%\")\n",
    "get_false_nums(fails)\n",
    "print(fails.head(10))\n",
    "fails.to_csv(\"svm_false.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
