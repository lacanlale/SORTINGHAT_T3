{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import linear_model\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv\n",
    "dict_label = {\n",
    "    'Datetime':0, \n",
    "    'Sentence':1, \n",
    "    'Custom Object': 2, \n",
    "    'URL': 3, \n",
    "    'Numbers': 4, \n",
    "    'List': 5}\n",
    "data = pd.read_csv('data/needs_extraction_data/labelled_data.csv')\n",
    "\n",
    "data['y_act'] = [dict_label[i] for i in data['y_act']]\n",
    "y = data.loc[:,['y_act']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Data mean: scaled_perc_nans             -2.745801e-16\n",
      "scaled_mean_token_count      -1.117919e-16\n",
      "scaled_std_dev_token_count   -2.236863e-17\n",
      "has_delimiters                3.105360e-01\n",
      "dtype: float64\n",
      "> Data median: scaled_perc_nans             -0.653046\n",
      "scaled_mean_token_count      -0.144106\n",
      "scaled_std_dev_token_count   -0.171320\n",
      "has_delimiters                0.000000\n",
      "dtype: float64\n",
      "> Data stdev: scaled_perc_nans              1.000925\n",
      "scaled_mean_token_count       1.000925\n",
      "scaled_std_dev_token_count    1.000925\n",
      "has_delimiters                0.463141\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data1 = data[['%_nans', 'mean_word_count', 'std_dev_word_count', 'has_delimiters']]\n",
    "data1 = data1.fillna(0)\n",
    "\n",
    "data1 = data1.rename(columns={'mean_word_count': 'scaled_mean_token_count', 'std_dev_word_count': 'scaled_std_dev_token_count', '%_nans': 'scaled_perc_nans'})\n",
    "data1.loc[data1['scaled_mean_token_count'] > 10000, 'scaled_mean_token_count'] = 10000\n",
    "data1.loc[data1['scaled_mean_token_count'] < -10000, 'scaled_mean_token_count'] = -10000\n",
    "data1.loc[data1['scaled_std_dev_token_count'] > 10000, 'scaled_std_dev_token_count'] = 10000\n",
    "data1.loc[data1['scaled_std_dev_token_count'] < -10000, 'scaled_std_dev_token_count'] = -10000\n",
    "data1.loc[data1['scaled_perc_nans'] > 10000, 'scaled_perc_nans'] = 10000\n",
    "data1.loc[data1['scaled_perc_nans'] < -10000, 'scaled_perc_nans'] = -10000\n",
    "column_names_to_normalize = ['scaled_mean_token_count', 'scaled_std_dev_token_count','scaled_perc_nans']\n",
    "x = data1[column_names_to_normalize].values\n",
    "x = np.nan_to_num(x)\n",
    "x_scaled = StandardScaler().fit_transform(x)\n",
    "df_temp = pd.DataFrame(x_scaled, columns=column_names_to_normalize, index = data1.index)\n",
    "data1[column_names_to_normalize] = df_temp\n",
    "\n",
    "y.y_act = y.y_act.astype(float)\n",
    "\n",
    "print(f\"> Data mean: {data1.mean()}\")\n",
    "print(f\"> Data median: {data1.median()}\")\n",
    "print(f\"> Data stdev: {data1.std()}\")\n",
    "\n",
    "# data1.to_csv('before.csv')\n",
    "# f = open('current.txt','w')\n",
    "# d = enchant.Dict(\"en_US\")\n",
    "\n",
    "# for i in data.index:\n",
    "#     ival = data.at[i,'Attribute_name']\n",
    "#     if ival != 'id' and d.check(ivadf_tempdata1)\n",
    "#         print(f,ival)\n",
    "#         print(f,y.at[i,'y_act'])\n",
    "#         data1.at[i,'dictionary_item'] = 1\n",
    "#     else:\n",
    "#         data1.at[i,'dictionary_item'] = 0\n",
    "\n",
    "# data1.to_csv('after.csv')\n",
    "# f.close()\n",
    "# print(data1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===[VECTORIZATION]===\n",
      "> Length of vectorized feature_names: 8528\n",
      "X_train preview:      scaled_perc_nans  scaled_mean_token_count  scaled_std_dev_token_count  \\\n",
      "453         -0.653097                 0.686283                    3.364514   \n",
      "43          -0.653120                 0.162079                   -0.054513   \n",
      "133          1.978459                -0.148544                   -0.167108   \n",
      "205         -0.653120                -0.141062                   -0.175870   \n",
      "282         -0.653120                -0.148960                   -0.175870   \n",
      "\n",
      "     has_delimiters  0  1  2  3  4  5  ...   8518  8519  8520  8521  8522  \\\n",
      "453            True  0  0  0  0  0  0  ...      0     0     0     0     0   \n",
      "43             True  0  0  0  0  0  0  ...      0     0     0     0     0   \n",
      "133            True  0  0  0  0  0  0  ...      0     0     0     0     0   \n",
      "205           False  0  0  0  0  0  0  ...      0     0     0     0     0   \n",
      "282           False  0  0  0  0  0  0  ...      0     0     0     0     0   \n",
      "\n",
      "     8523  8524  8525  8526  8527  \n",
      "453     0     0     0     0     0  \n",
      "43      0     0     0     0     0  \n",
      "133     0     0     0     0     0  \n",
      "205     0     0     0     0     0  \n",
      "282     0     0     0     0     0  \n",
      "\n",
      "[5 rows x 18519 columns]\n",
      "y_train preview:      y_act\n",
      "453    1.0\n",
      "43     1.0\n",
      "133    2.0\n",
      "205    0.0\n",
      "282    0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"===[VECTORIZATION]===\")\n",
    "arr = data['Attribute_name'].values\n",
    "data = data.fillna(0)\n",
    "arr1 = data['sample_1'].values\n",
    "arr1 = [str(x) for x in arr1]\n",
    "arr2 = data['sample_2'].values\n",
    "arr2 = [str(x) for x in arr2]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(3, 3), analyzer='char')\n",
    "X = vectorizer.fit_transform(arr)\n",
    "X1 = vectorizer.fit_transform(arr1)\n",
    "X2 = vectorizer.fit_transform(arr2)\n",
    "\n",
    "print(f\"> Length of vectorized feature_names: {len(vectorizer.get_feature_names())}\")\n",
    "\n",
    "data1.to_csv('data/preprocessing/before.csv')\n",
    "tempdf = pd.DataFrame(X.toarray())\n",
    "tempdf1 = pd.DataFrame(X1.toarray())\n",
    "tempdf2 = pd.DataFrame(X2.toarray())\n",
    "\n",
    "data2 = pd.concat([data1, tempdf, tempdf1, tempdf2], axis=1, sort=False)\n",
    "data2.to_csv('data/preprocessing/after.csv')\n",
    "data2.head()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data2, y, test_size=0.2, random_state=100)\n",
    "\n",
    "# X_train_train, X_test_train,y_train_train,y_test_train = train_test_split(X_train,y_train, test_size=0.25)\n",
    "# print(X_train.head())\n",
    "# print(y_train.head())\n",
    "\n",
    "X_train_new = X_train.reset_index(drop=True)\n",
    "y_train_new = y_train.reset_index(drop=True)\n",
    "print(f\"X_train preview: {X_train.head()}\")\n",
    "print(f\"y_train preview: {y_train.head()}\")\n",
    "\n",
    "X_train_new = X_train_new.values\n",
    "y_train_new = y_train_new.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "> Best training score: 0.9594202898550724\n",
      "> Best test score: 0.7816091954022989\n",
      "> Best held score: 0.7798165137614679\n",
      "==========\n",
      "> Best training score: 0.9304347826086956\n",
      "> Best test score: 0.8160919540229885\n",
      "> Best held score: 0.8440366972477065\n",
      "==========\n",
      "> Best training score: 0.930635838150289\n",
      "> Best test score: 0.8488372093023255\n",
      "> Best held score: 0.8256880733944955\n",
      "==========\n",
      "> Best training score: 0.9479768786127167\n",
      "> Best test score: 0.7790697674418605\n",
      "> Best held score: 0.8073394495412844\n",
      "==========\n",
      "> Best training score: 0.9508670520231214\n",
      "> Best test score: 0.8023255813953488\n",
      "> Best held score: 0.8073394495412844\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "# logisticRegr = LogisticRegression(penalty='l2',multi_class='multinomial', solver='lbfgs',C = 100,max_iter=200)\n",
    "# logisticRegr = LogisticRegressionCV(cv=5,penalty='l2',multi_class='multinomial', solver='lbfgs',Cs = 1,max_iter=200)\n",
    "\n",
    "k = 5\n",
    "kf = KFold(n_splits=k)\n",
    "avg_train_acc, avg_test_acc = 0, 0\n",
    "\n",
    "val_arr = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]\n",
    "# bestPerformingModel = LogisticRegression(penalty='l2',multi_class='multinomial', solver='lbfgs',C = 1)\n",
    "# bestscore = 0\n",
    "# for val in val_arr:\n",
    "#     logisticRegr = LogisticRegression(penalty='l2',multi_class='multinomial', solver='lbfgs',C = val)\n",
    "#     avgsc = 0\n",
    "#     for train_index, test_index in kf.split(X_train_new):\n",
    "#         X_train_cur, X_test_cur = X_train_new[train_index], X_train_new[test_index]\n",
    "#         y_train_cur, y_test_cur = y_train_new[train_index], y_train_new[test_index]\n",
    "\n",
    "#         logisticRegr.fit(X_train_cur, y_train_cur)\n",
    "#         sc = logisticRegr.score(X_test_cur, y_test_cur)\n",
    "#         avgsc = avgsc + sc\n",
    "#     avgsc = avgsc/k\n",
    "#     print(avgsc)\n",
    "#     if bestscore < avgsc:\n",
    "#         bestscore = avgsc\n",
    "#         bestPerformingModel = logisticRegr\n",
    "#         print(bestPerformingModel)\n",
    "\n",
    "\n",
    "avgsc_lst, avgsc_train_lst, avgsc_hld_lst = [], [], []\n",
    "avgsc, avgsc_train, avgsc_hld = 0, 0, 0\n",
    "\n",
    "for train_index, test_index in kf.split(X_train_new):\n",
    "    X_train_cur, X_test_cur = X_train_new[train_index], X_train_new[test_index]\n",
    "    y_train_cur, y_test_cur = y_train_new[train_index], y_train_new[test_index]\n",
    "    X_train_train, X_val, y_train_train, y_val = train_test_split(\n",
    "        X_train_cur, y_train_cur, test_size=0.25, random_state=100)\n",
    "\n",
    "    bestPerformingModel = LogisticRegression(\n",
    "        penalty='l2', multi_class='multinomial', solver='lbfgs', C=1)\n",
    "    bestscore = 0\n",
    "    for val in val_arr:\n",
    "        clf = LogisticRegression(\n",
    "            penalty='l2', multi_class='multinomial', solver='lbfgs', C=val)\n",
    "        clf.fit(X_train_train, y_train_train)\n",
    "        sc = clf.score(X_val, y_val)\n",
    "        print(f\"[C: {val}, accuracy: {sc}]\")\n",
    "        if bestscore < sc:\n",
    "            bestscore = sc\n",
    "            bestPerformingModel = clf\n",
    "#                 print(bestPerformingModel)\n",
    "\n",
    "    bscr_train = bestPerformingModel.score(X_train_cur, y_train_cur)\n",
    "    bscr = bestPerformingModel.score(X_test_cur, y_test_cur)\n",
    "    bscr_hld = bestPerformingModel.score(X_test, y_test)\n",
    "\n",
    "    avgsc_train_lst.append(bscr_train)\n",
    "    avgsc_lst.append(bscr)\n",
    "    avgsc_hld_lst.append(bscr_hld)\n",
    "\n",
    "    avgsc_train = avgsc_train + bscr_train\n",
    "    avgsc = avgsc + bscr\n",
    "    avgsc_hld = avgsc_hld + bscr_hld\n",
    "    print('='*10)\n",
    "    print(f\"> Best training score: {bscr_train}\")\n",
    "    print(f\"> Best test score: {bscr}\")\n",
    "    print(f\"> Best held score: {bscr_hld}\")\n",
    "print('='*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Average training score list: [0.9594202898550724, 0.9304347826086956, 0.930635838150289, 0.9479768786127167, 0.9508670520231214]\n",
      "> Average testing score list: [0.7816091954022989, 0.8160919540229885, 0.8488372093023255, 0.7790697674418605, 0.8023255813953488]\n",
      "> Average held score list: [0.7798165137614679, 0.8440366972477065, 0.8256880733944955, 0.8073394495412844, 0.8073394495412844]\n",
      "\n",
      "> Average training score list/k: 0.9438669682499791\n",
      "> Average testing score list/k: 0.8055867415129645\n",
      "> Average held score list/k: 0.8128440366972477\n",
      "\n",
      "Confusion Matrix: Actual (Row) vs Predicted (Column)\n",
      "[[24  0  3  0  0  0]\n",
      " [ 0 13  9  0  0  0]\n",
      " [ 1  1 49  0  0  1]\n",
      " [ 0  0  0  2  0  0]\n",
      " [ 0  0  1  0  0  0]\n",
      " [ 0  1  3  1  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"> Average training score list: {avgsc_train_lst}\")\n",
    "print(f\"> Average testing score list: {avgsc_lst}\")\n",
    "print(f\"> Average held score list: {avgsc_hld_lst}\")\n",
    "print()\n",
    "print(f\"> Average training score list/k: {avgsc_train/k}\")\n",
    "print(f\"> Average testing score list/k: {avgsc/k}\")\n",
    "print(f\"> Average held score list/k: {avgsc_hld/k}\")\n",
    "print()\n",
    "y_pred = bestPerformingModel.predict(X_test)\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix: Actual (Row) vs Predicted (Column)')\n",
    "print(cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Y probabilities: [[1.77481015e-001 1.11523637e-001 5.61645555e-001 1.65324400e-002\n",
      "  9.95860204e-002 3.32313337e-002]\n",
      " [1.64705375e-005 1.45111174e-004 2.56612939e-005 9.99735295e-001\n",
      "  2.91467002e-005 4.83148543e-005]\n",
      " [9.95124649e-001 8.90535645e-004 2.01609320e-003 3.28124846e-004\n",
      "  8.95781674e-004 7.44815716e-004]\n",
      " [9.74627350e-001 3.22952839e-003 1.24726633e-002 1.16290058e-003\n",
      "  5.81094725e-003 2.69661004e-003]\n",
      " [1.58428141e-001 1.02213113e-001 5.34566275e-001 1.67023191e-002\n",
      "  1.15209643e-001 7.28805087e-002]\n",
      " [2.49667932e-001 2.64809080e-002 7.02132939e-001 1.95932113e-003\n",
      "  1.65326998e-002 3.22620050e-003]\n",
      " [5.42382483e-003 5.05286387e-003 9.80559643e-001 5.39674123e-004\n",
      "  7.47298765e-003 9.51006475e-004]\n",
      " [9.96115043e-001 7.48212406e-004 1.54638634e-003 2.65382754e-004\n",
      "  7.76166742e-004 5.48808621e-004]\n",
      " [8.98285320e-001 1.53749231e-002 3.93950505e-002 4.14865581e-003\n",
      "  3.15166614e-002 1.12793896e-002]\n",
      " [4.79198127e-002 4.10762684e-001 4.89999760e-001 8.01779796e-003\n",
      "  1.84632127e-002 2.48367331e-002]\n",
      " [2.98808637e-001 4.29948040e-002 5.25742137e-001 1.16913382e-002\n",
      "  6.98339233e-002 5.09291605e-002]\n",
      " [1.40169300e-002 8.00998958e-001 1.60983719e-001 3.50284077e-003\n",
      "  4.02113356e-003 1.64764190e-002]\n",
      " [9.98972542e-001 1.28233570e-004 4.01231648e-004 7.93746296e-005\n",
      "  2.75750691e-004 1.42867626e-004]\n",
      " [9.82676476e-001 2.31474043e-003 8.32332369e-003 1.00696924e-003\n",
      "  3.48384749e-003 2.19464361e-003]\n",
      " [2.21679981e-001 9.83870504e-002 4.77106680e-001 1.76379816e-002\n",
      "  1.49791319e-001 3.53969871e-002]\n",
      " [2.58592965e-001 4.38776122e-002 6.12153761e-001 7.47437077e-003\n",
      "  5.80276462e-002 1.98736449e-002]\n",
      " [1.36512877e-016 9.99999991e-001 8.63890318e-009 5.78229219e-011\n",
      "  7.95910346e-015 1.44267226e-010]\n",
      " [1.45994709e-001 8.25794414e-002 6.43797561e-001 9.92077130e-003\n",
      "  8.46272900e-002 3.30802282e-002]\n",
      " [1.27419352e-001 1.37260907e-001 5.90459922e-001 1.02269663e-002\n",
      "  1.04684940e-001 2.99479124e-002]\n",
      " [1.65664734e-001 3.23241677e-001 3.43137732e-001 1.42693283e-002\n",
      "  1.11233253e-001 4.24532751e-002]\n",
      " [1.99227731e-001 6.79099626e-002 5.81113743e-001 1.14687250e-002\n",
      "  1.14173311e-001 2.61065269e-002]\n",
      " [3.43159477e-001 6.21486429e-002 3.89901804e-001 1.38004396e-002\n",
      "  1.57579082e-001 3.34105547e-002]\n",
      " [4.85337999e-016 9.99997995e-001 2.00076291e-006 4.78452548e-010\n",
      "  1.15387936e-014 3.47117651e-009]\n",
      " [9.74190107e-001 2.24848023e-003 1.73581861e-002 8.58398433e-004\n",
      "  3.07612487e-003 2.26870293e-003]\n",
      " [1.54635122e-001 5.96888634e-002 7.50755170e-001 2.83511033e-003\n",
      "  2.66983039e-002 5.38743035e-003]\n",
      " [1.22176747e-001 3.78457638e-002 7.38541087e-001 1.87451616e-002\n",
      "  6.25608162e-002 2.01304242e-002]\n",
      " [2.15410950e-001 4.19474152e-001 2.74621976e-001 9.25374673e-003\n",
      "  5.61269400e-002 2.51122347e-002]\n",
      " [3.35637700e-002 3.45423997e-002 8.78079373e-001 2.23387302e-003\n",
      "  4.74051432e-002 4.17544084e-003]\n",
      " [9.99564686e-001 4.50818129e-005 2.26483505e-004 3.16710724e-005\n",
      "  4.49808205e-005 8.70966431e-005]\n",
      " [9.53760874e-001 5.12149521e-003 1.52299208e-002 1.74196610e-003\n",
      "  2.06820715e-002 3.46367221e-003]\n",
      " [5.42382483e-003 5.05286387e-003 9.80559643e-001 5.39674123e-004\n",
      "  7.47298765e-003 9.51006475e-004]\n",
      " [9.99583878e-001 1.04214585e-004 1.14337330e-004 4.19829246e-005\n",
      "  8.08659252e-005 7.47212308e-005]\n",
      " [1.85560499e-006 9.91163039e-001 8.58762766e-003 3.08394144e-005\n",
      "  8.21199639e-006 2.08426044e-004]\n",
      " [9.23704421e-001 4.57995262e-003 5.97179072e-002 1.70187184e-003\n",
      "  6.18225403e-003 4.11359324e-003]\n",
      " [4.54444252e-003 2.42683878e-001 7.40563176e-001 1.17688646e-003\n",
      "  3.51063391e-003 7.52098337e-003]\n",
      " [3.86029399e-002 3.20971342e-002 8.89964835e-001 2.05177652e-003\n",
      "  3.30336006e-002 4.24971417e-003]\n",
      " [1.34905992e-001 3.01267757e-002 7.76898648e-001 5.86656560e-003\n",
      "  3.92888172e-002 1.29132006e-002]\n",
      " [5.85451638e-003 3.20044113e-003 9.85112041e-001 3.99637245e-004\n",
      "  4.56586179e-003 8.67502184e-004]\n",
      " [4.79018173e-002 1.97746498e-002 8.97250336e-001 3.19487581e-003\n",
      "  2.11100463e-002 1.07682749e-002]\n",
      " [9.98907181e-001 1.96127821e-004 1.48572564e-004 8.25828838e-005\n",
      "  5.09310204e-004 1.56225281e-004]\n",
      " [6.00168006e-003 5.35562393e-003 9.78946560e-001 5.85830931e-004\n",
      "  8.07013169e-003 1.04017339e-003]\n",
      " [4.82054683e-003 2.74329642e-003 9.88671320e-001 2.90113565e-004\n",
      "  2.87958206e-003 5.95141028e-004]\n",
      " [8.53533059e-002 8.25980929e-003 8.92087776e-001 1.08441765e-003\n",
      "  1.08604037e-002 2.35428700e-003]\n",
      " [8.38771290e-002 1.80838708e-002 8.47176920e-001 4.00018790e-003\n",
      "  3.59765650e-002 1.08853277e-002]\n",
      " [7.28272564e-001 2.31184582e-002 1.93717216e-001 3.65040053e-003\n",
      "  4.18710196e-002 9.37034219e-003]\n",
      " [1.45566243e-001 5.12029421e-002 6.60975830e-001 8.06541629e-003\n",
      "  1.15558528e-001 1.86310407e-002]\n",
      " [1.12577548e-001 1.29400926e-001 6.71391310e-001 1.08866100e-002\n",
      "  4.71813790e-002 2.85622262e-002]\n",
      " [9.60263786e-001 4.73226296e-003 2.18069747e-002 2.73621819e-003\n",
      "  6.47443985e-003 3.98631834e-003]\n",
      " [8.85523201e-001 1.16179638e-002 7.24153464e-002 3.41521505e-003\n",
      "  1.93028954e-002 7.72537848e-003]\n",
      " [1.23827695e-001 1.64106648e-001 5.91099657e-001 1.35893678e-002\n",
      "  7.77381935e-002 2.96384387e-002]\n",
      " [1.69797076e-010 9.99964613e-001 3.16696702e-005 3.55206114e-007\n",
      "  1.10721196e-009 3.36090754e-006]\n",
      " [1.33445073e-002 7.79988667e-003 9.67260850e-001 8.32977200e-004\n",
      "  8.61023467e-003 2.15154452e-003]\n",
      " [2.78062465e-003 2.80853236e-003 4.16314553e-003 9.85873138e-001\n",
      "  2.89761235e-003 1.47694754e-003]\n",
      " [8.07288868e-003 7.62148570e-001 2.08271623e-001 3.76083523e-003\n",
      "  5.77358154e-003 1.19725018e-002]\n",
      " [9.79178726e-001 3.70808937e-003 8.57484880e-003 1.16517964e-003\n",
      "  4.92582727e-003 2.44732884e-003]\n",
      " [5.29276020e-003 3.08901107e-003 9.86153285e-001 3.60805710e-004\n",
      "  4.33400219e-003 7.70135880e-004]\n",
      " [2.57525117e-001 1.13191770e-001 5.62806340e-001 5.99622939e-003\n",
      "  4.68763781e-002 1.36041647e-002]\n",
      " [3.12019933e-001 4.86679777e-002 3.89784193e-001 1.21044135e-002\n",
      "  2.12178514e-001 2.52449687e-002]\n",
      " [5.07257050e-003 4.57510493e-003 9.82113342e-001 5.16651264e-004\n",
      "  6.80727202e-003 9.15059585e-004]\n",
      " [1.35161985e-001 1.04645545e-001 6.44843247e-001 9.54009158e-003\n",
      "  8.56760166e-002 2.01331154e-002]\n",
      " [8.60105145e-001 1.53950374e-002 1.02822885e-001 2.60036840e-003\n",
      "  1.41639802e-002 4.91258454e-003]\n",
      " [1.83164333e-001 7.34085717e-002 6.24161816e-001 9.82621115e-003\n",
      "  7.46496771e-002 3.47893908e-002]\n",
      " [1.45058772e-001 2.44285191e-002 7.79346434e-001 4.71385623e-003\n",
      "  3.06454001e-002 1.58070177e-002]\n",
      " [1.51572520e-001 1.34242540e-001 4.99789335e-001 1.37905168e-002\n",
      "  7.10839136e-002 1.29521175e-001]\n",
      " [1.26075924e-002 9.70790688e-001 7.60722569e-003 1.22748372e-003\n",
      "  4.33511374e-003 3.43189689e-003]\n",
      " [3.71513299e-001 3.30567233e-002 5.57714608e-001 3.49195787e-003\n",
      "  2.77227352e-002 6.50067673e-003]\n",
      " [2.52461490e-008 9.98176906e-001 1.77696474e-003 4.75049079e-006\n",
      "  4.85962036e-008 4.13049869e-005]\n",
      " [7.28038051e-003 3.57751759e-003 9.82244799e-001 4.92943041e-004\n",
      "  5.30464986e-003 1.09971036e-003]\n",
      " [6.41230952e-002 2.68019660e-002 8.55465977e-001 2.43794382e-003\n",
      "  4.55285859e-002 5.64243212e-003]\n",
      " [2.66804707e-001 1.36482232e-001 3.96851919e-001 1.36223144e-002\n",
      "  1.55368222e-001 3.08706062e-002]\n",
      " [5.38872245e-025 2.04984530e-009 9.71923749e-018 1.50925344e-003\n",
      "  3.08333672e-023 9.98490745e-001]\n",
      " [9.40381577e-001 8.43661270e-003 1.54170741e-002 2.31819451e-003\n",
      "  2.86848003e-002 4.76174179e-003]\n",
      " [1.32198908e-256 1.00000000e+000 7.11578073e-114 1.80517414e-139\n",
      "  4.02144199e-222 9.36065223e-124]\n",
      " [9.66070387e-001 4.56205642e-003 1.83196265e-002 1.46529369e-003\n",
      "  6.32098535e-003 3.26165148e-003]\n",
      " [1.15080790e-002 1.02490216e-002 9.59856024e-001 9.51432320e-004\n",
      "  1.57437476e-002 1.69169562e-003]\n",
      " [4.16380909e-002 4.36720057e-001 3.90323945e-001 8.86832898e-003\n",
      "  9.84936883e-002 2.39558895e-002]\n",
      " [1.35565051e-003 7.32153990e-001 2.57243020e-001 2.28987144e-003\n",
      "  1.59134607e-003 5.36612174e-003]\n",
      " [1.05993894e-001 1.98062589e-001 5.63948569e-001 1.76929137e-002\n",
      "  7.44182012e-002 3.98838325e-002]\n",
      " [2.22586654e-001 4.25679587e-002 6.21822968e-001 8.52696510e-003\n",
      "  7.69980132e-002 2.74974416e-002]\n",
      " [8.72115484e-001 1.07007652e-002 5.37425789e-002 3.45499931e-003\n",
      "  5.10709013e-002 8.91527131e-003]\n",
      " [1.77344106e-001 1.08939257e-001 5.71396144e-001 1.26648258e-002\n",
      "  1.00116930e-001 2.95387376e-002]\n",
      " [1.11119007e-001 5.04938519e-002 5.76743871e-001 7.58448102e-003\n",
      "  2.38131126e-001 1.59276623e-002]\n",
      " [1.82949569e-002 7.68181458e-003 9.57726897e-001 1.75613372e-003\n",
      "  9.14737688e-003 5.39282061e-003]\n",
      " [9.60440647e-001 4.50039429e-003 1.29250434e-002 1.54471385e-003\n",
      "  1.75308821e-002 3.05831950e-003]\n",
      " [2.09946274e-001 7.49534626e-002 5.54566725e-001 2.96801668e-002\n",
      "  8.77093754e-002 4.31439953e-002]\n",
      " [1.03642738e-001 3.59806280e-002 8.18081978e-001 4.57692125e-003\n",
      "  2.46790190e-002 1.30387154e-002]\n",
      " [1.33985718e-001 3.59531934e-001 3.88701092e-001 9.20898527e-003\n",
      "  3.53965986e-002 7.31756718e-002]\n",
      " [1.05690959e-001 3.27626379e-002 7.85483444e-001 5.85925481e-003\n",
      "  5.42297198e-002 1.59739849e-002]\n",
      " [1.26066141e-002 9.70792912e-001 7.60656892e-003 1.22740861e-003\n",
      "  4.33480052e-003 3.43169584e-003]\n",
      " [2.33475733e-002 1.10746311e-001 8.33232637e-001 4.63032776e-003\n",
      "  2.07407450e-002 7.30240552e-003]\n",
      " [6.65331960e-001 1.38951038e-002 2.58053678e-001 5.88233990e-003\n",
      "  4.73267548e-002 9.51016375e-003]\n",
      " [9.17028328e-001 8.45120160e-003 5.53154602e-002 1.81562211e-003\n",
      "  1.36231451e-002 3.76624250e-003]\n",
      " [6.19002574e-002 6.65324700e-002 7.69959807e-001 6.59982319e-003\n",
      "  2.02907118e-002 7.47169304e-002]\n",
      " [6.33997743e-002 7.95782473e-002 7.40787081e-001 6.64602288e-003\n",
      "  8.33360836e-002 2.62527908e-002]\n",
      " [2.08228905e-001 7.46384675e-002 6.13052713e-001 8.97745251e-003\n",
      "  7.00718075e-002 2.50306544e-002]\n",
      " [2.63735850e-001 5.94784089e-002 4.49773729e-001 1.27482019e-002\n",
      "  1.81523607e-001 3.27402041e-002]\n",
      " [1.32702319e-001 5.83377930e-002 6.67025319e-001 8.08864387e-003\n",
      "  1.09006640e-001 2.48392852e-002]\n",
      " [2.57007072e-002 1.08059258e-001 7.81248469e-001 3.84126252e-002\n",
      "  2.09213976e-002 2.56575429e-002]\n",
      " [3.69541856e-003 1.78121615e-003 9.88038652e-001 6.19025553e-004\n",
      "  3.81037547e-003 2.05531255e-003]\n",
      " [2.14231690e-002 8.91207599e-001 7.22638210e-002 1.88460210e-003\n",
      "  4.30565609e-003 8.91515239e-003]\n",
      " [9.78471490e-001 4.15552585e-003 8.59938981e-003 1.13176418e-003\n",
      "  5.30095510e-003 2.34087513e-003]\n",
      " [1.80292130e-002 1.55239546e-002 5.39944903e-002 8.86608533e-001\n",
      "  1.44486233e-002 1.13951860e-002]\n",
      " [5.57821536e-003 5.11250329e-003 9.80151675e-001 5.50104517e-004\n",
      "  7.63700917e-003 9.70492720e-004]\n",
      " [8.56019126e-003 4.75560568e-003 9.77947539e-001 5.22708438e-004\n",
      "  7.08592003e-003 1.12803567e-003]\n",
      " [6.27265136e-002 2.92460029e-001 5.71151074e-001 7.61369334e-003\n",
      "  1.93235355e-002 4.67251553e-002]\n",
      " [4.90251487e-002 4.88317757e-001 3.72476530e-001 1.41132750e-002\n",
      "  2.19820156e-002 5.40852738e-002]\n",
      " [4.51389811e-003 2.79974815e-003 9.87473244e-001 3.89468502e-004\n",
      "  4.16604661e-003 6.57594713e-004]\n",
      " [9.73386776e-001 4.31015448e-003 1.21362483e-002 1.37913757e-003\n",
      "  5.66356421e-003 3.12411917e-003]\n",
      " [4.86670018e-003 4.51496010e-003 9.82612921e-001 4.92765034e-004\n",
      "  6.64680937e-003 8.65844017e-004]]\n",
      "                0         1             2             3             4  \\\n",
      "0    1.774810e-01  0.111524  5.616456e-01  1.653244e-02  9.958602e-02   \n",
      "1    1.647054e-05  0.000145  2.566129e-05  9.997353e-01  2.914670e-05   \n",
      "2    9.951246e-01  0.000891  2.016093e-03  3.281248e-04  8.957817e-04   \n",
      "3    9.746274e-01  0.003230  1.247266e-02  1.162901e-03  5.810947e-03   \n",
      "4    1.584281e-01  0.102213  5.345663e-01  1.670232e-02  1.152096e-01   \n",
      "5    2.496679e-01  0.026481  7.021329e-01  1.959321e-03  1.653270e-02   \n",
      "6    5.423825e-03  0.005053  9.805596e-01  5.396741e-04  7.472988e-03   \n",
      "7    9.961150e-01  0.000748  1.546386e-03  2.653828e-04  7.761667e-04   \n",
      "8    8.982853e-01  0.015375  3.939505e-02  4.148656e-03  3.151666e-02   \n",
      "9    4.791981e-02  0.410763  4.899998e-01  8.017798e-03  1.846321e-02   \n",
      "10   2.988086e-01  0.042995  5.257421e-01  1.169134e-02  6.983392e-02   \n",
      "11   1.401693e-02  0.800999  1.609837e-01  3.502841e-03  4.021134e-03   \n",
      "12   9.989725e-01  0.000128  4.012316e-04  7.937463e-05  2.757507e-04   \n",
      "13   9.826765e-01  0.002315  8.323324e-03  1.006969e-03  3.483847e-03   \n",
      "14   2.216800e-01  0.098387  4.771067e-01  1.763798e-02  1.497913e-01   \n",
      "15   2.585930e-01  0.043878  6.121538e-01  7.474371e-03  5.802765e-02   \n",
      "16   1.365129e-16  1.000000  8.638903e-09  5.782292e-11  7.959103e-15   \n",
      "17   1.459947e-01  0.082579  6.437976e-01  9.920771e-03  8.462729e-02   \n",
      "18   1.274194e-01  0.137261  5.904599e-01  1.022697e-02  1.046849e-01   \n",
      "19   1.656647e-01  0.323242  3.431377e-01  1.426933e-02  1.112333e-01   \n",
      "20   1.992277e-01  0.067910  5.811137e-01  1.146873e-02  1.141733e-01   \n",
      "21   3.431595e-01  0.062149  3.899018e-01  1.380044e-02  1.575791e-01   \n",
      "22   4.853380e-16  0.999998  2.000763e-06  4.784525e-10  1.153879e-14   \n",
      "23   9.741901e-01  0.002248  1.735819e-02  8.583984e-04  3.076125e-03   \n",
      "24   1.546351e-01  0.059689  7.507552e-01  2.835110e-03  2.669830e-02   \n",
      "25   1.221767e-01  0.037846  7.385411e-01  1.874516e-02  6.256082e-02   \n",
      "26   2.154109e-01  0.419474  2.746220e-01  9.253747e-03  5.612694e-02   \n",
      "27   3.356377e-02  0.034542  8.780794e-01  2.233873e-03  4.740514e-02   \n",
      "28   9.995647e-01  0.000045  2.264835e-04  3.167107e-05  4.498082e-05   \n",
      "29   9.537609e-01  0.005121  1.522992e-02  1.741966e-03  2.068207e-02   \n",
      "..            ...       ...           ...           ...           ...   \n",
      "79   8.721155e-01  0.010701  5.374258e-02  3.454999e-03  5.107090e-02   \n",
      "80   1.773441e-01  0.108939  5.713961e-01  1.266483e-02  1.001169e-01   \n",
      "81   1.111190e-01  0.050494  5.767439e-01  7.584481e-03  2.381311e-01   \n",
      "82   1.829496e-02  0.007682  9.577269e-01  1.756134e-03  9.147377e-03   \n",
      "83   9.604406e-01  0.004500  1.292504e-02  1.544714e-03  1.753088e-02   \n",
      "84   2.099463e-01  0.074953  5.545667e-01  2.968017e-02  8.770938e-02   \n",
      "85   1.036427e-01  0.035981  8.180820e-01  4.576921e-03  2.467902e-02   \n",
      "86   1.339857e-01  0.359532  3.887011e-01  9.208985e-03  3.539660e-02   \n",
      "87   1.056910e-01  0.032763  7.854834e-01  5.859255e-03  5.422972e-02   \n",
      "88   1.260661e-02  0.970793  7.606569e-03  1.227409e-03  4.334801e-03   \n",
      "89   2.334757e-02  0.110746  8.332326e-01  4.630328e-03  2.074074e-02   \n",
      "90   6.653320e-01  0.013895  2.580537e-01  5.882340e-03  4.732675e-02   \n",
      "91   9.170283e-01  0.008451  5.531546e-02  1.815622e-03  1.362315e-02   \n",
      "92   6.190026e-02  0.066532  7.699598e-01  6.599823e-03  2.029071e-02   \n",
      "93   6.339977e-02  0.079578  7.407871e-01  6.646023e-03  8.333608e-02   \n",
      "94   2.082289e-01  0.074638  6.130527e-01  8.977453e-03  7.007181e-02   \n",
      "95   2.637358e-01  0.059478  4.497737e-01  1.274820e-02  1.815236e-01   \n",
      "96   1.327023e-01  0.058338  6.670253e-01  8.088644e-03  1.090066e-01   \n",
      "97   2.570071e-02  0.108059  7.812485e-01  3.841263e-02  2.092140e-02   \n",
      "98   3.695419e-03  0.001781  9.880387e-01  6.190256e-04  3.810375e-03   \n",
      "99   2.142317e-02  0.891208  7.226382e-02  1.884602e-03  4.305656e-03   \n",
      "100  9.784715e-01  0.004156  8.599390e-03  1.131764e-03  5.300955e-03   \n",
      "101  1.802921e-02  0.015524  5.399449e-02  8.866085e-01  1.444862e-02   \n",
      "102  5.578215e-03  0.005113  9.801517e-01  5.501045e-04  7.637009e-03   \n",
      "103  8.560191e-03  0.004756  9.779475e-01  5.227084e-04  7.085920e-03   \n",
      "104  6.272651e-02  0.292460  5.711511e-01  7.613693e-03  1.932354e-02   \n",
      "105  4.902515e-02  0.488318  3.724765e-01  1.411327e-02  2.198202e-02   \n",
      "106  4.513898e-03  0.002800  9.874732e-01  3.894685e-04  4.166047e-03   \n",
      "107  9.733868e-01  0.004310  1.213625e-02  1.379138e-03  5.663564e-03   \n",
      "108  4.866700e-03  0.004515  9.826129e-01  4.927650e-04  6.646809e-03   \n",
      "\n",
      "                5  \n",
      "0    3.323133e-02  \n",
      "1    4.831485e-05  \n",
      "2    7.448157e-04  \n",
      "3    2.696610e-03  \n",
      "4    7.288051e-02  \n",
      "5    3.226200e-03  \n",
      "6    9.510065e-04  \n",
      "7    5.488086e-04  \n",
      "8    1.127939e-02  \n",
      "9    2.483673e-02  \n",
      "10   5.092916e-02  \n",
      "11   1.647642e-02  \n",
      "12   1.428676e-04  \n",
      "13   2.194644e-03  \n",
      "14   3.539699e-02  \n",
      "15   1.987364e-02  \n",
      "16   1.442672e-10  \n",
      "17   3.308023e-02  \n",
      "18   2.994791e-02  \n",
      "19   4.245328e-02  \n",
      "20   2.610653e-02  \n",
      "21   3.341055e-02  \n",
      "22   3.471177e-09  \n",
      "23   2.268703e-03  \n",
      "24   5.387430e-03  \n",
      "25   2.013042e-02  \n",
      "26   2.511223e-02  \n",
      "27   4.175441e-03  \n",
      "28   8.709664e-05  \n",
      "29   3.463672e-03  \n",
      "..            ...  \n",
      "79   8.915271e-03  \n",
      "80   2.953874e-02  \n",
      "81   1.592766e-02  \n",
      "82   5.392821e-03  \n",
      "83   3.058320e-03  \n",
      "84   4.314400e-02  \n",
      "85   1.303872e-02  \n",
      "86   7.317567e-02  \n",
      "87   1.597398e-02  \n",
      "88   3.431696e-03  \n",
      "89   7.302406e-03  \n",
      "90   9.510164e-03  \n",
      "91   3.766242e-03  \n",
      "92   7.471693e-02  \n",
      "93   2.625279e-02  \n",
      "94   2.503065e-02  \n",
      "95   3.274020e-02  \n",
      "96   2.483929e-02  \n",
      "97   2.565754e-02  \n",
      "98   2.055313e-03  \n",
      "99   8.915152e-03  \n",
      "100  2.340875e-03  \n",
      "101  1.139519e-02  \n",
      "102  9.704927e-04  \n",
      "103  1.128036e-03  \n",
      "104  4.672516e-02  \n",
      "105  5.408527e-02  \n",
      "106  6.575947e-04  \n",
      "107  3.124119e-03  \n",
      "108  8.658440e-04  \n",
      "\n",
      "[109 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# save the model to disk\n",
    "filename = 'data/pretrained/lr_finalized_model.pickle'\n",
    "pickle.dump(bestPerformingModel, open(filename, 'wb+'))\n",
    "\n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb+'))\n",
    "result = loaded_model.score(X_test, y_test)\n",
    "y_prob = bestPerformingModel.predict_proba(X_test)\n",
    "\n",
    "df = pd.DataFrame.from_records(y_prob)\n",
    "print(df)\n",
    "df.to_csv('data/model_predictions/lr_predictions.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
