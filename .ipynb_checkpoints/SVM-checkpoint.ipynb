{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# from keras.utils import to_categorical\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import pickle\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv\n",
    "dict_label = {\n",
    "    'Datetime':0, \n",
    "    'Sentence':1, \n",
    "    'Custom Object': 2, \n",
    "    'URL': 3, \n",
    "    'Numbers': 4, \n",
    "    'List': 5}\n",
    "data = pd.read_csv('data/needs_extraction_data/labelled_data.csv')\n",
    "\n",
    "data['y_act'] = [dict_label[i] for i in data['y_act']]\n",
    "y = data.loc[:,['y_act']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Data mean: \n",
      "scaled_perc_nans             -2.745801e-16\n",
      "scaled_mean_token_count      -1.117919e-16\n",
      "scaled_std_dev_token_count   -2.236863e-17\n",
      "has_delimiters                3.105360e-01\n",
      "dtype: float64\n",
      "> Data median: \n",
      "scaled_perc_nans             -0.653046\n",
      "scaled_mean_token_count      -0.144106\n",
      "scaled_std_dev_token_count   -0.171320\n",
      "has_delimiters                0.000000\n",
      "dtype: float64\n",
      "> Data stdev: \n",
      "scaled_perc_nans              1.000925\n",
      "scaled_mean_token_count       1.000925\n",
      "scaled_std_dev_token_count    1.000925\n",
      "has_delimiters                0.463141\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data1 = data[['%_nans', 'mean_word_count', 'std_dev_word_count', 'has_delimiters']]\n",
    "data1 = data1.fillna(0)\n",
    "\n",
    "data1 = data1.rename(columns={'mean_word_count': 'scaled_mean_token_count', 'std_dev_word_count': 'scaled_std_dev_token_count', '%_nans': 'scaled_perc_nans'})\n",
    "data1.loc[data1['scaled_mean_token_count'] > 10000, 'scaled_mean_token_count'] = 10000\n",
    "data1.loc[data1['scaled_mean_token_count'] < -10000, 'scaled_mean_token_count'] = -10000\n",
    "data1.loc[data1['scaled_std_dev_token_count'] > 10000, 'scaled_std_dev_token_count'] = 10000\n",
    "data1.loc[data1['scaled_std_dev_token_count'] < -10000, 'scaled_std_dev_token_count'] = -10000\n",
    "data1.loc[data1['scaled_perc_nans'] > 10000, 'scaled_perc_nans'] = 10000\n",
    "data1.loc[data1['scaled_perc_nans'] < -10000, 'scaled_perc_nans'] = -10000\n",
    "column_names_to_normalize = ['scaled_mean_token_count', 'scaled_std_dev_token_count','scaled_perc_nans']\n",
    "x = data1[column_names_to_normalize].values\n",
    "x = np.nan_to_num(x)\n",
    "x_scaled = StandardScaler().fit_transform(x)\n",
    "df_temp = pd.DataFrame(x_scaled, columns=column_names_to_normalize, index = data1.index)\n",
    "data1[column_names_to_normalize] = df_temp\n",
    "\n",
    "y.y_act = y.y_act.astype(float)\n",
    "\n",
    "print(f\"> Data mean: \\n{data1.mean()}\")\n",
    "print(f\"> Data median: \\n{data1.median()}\")\n",
    "print(f\"> Data stdev: \\n{data1.std()}\")\n",
    "\n",
    "# data1.to_csv('before.csv')\n",
    "# f = open('current.txt','w')\n",
    "# d = enchant.Dict(\"en_US\")\n",
    "\n",
    "# for i in data.index:\n",
    "#     ival = data.at[i,'Attribute_name']\n",
    "#     if ival != 'id' and d.check(ivadf_tempdata1)\n",
    "#         print(f,ival)\n",
    "#         print(f,y.at[i,'y_act'])\n",
    "#         data1.at[i,'dictionary_item'] = 1\n",
    "#     else:\n",
    "#         data1.at[i,'dictionary_item'] = 0\n",
    "\n",
    "# data1.to_csv('after.csv')\n",
    "# f.close()\n",
    "# print(data1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===[VECTORIZATION]===\n",
      "> Length of vectorized feature_names: 8528\n",
      "X_train preview:      scaled_perc_nans  scaled_mean_token_count  scaled_std_dev_token_count  \\\n",
      "453         -0.653097                 0.686283                    3.364514   \n",
      "43          -0.653120                 0.162079                   -0.054513   \n",
      "133          1.978459                -0.148544                   -0.167108   \n",
      "205         -0.653120                -0.141062                   -0.175870   \n",
      "282         -0.653120                -0.148960                   -0.175870   \n",
      "\n",
      "     has_delimiters  0  1  2  3  4  5  ...   8518  8519  8520  8521  8522  \\\n",
      "453            True  0  0  0  0  0  0  ...      0     0     0     0     0   \n",
      "43             True  0  0  0  0  0  0  ...      0     0     0     0     0   \n",
      "133            True  0  0  0  0  0  0  ...      0     0     0     0     0   \n",
      "205           False  0  0  0  0  0  0  ...      0     0     0     0     0   \n",
      "282           False  0  0  0  0  0  0  ...      0     0     0     0     0   \n",
      "\n",
      "     8523  8524  8525  8526  8527  \n",
      "453     0     0     0     0     0  \n",
      "43      0     0     0     0     0  \n",
      "133     0     0     0     0     0  \n",
      "205     0     0     0     0     0  \n",
      "282     0     0     0     0     0  \n",
      "\n",
      "[5 rows x 18519 columns]\n",
      "y_train preview:      y_act\n",
      "453    1.0\n",
      "43     1.0\n",
      "133    2.0\n",
      "205    0.0\n",
      "282    0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"===[VECTORIZATION]===\")\n",
    "arr = data['Attribute_name'].values\n",
    "data = data.fillna(0)\n",
    "arr1 = data['sample_1'].values\n",
    "arr1 = [str(x) for x in arr1]\n",
    "arr2 = data['sample_2'].values\n",
    "arr2 = [str(x) for x in arr2]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(3, 3), analyzer='char')\n",
    "X = vectorizer.fit_transform(arr)\n",
    "X1 = vectorizer.fit_transform(arr1)\n",
    "X2 = vectorizer.fit_transform(arr2)\n",
    "\n",
    "print(f\"> Length of vectorized feature_names: {len(vectorizer.get_feature_names())}\")\n",
    "\n",
    "data1.to_csv('data/preprocessing/before.csv')\n",
    "tempdf = pd.DataFrame(X.toarray())\n",
    "tempdf1 = pd.DataFrame(X1.toarray())\n",
    "tempdf2 = pd.DataFrame(X2.toarray())\n",
    "\n",
    "data2 = pd.concat([data1, tempdf, tempdf1, tempdf2], axis=1, sort=False)\n",
    "data2.to_csv('data/preprocessing/after.csv')\n",
    "data2.head()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data2, y, test_size=0.2, random_state=100)\n",
    "atr_train,atr_test = train_test_split(data2, test_size=0.2,random_state=100)\n",
    "\n",
    "# X_train_train, X_test_train,y_train_train,y_test_train = train_test_split(X_train,y_train, test_size=0.25)\n",
    "# print(X_train.head())\n",
    "# print(y_train.head())\n",
    "\n",
    "X_train_new = X_train.reset_index(drop=True)\n",
    "y_train_new = y_train.reset_index(drop=True)\n",
    "print(f\"X_train preview: {X_train.head()}\")\n",
    "print(f\"y_train preview: {y_train.head()}\")\n",
    "\n",
    "X_train_new = X_train_new.values\n",
    "y_train_new = y_train_new.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "> Best training score: 0.9623188405797102\n",
      "> Best test score: 0.7701149425287356\n",
      "> Best held score: 0.8165137614678899\n",
      "==========\n",
      "> Best training score: 0.9420289855072463\n",
      "> Best test score: 0.7931034482758621\n",
      "> Best held score: 0.8532110091743119\n",
      "==========\n",
      "> Best training score: 0.9364161849710982\n",
      "> Best test score: 0.7906976744186046\n",
      "> Best held score: 0.8532110091743119\n",
      "==========\n",
      "> Best training score: 0.9479768786127167\n",
      "> Best test score: 0.7906976744186046\n",
      "> Best held score: 0.8532110091743119\n",
      "==========\n",
      "> Best training score: 0.9479768786127167\n",
      "> Best test score: 0.8023255813953488\n",
      "> Best held score: 0.8165137614678899\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "kf = KFold(n_splits=k)\n",
    "avg_train_acc, avg_test_acc = 0, 0\n",
    "\n",
    "cvals = [0.1, 1, 10, 100, 1000]\n",
    "gamavals = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "\n",
    "\n",
    "avgsc_lst, avgsc_train_lst, avgsc_hld_lst = [], [], []\n",
    "avgsc, avgsc_train, avgsc_hld = 0, 0, 0\n",
    "\n",
    "for train_index, test_index in kf.split(X_train_new):\n",
    "    X_train_cur, X_test_cur = X_train_new[train_index], X_train_new[test_index]\n",
    "    y_train_cur, y_test_cur = y_train_new[train_index], y_train_new[test_index]\n",
    "    X_train_train, X_val, y_train_train, y_val = train_test_split(\n",
    "        X_train_cur, y_train_cur, test_size=0.25, random_state=100)\n",
    "\n",
    "    bestPerformingModel = svm.SVC(\n",
    "        C=100, decision_function_shape=\"ovo\", gamma=0.001, probability=True)\n",
    "    bestscore = 0\n",
    "    for cval in cvals:\n",
    "        for gval in gamavals:\n",
    "            clf = svm.SVC(C=cval, decision_function_shape=\"ovo\", gamma=gval, probability=True)\n",
    "            clf.fit(X_train_train, y_train_train)\n",
    "            sc = clf.score(X_val, y_val)\n",
    "            print(f\"[C: {cval}, accuracy: {sc}]\")\n",
    "            if bestscore < sc:\n",
    "                bestscore = sc\n",
    "                bestPerformingModel = clf\n",
    "#                 print(bestPerformingModel)\n",
    "\n",
    "    bscr_train = bestPerformingModel.score(X_train_cur, y_train_cur)\n",
    "    bscr = bestPerformingModel.score(X_test_cur, y_test_cur)\n",
    "    bscr_hld = bestPerformingModel.score(X_test, y_test)\n",
    "\n",
    "    avgsc_train_lst.append(bscr_train)\n",
    "    avgsc_lst.append(bscr)\n",
    "    avgsc_hld_lst.append(bscr_hld)\n",
    "\n",
    "    avgsc_train = avgsc_train + bscr_train\n",
    "    avgsc = avgsc + bscr\n",
    "    avgsc_hld = avgsc_hld + bscr_hld\n",
    "\n",
    "    print('='*10)\n",
    "    print(f\"> Best training score: {bscr_train}\")\n",
    "    print(f\"> Best test score: {bscr}\")\n",
    "    print(f\"> Best held score: {bscr_hld}\")\n",
    "print('='*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Average training score list: [0.9623188405797102, 0.9420289855072463, 0.9364161849710982, 0.9479768786127167, 0.9479768786127167]\n",
      "> Average testing score list: [0.7701149425287356, 0.7931034482758621, 0.7906976744186046, 0.7906976744186046, 0.8023255813953488]\n",
      "> Average held score list: [0.8165137614678899, 0.8532110091743119, 0.8532110091743119, 0.8532110091743119, 0.8165137614678899]\n",
      "\n",
      "> Average training score list/k: 0.9473435536566976\n",
      "> Average testing score list/k: 0.7893878642074312\n",
      "> Average held score list/k: 0.8385321100917432\n",
      "\n",
      "Confusion Matrix: Actual (Row) vs Predicted (Column)\n",
      "[[24  0  3  0  0  0]\n",
      " [ 0 15  7  0  0  0]\n",
      " [ 0  4 48  0  0  0]\n",
      " [ 0  0  0  2  0  0]\n",
      " [ 0  0  1  0  0  0]\n",
      " [ 0  2  3  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"> Average training score list: {avgsc_train_lst}\")\n",
    "print(f\"> Average testing score list: {avgsc_lst}\")\n",
    "print(f\"> Average held score list: {avgsc_hld_lst}\")\n",
    "print()\n",
    "print(f\"> Average training score list/k: {avgsc_train/k}\")\n",
    "print(f\"> Average testing score list/k: {avgsc/k}\")\n",
    "print(f\"> Average held score list/k: {avgsc_hld/k}\")\n",
    "print()\n",
    "y_pred = bestPerformingModel.predict(X_test)\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix: Actual (Row) vs Predicted (Column)')\n",
    "print(cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.88232656e-02 1.96000095e-01 5.77570078e-01 1.94492187e-02\n",
      "  5.48207777e-02 8.33365659e-02]\n",
      " [1.15221634e-03 4.25933140e-01 5.37585057e-02 4.55662726e-01\n",
      "  9.21192339e-04 6.25722192e-02]\n",
      " [9.68496108e-01 9.96144982e-03 4.34072162e-03 5.03628343e-03\n",
      "  7.01386725e-03 5.15157005e-03]\n",
      " [9.43835003e-01 1.23136220e-02 1.37453207e-02 5.49645085e-03\n",
      "  1.74741769e-02 7.13542669e-03]\n",
      " [1.46545964e-02 2.40406198e-01 5.42337477e-01 2.65797248e-02\n",
      "  1.09191671e-02 1.65102837e-01]\n",
      " [2.02686398e-01 7.22981549e-02 6.98703725e-01 4.71469547e-03\n",
      "  1.14255611e-02 1.01714656e-02]\n",
      " [2.21161833e-02 4.79545083e-02 8.93136346e-01 4.41783910e-03\n",
      "  2.09284464e-02 1.14466770e-02]\n",
      " [9.59720963e-01 1.38022432e-02 7.08591175e-03 5.67572317e-03\n",
      "  7.39432631e-03 6.32083234e-03]\n",
      " [6.79211202e-01 7.24444541e-02 6.70622202e-02 1.66061680e-02\n",
      "  9.03307487e-02 7.43452068e-02]\n",
      " [1.23680273e-02 5.16530990e-01 3.41744581e-01 1.80273074e-02\n",
      "  1.37366976e-02 9.75923961e-02]\n",
      " [1.68988382e-01 8.24090242e-02 5.56500250e-01 1.37484469e-02\n",
      "  6.72606417e-02 1.11093255e-01]\n",
      " [4.46338323e-03 6.67707618e-01 1.83537835e-01 1.75585230e-02\n",
      "  5.64467975e-03 1.21087961e-01]\n",
      " [9.93668431e-01 1.28546132e-03 5.41961955e-04 2.95412325e-03\n",
      "  8.12308786e-04 7.37713815e-04]\n",
      " [9.47713243e-01 1.20277020e-02 9.96088274e-03 6.99521826e-03\n",
      "  1.37796336e-02 9.52332050e-03]\n",
      " [1.21866073e-01 1.47096905e-01 5.33677222e-01 1.73882155e-02\n",
      "  1.14935396e-01 6.50361891e-02]\n",
      " [1.54585085e-01 6.74753351e-02 6.55274494e-01 7.70511576e-03\n",
      "  7.49169340e-02 4.00430366e-02]\n",
      " [1.63341074e-03 8.11895761e-01 4.18884338e-02 6.55271136e-02\n",
      "  1.28190379e-03 7.77733766e-02]\n",
      " [7.28375337e-02 1.31518071e-01 6.24793790e-01 1.13822166e-02\n",
      "  7.46307976e-02 8.48375911e-02]\n",
      " [6.22304876e-02 1.95817636e-01 6.04211494e-01 1.12446636e-02\n",
      "  5.87800058e-02 6.77157124e-02]\n",
      " [7.25290152e-02 3.81179470e-01 3.51685084e-01 1.25387153e-02\n",
      "  1.10725460e-01 7.13422554e-02]\n",
      " [1.23041824e-01 1.05802225e-01 5.82321236e-01 1.17581410e-02\n",
      "  1.30653182e-01 4.64233920e-02]\n",
      " [2.42386719e-01 1.03844344e-01 3.85584818e-01 1.63417275e-02\n",
      "  1.89083415e-01 6.27589767e-02]\n",
      " [1.83481385e-03 8.00508247e-01 4.64063237e-02 6.72550794e-02\n",
      "  1.44234469e-03 8.25531917e-02]\n",
      " [9.37962562e-01 1.13403277e-02 2.23384693e-02 6.24951070e-03\n",
      "  1.02316563e-02 1.18774743e-02]\n",
      " [9.08008465e-02 1.19918532e-01 7.51633810e-01 5.11947659e-03\n",
      "  1.78263448e-02 1.47009906e-02]\n",
      " [5.58212365e-02 6.29095041e-02 7.73090523e-01 1.90651508e-02\n",
      "  5.08284051e-02 3.82851803e-02]\n",
      " [9.88288051e-02 4.19189307e-01 3.50463186e-01 1.01615523e-02\n",
      "  6.23168185e-02 5.90403307e-02]\n",
      " [3.37046113e-02 8.36934866e-02 8.29278814e-01 5.14255381e-03\n",
      "  3.57225930e-02 1.24579417e-02]\n",
      " [9.93931257e-01 7.12953233e-04 3.44617883e-04 3.00939840e-03\n",
      "  5.79130870e-04 1.42264281e-03]\n",
      " [9.09436064e-01 2.20218442e-02 1.97068044e-02 7.01717650e-03\n",
      "  3.19786995e-02 9.83941116e-03]\n",
      " [2.21161833e-02 4.79545083e-02 8.93136346e-01 4.41783910e-03\n",
      "  2.09284464e-02 1.14466770e-02]\n",
      " [9.79289321e-01 5.73255048e-03 1.92318694e-03 5.78291696e-03\n",
      "  2.30982748e-03 4.96219759e-03]\n",
      " [5.05497430e-04 8.97972831e-01 2.46325233e-02 1.61543382e-02\n",
      "  4.25951464e-04 6.03088586e-02]\n",
      " [8.44015979e-01 1.98813597e-02 9.22126162e-02 7.67091218e-03\n",
      "  1.39266653e-02 2.22924678e-02]\n",
      " [1.90608834e-03 5.77798223e-01 3.15892155e-01 1.13904229e-02\n",
      "  3.09913679e-03 8.99139733e-02]\n",
      " [2.73809407e-02 7.10545183e-02 8.62273240e-01 4.68224396e-03\n",
      "  2.06664373e-02 1.39426202e-02]\n",
      " [8.62193191e-02 8.11681141e-02 7.37796058e-01 9.02662408e-03\n",
      "  4.07906136e-02 4.49992715e-02]\n",
      " [1.61401837e-02 2.96268597e-02 9.26509591e-01 3.28151087e-03\n",
      "  1.23169382e-02 1.21249167e-02]\n",
      " [4.54522121e-02 7.13703732e-02 7.89613890e-01 6.44032971e-03\n",
      "  3.59239250e-02 5.11992700e-02]\n",
      " [9.73455412e-01 9.39855118e-03 2.99101685e-03 5.73989828e-03\n",
      "  3.80479732e-03 4.61032422e-03]\n",
      " [2.25943347e-02 4.69856685e-02 8.92891105e-01 4.45031458e-03\n",
      "  2.14111890e-02 1.16673878e-02]\n",
      " [1.91031039e-02 4.58604031e-02 9.09259695e-01 3.75248529e-03\n",
      "  9.32417991e-03 1.27001323e-02]\n",
      " [1.18571571e-02 2.37137224e-02 9.40799024e-01 3.10923020e-03\n",
      "  7.88532447e-03 1.26355417e-02]\n",
      " [4.25923000e-02 4.04668514e-02 8.49474020e-01 5.42677860e-03\n",
      "  3.72007168e-02 2.48393335e-02]\n",
      " [7.34342112e-01 4.69965776e-02 1.43984159e-01 7.95881354e-03\n",
      "  4.43362351e-02 2.23821031e-02]\n",
      " [1.03347009e-01 8.58424592e-02 6.45496842e-01 9.94605110e-03\n",
      "  1.23601221e-01 3.17664174e-02]\n",
      " [3.49100280e-02 1.95700184e-01 6.50039494e-01 1.48846008e-02\n",
      "  2.53907663e-02 7.90749275e-02]\n",
      " [8.77869493e-01 2.77710996e-02 3.70330045e-02 1.76471744e-02\n",
      "  1.81956899e-02 2.14835384e-02]\n",
      " [8.05015270e-01 5.02378468e-02 7.00466224e-02 1.08795432e-02\n",
      "  3.42448790e-02 2.95758384e-02]\n",
      " [4.75426158e-02 2.44667332e-01 5.72248285e-01 1.52460955e-02\n",
      "  4.81443382e-02 7.21513329e-02]\n",
      " [8.87693180e-04 8.63965137e-01 2.73141255e-02 3.48140698e-02\n",
      "  6.92873994e-04 7.23261000e-02]\n",
      " [1.86450172e-02 4.40083136e-02 9.00876545e-01 3.87444005e-03\n",
      "  1.61881567e-02 1.64075273e-02]\n",
      " [2.97014906e-04 3.78738691e-03 5.07875088e-03 9.85557908e-01\n",
      "  1.16580932e-03 4.11312969e-03]\n",
      " [3.37058310e-03 7.03065259e-01 1.89803363e-01 1.37529482e-02\n",
      "  6.63523606e-03 8.33726106e-02]\n",
      " [9.03442974e-01 3.05274282e-02 2.13900017e-02 8.91212568e-03\n",
      "  1.90143905e-02 1.67130802e-02]\n",
      " [1.44370425e-02 2.95461110e-02 9.30211728e-01 3.18663435e-03\n",
      "  1.12255571e-02 1.13929273e-02]\n",
      " [1.41407685e-01 2.00561406e-01 5.80394579e-01 8.67555876e-03\n",
      "  3.29647988e-02 3.59959726e-02]\n",
      " [3.07904146e-03 7.28745814e-01 6.90767467e-02 9.88143654e-02\n",
      "  2.47238934e-03 9.78116433e-02]\n",
      " [2.15139973e-02 4.43815004e-02 8.98660018e-01 4.37062610e-03\n",
      "  1.95025201e-02 1.15713384e-02]\n",
      " [1.74761148e-02 2.69590497e-01 6.07603624e-01 1.82337675e-02\n",
      "  1.09996280e-02 7.60963684e-02]\n",
      " [8.47481035e-01 4.46827913e-02 7.53747887e-02 6.16880471e-03\n",
      "  1.85775187e-02 7.71506103e-03]\n",
      " [6.54132746e-02 1.09245800e-01 6.90424980e-01 9.87595421e-03\n",
      "  4.74534050e-02 7.75865860e-02]\n",
      " [2.19583331e-01 8.02717976e-02 5.61753237e-01 9.49373144e-03\n",
      "  4.92868309e-02 7.96110720e-02]\n",
      " [4.16174562e-02 1.71455114e-01 4.62161461e-01 1.49907980e-02\n",
      "  3.80030909e-02 2.71772080e-01]\n",
      " [1.79767215e-02 8.46887298e-01 4.67195451e-02 8.77062210e-03\n",
      "  1.96158914e-02 6.00299220e-02]\n",
      " [3.75082809e-01 6.69735128e-02 5.19481075e-01 4.75024276e-03\n",
      "  2.26040147e-02 1.11083466e-02]\n",
      " [1.92918468e-03 7.83044992e-01 5.61366008e-02 6.72601475e-02\n",
      "  1.51043753e-03 9.01186379e-02]\n",
      " [1.87091971e-02 3.12893092e-02 9.17688343e-01 3.50248201e-03\n",
      "  1.53480447e-02 1.34626235e-02]\n",
      " [3.13042058e-02 4.73726693e-02 8.71827266e-01 4.08588438e-03\n",
      "  3.12560227e-02 1.41539517e-02]\n",
      " [1.38464511e-01 2.12016726e-01 4.45581867e-01 1.47446657e-02\n",
      "  1.25979945e-01 6.32122855e-02]\n",
      " [3.07902806e-03 7.28752428e-01 6.90719663e-02 9.88141709e-02\n",
      "  2.47237755e-03 9.78100291e-02]\n",
      " [8.92646965e-01 2.90024242e-02 2.38425267e-02 7.94071566e-03\n",
      "  3.35529046e-02 1.30144639e-02]\n",
      " [3.07902796e-03 7.28752431e-01 6.90719678e-02 9.88141679e-02\n",
      "  2.47237748e-03 9.78100278e-02]\n",
      " [9.33416187e-01 1.54181326e-02 1.76420176e-02 6.38612494e-03\n",
      "  1.73405106e-02 9.79702692e-03]\n",
      " [1.55370875e-02 4.36731878e-02 9.11411110e-01 4.19289221e-03\n",
      "  1.56461520e-02 9.53957044e-03]\n",
      " [8.19769365e-03 5.42790927e-01 3.21459219e-01 1.62813291e-02\n",
      "  2.11838214e-02 9.00870097e-02]\n",
      " [2.32709751e-03 7.29606542e-01 1.74493321e-01 1.94586574e-02\n",
      "  3.97360367e-03 7.01407784e-02]\n",
      " [3.66050294e-02 2.47666422e-01 5.70594189e-01 1.83001748e-02\n",
      "  3.99738119e-02 8.68603723e-02]\n",
      " [9.99302897e-02 7.86627675e-02 6.82338629e-01 1.01528114e-02\n",
      "  5.66936526e-02 7.22218495e-02]\n",
      " [8.99400832e-01 1.80468960e-02 2.87507296e-02 6.46568519e-03\n",
      "  3.60086498e-02 1.13272073e-02]\n",
      " [9.98859932e-02 1.59545460e-01 5.74014993e-01 1.34337864e-02\n",
      "  8.64692207e-02 6.66505474e-02]\n",
      " [1.13534903e-01 1.18882431e-01 3.79351042e-01 1.96712230e-02\n",
      "  3.28948954e-01 3.96114471e-02]\n",
      " [2.29759115e-02 5.32193013e-02 8.45560405e-01 6.08655466e-03\n",
      "  2.05841161e-02 5.15737117e-02]\n",
      " [9.19029616e-01 1.99556477e-02 1.70318011e-02 6.69604719e-03\n",
      "  2.83958119e-02 8.89107604e-03]\n",
      " [6.64396955e-02 1.19923511e-01 6.53796446e-01 2.83431036e-02\n",
      "  4.53595061e-02 8.61377381e-02]\n",
      " [6.60829964e-02 8.84643348e-02 7.67046977e-01 6.50025198e-03\n",
      "  3.12513164e-02 4.06541236e-02]\n",
      " [2.29455801e-02 3.86252403e-01 3.76052391e-01 1.33485548e-02\n",
      "  1.33332997e-02 1.88067772e-01]\n",
      " [5.95576836e-02 6.03621663e-02 7.90893977e-01 7.41392958e-03\n",
      "  4.43902120e-02 3.73820312e-02]\n",
      " [1.79758413e-02 8.46892187e-01 4.67169603e-02 8.77046783e-03\n",
      "  1.96152835e-02 6.00292599e-02]\n",
      " [1.48486630e-02 3.22271130e-01 5.79591966e-01 1.26254143e-02\n",
      "  2.22967844e-02 4.83660422e-02]\n",
      " [3.07902796e-03 7.28752431e-01 6.90719678e-02 9.88141679e-02\n",
      "  2.47237748e-03 9.78100278e-02]\n",
      " [9.11491580e-01 1.88891250e-02 4.14404475e-02 5.12521361e-03\n",
      "  1.77013903e-02 5.35224308e-03]\n",
      " [2.23871003e-02 1.48951163e-01 5.84595925e-01 1.27965438e-02\n",
      "  1.76260532e-02 2.13643215e-01]\n",
      " [4.19553917e-02 1.41261220e-01 6.81658506e-01 9.34340837e-03\n",
      "  5.20065397e-02 7.37749336e-02]\n",
      " [1.14820389e-01 9.55766535e-02 6.51388521e-01 9.13402440e-03\n",
      "  7.47630041e-02 5.43174084e-02]\n",
      " [1.78095901e-01 1.07172622e-01 4.25267585e-01 1.66816671e-02\n",
      "  2.06281595e-01 6.65006295e-02]\n",
      " [9.38918572e-02 9.00797551e-02 6.42383425e-01 9.62132518e-03\n",
      "  1.08809113e-01 5.52145248e-02]\n",
      " [5.01324436e-03 3.04367803e-01 4.94070255e-01 9.38736428e-02\n",
      "  4.66177026e-03 9.80132845e-02]\n",
      " [1.19707398e-02 3.63563839e-02 8.93837483e-01 5.26750812e-03\n",
      "  1.88715279e-02 3.36963574e-02]\n",
      " [4.14939888e-03 7.78088324e-01 1.14307698e-01 1.09660951e-02\n",
      "  5.55819750e-03 8.69302866e-02]\n",
      " [8.96605579e-01 3.28152845e-02 2.26043805e-02 8.89626885e-03\n",
      "  2.23778873e-02 1.67005999e-02]\n",
      " [4.68015482e-03 2.59856496e-02 6.08603297e-02 8.76628764e-01\n",
      "  7.46564504e-03 2.43794566e-02]\n",
      " [2.15937826e-02 4.66663325e-02 8.95395050e-01 4.39936836e-03\n",
      "  2.05306668e-02 1.14147994e-02]\n",
      " [1.37899092e-02 2.79911609e-02 9.32641997e-01 3.16988101e-03\n",
      "  1.12024643e-02 1.12045880e-02]\n",
      " [1.18389416e-02 3.84872519e-01 4.06418144e-01 1.49473703e-02\n",
      "  9.92128173e-03 1.72001744e-01]\n",
      " [9.22597625e-03 4.58057268e-01 3.48044720e-01 2.56518683e-02\n",
      "  1.05060498e-02 1.48514117e-01]\n",
      " [1.80852595e-02 4.06193431e-02 9.09531517e-01 4.50226863e-03\n",
      "  1.50718357e-02 1.21897762e-02]\n",
      " [9.00222651e-01 3.12831963e-02 1.92327523e-02 9.17498573e-03\n",
      "  1.84947353e-02 2.15916797e-02]\n",
      " [2.08828992e-02 4.44240749e-02 8.99898885e-01 4.32669871e-03\n",
      "  1.92222440e-02 1.12451979e-02]]\n",
      "            0         1         2         3         4         5\n",
      "0    0.068823  0.196000  0.577570  0.019449  0.054821  0.083337\n",
      "1    0.001152  0.425933  0.053759  0.455663  0.000921  0.062572\n",
      "2    0.968496  0.009961  0.004341  0.005036  0.007014  0.005152\n",
      "3    0.943835  0.012314  0.013745  0.005496  0.017474  0.007135\n",
      "4    0.014655  0.240406  0.542337  0.026580  0.010919  0.165103\n",
      "5    0.202686  0.072298  0.698704  0.004715  0.011426  0.010171\n",
      "6    0.022116  0.047955  0.893136  0.004418  0.020928  0.011447\n",
      "7    0.959721  0.013802  0.007086  0.005676  0.007394  0.006321\n",
      "8    0.679211  0.072444  0.067062  0.016606  0.090331  0.074345\n",
      "9    0.012368  0.516531  0.341745  0.018027  0.013737  0.097592\n",
      "10   0.168988  0.082409  0.556500  0.013748  0.067261  0.111093\n",
      "11   0.004463  0.667708  0.183538  0.017559  0.005645  0.121088\n",
      "12   0.993668  0.001285  0.000542  0.002954  0.000812  0.000738\n",
      "13   0.947713  0.012028  0.009961  0.006995  0.013780  0.009523\n",
      "14   0.121866  0.147097  0.533677  0.017388  0.114935  0.065036\n",
      "15   0.154585  0.067475  0.655274  0.007705  0.074917  0.040043\n",
      "16   0.001633  0.811896  0.041888  0.065527  0.001282  0.077773\n",
      "17   0.072838  0.131518  0.624794  0.011382  0.074631  0.084838\n",
      "18   0.062230  0.195818  0.604211  0.011245  0.058780  0.067716\n",
      "19   0.072529  0.381179  0.351685  0.012539  0.110725  0.071342\n",
      "20   0.123042  0.105802  0.582321  0.011758  0.130653  0.046423\n",
      "21   0.242387  0.103844  0.385585  0.016342  0.189083  0.062759\n",
      "22   0.001835  0.800508  0.046406  0.067255  0.001442  0.082553\n",
      "23   0.937963  0.011340  0.022338  0.006250  0.010232  0.011877\n",
      "24   0.090801  0.119919  0.751634  0.005119  0.017826  0.014701\n",
      "25   0.055821  0.062910  0.773091  0.019065  0.050828  0.038285\n",
      "26   0.098829  0.419189  0.350463  0.010162  0.062317  0.059040\n",
      "27   0.033705  0.083693  0.829279  0.005143  0.035723  0.012458\n",
      "28   0.993931  0.000713  0.000345  0.003009  0.000579  0.001423\n",
      "29   0.909436  0.022022  0.019707  0.007017  0.031979  0.009839\n",
      "..        ...       ...       ...       ...       ...       ...\n",
      "79   0.899401  0.018047  0.028751  0.006466  0.036009  0.011327\n",
      "80   0.099886  0.159545  0.574015  0.013434  0.086469  0.066651\n",
      "81   0.113535  0.118882  0.379351  0.019671  0.328949  0.039611\n",
      "82   0.022976  0.053219  0.845560  0.006087  0.020584  0.051574\n",
      "83   0.919030  0.019956  0.017032  0.006696  0.028396  0.008891\n",
      "84   0.066440  0.119924  0.653796  0.028343  0.045360  0.086138\n",
      "85   0.066083  0.088464  0.767047  0.006500  0.031251  0.040654\n",
      "86   0.022946  0.386252  0.376052  0.013349  0.013333  0.188068\n",
      "87   0.059558  0.060362  0.790894  0.007414  0.044390  0.037382\n",
      "88   0.017976  0.846892  0.046717  0.008770  0.019615  0.060029\n",
      "89   0.014849  0.322271  0.579592  0.012625  0.022297  0.048366\n",
      "90   0.003079  0.728752  0.069072  0.098814  0.002472  0.097810\n",
      "91   0.911492  0.018889  0.041440  0.005125  0.017701  0.005352\n",
      "92   0.022387  0.148951  0.584596  0.012797  0.017626  0.213643\n",
      "93   0.041955  0.141261  0.681659  0.009343  0.052007  0.073775\n",
      "94   0.114820  0.095577  0.651389  0.009134  0.074763  0.054317\n",
      "95   0.178096  0.107173  0.425268  0.016682  0.206282  0.066501\n",
      "96   0.093892  0.090080  0.642383  0.009621  0.108809  0.055215\n",
      "97   0.005013  0.304368  0.494070  0.093874  0.004662  0.098013\n",
      "98   0.011971  0.036356  0.893837  0.005268  0.018872  0.033696\n",
      "99   0.004149  0.778088  0.114308  0.010966  0.005558  0.086930\n",
      "100  0.896606  0.032815  0.022604  0.008896  0.022378  0.016701\n",
      "101  0.004680  0.025986  0.060860  0.876629  0.007466  0.024379\n",
      "102  0.021594  0.046666  0.895395  0.004399  0.020531  0.011415\n",
      "103  0.013790  0.027991  0.932642  0.003170  0.011202  0.011205\n",
      "104  0.011839  0.384873  0.406418  0.014947  0.009921  0.172002\n",
      "105  0.009226  0.458057  0.348045  0.025652  0.010506  0.148514\n",
      "106  0.018085  0.040619  0.909532  0.004502  0.015072  0.012190\n",
      "107  0.900223  0.031283  0.019233  0.009175  0.018495  0.021592\n",
      "108  0.020883  0.044424  0.899899  0.004327  0.019222  0.011245\n",
      "\n",
      "[109 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# save the model to disk\n",
    "filename = 'data/pretrained/svm_finalized_model.pickle'\n",
    "pickle.dump(bestPerformingModel, open(filename, 'wb'))\n",
    "\n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(X_test, y_test)\n",
    "y_prob = bestPerformingModel.predict_proba(X_test)\n",
    "\n",
    "df = pd.DataFrame.from_records(y_prob)\n",
    "print(df)\n",
    "df.to_csv('data/model_predictions/svm_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
