{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTICE\n",
    "Neural Model testing did **NOT** occur for embedded feature type inferencing. Results are copied results from original repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a3163d96f493>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m \u001b[0;31m# linear algebra\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m \u001b[0;31m# data processing, CSV file I/O (e.g. pd.read_csv)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxPooling1D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGlobalMaxPool1D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mGlobalMaxPooling1D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "#Copyright 2019 Vraj Shah, Arun Kumar\n",
    "#\n",
    "#Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#you may not use this file except in compliance with the License.\n",
    "#You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#Unless required by applicable law or agreed to in writing, software\n",
    "#distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#See the License for the specific language governing permissions and\n",
    "#limitations under the License.\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Embedding, Input, Flatten, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPool1D, Dropout, concatenate,GlobalMaxPooling1D\n",
    "from keras.preprocessing import text as keras_text, sequence as keras_seq\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "np.random.seed(512)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "from keras.layers import LeakyReLU, BatchNormalization\n",
    "from keras import initializers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "import keras\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "np.random.seed(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define network parameters\n",
    "max_features = 128\n",
    "maxlen = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py:2714: DtypeWarning: Columns (2,5,10,11,12,13,14,15,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "dict_label = {'Numbers': 0, 'List': 1, 'Datetime': 2, 'Sentence': 3,\n",
    "              'URL': 4, 'Custom Object': 5, 'Unusable': 6}\n",
    "data = pd.read_csv('data/needs_extraction_data/date_and_time_combined.csv')\n",
    "data['y_act'] = [dict_label[i] for i in data['y_act']]\n",
    "y = data.loc[:, ['y_act']]\n",
    "data_LSTM = pd.concat([data['Attribute_name']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.rename(columns={'Num of nans': 'Num_of_nans', 'num of dist_val': 'num_of_dist_val'})\n",
    "\n",
    "# # data['Num_of_nans'] = [float(data['Num_of_nans'][i])/float(data['Total_val'][i]) for i in data.index]\n",
    "# # data['num_of_dist_val'] = [float(data['num_of_dist_val'][i])/float(data['Total_val'][i]) for i in data.index]\n",
    "\n",
    "# data1 = data[['Num_of_nans', 'max_val', 'mean', 'min_val', 'num_of_dist_val','std_dev','castability','extractability', 'len_val','Total_val']]\n",
    "# data1 = data1.fillna(0)\n",
    "\n",
    "# data1['Num_of_nans'] = [data1['Num_of_nans'][i]*100/data1['Total_val'][i] for i in data1.index]\n",
    "# data1['num_of_dist_val'] = [(data1['num_of_dist_val'][i]*1.0)/data1['Total_val'][i] for i in data1.index]\n",
    "\n",
    "# data1 = data1.rename(columns={'mean': 'scaled_mean', 'min_val': 'scaled_min_val', 'max_val': 'scaled_max_val','std_dev': 'scaled_std_dev'})\n",
    "# data1.loc[data1['scaled_min_val'] > 10000, 'scaled_min_val'] = 10000\n",
    "# data1.loc[data1['scaled_min_val'] < -10000, 'scaled_min_val'] = -10000\n",
    "# data1.loc[data1['scaled_max_val'] > 10000, 'scaled_max_val'] = 10000\n",
    "# data1.loc[data1['scaled_max_val'] < -10000, 'scaled_max_val'] = -10000\n",
    "# data1.loc[data1['scaled_mean'] > 10000, 'scaled_mean'] = 10000\n",
    "# data1.loc[data1['scaled_mean'] < -10000, 'scaled_mean'] = -10000\n",
    "# data1.loc[data1['scaled_std_dev'] > 10000, 'scaled_std_dev'] = 10000\n",
    "# data1.loc[data1['scaled_std_dev'] < -10000, 'scaled_std_dev'] = -10000\n",
    "# column_names_to_normalize = ['scaled_max_val', 'scaled_mean', 'scaled_min_val','scaled_std_dev','num_of_dist_val','Num_of_nans','Total_val']\n",
    "# x = data1[column_names_to_normalize].values\n",
    "# x = np.nan_to_num(x)\n",
    "# x_scaled = StandardScaler().fit_transform(x)\n",
    "# df_temp = pd.DataFrame(x_scaled, columns=column_names_to_normalize, index = data1.index)\n",
    "# data1[column_names_to_normalize] = df_temp\n",
    "\n",
    "# data1.Num_of_nans = data1.Num_of_nans.astype(float)\n",
    "# data1.num_of_dist_val = data1.num_of_dist_val.astype(float)\n",
    "# data1.castability = data1.castability.astype(float)\n",
    "# data1.extractability = data1.extractability.astype(float)\n",
    "# y.y_act = y.y_act.astype(float)\n",
    "\n",
    "\n",
    "# data1 = data1[['Num_of_nans', 'scaled_max_val', 'scaled_mean', 'scaled_min_val', 'num_of_dist_val','scaled_std_dev','castability','extractability', 'len_val','Total_val']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1927\n"
     ]
    }
   ],
   "source": [
    "arr = data['Attribute_name'].values\n",
    "arr = [str(x) for x in arr]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1),analyzer='char')\n",
    "X = vectorizer.fit_transform(arr)\n",
    "print(len(vectorizer.get_feature_names()))\n",
    "\n",
    "temp_df = data.drop(['Record_id', 'Attribute_name', 'sample_1', 'sample_2', 'sample_3', 'sample_4', 'sample_5'],axis=1)\n",
    "df = pd.DataFrame(X.toarray())\n",
    "df = pd.concat([df,data], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7400\n",
      "7400\n",
      "5122\n",
      "7400\n",
      "7400\n"
     ]
    }
   ],
   "source": [
    "key_name = data['Attribute_name']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    key_name, y, test_size=0.2, random_state=100)\n",
    "\n",
    "\n",
    "# atr_train, atr_test = train_test_split(\n",
    "#     key_name, test_size=0.2, random_state=100)\n",
    "\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "\n",
    "structured_data_train = X_train\n",
    "structured_data_test = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7400\n",
      "['#NULL!' '0' '268' ... 'Hate' '-0.101' '2']\n",
      "['0' '78' '110' ... nan '0.171' '5']\n",
      "7400\n"
     ]
    }
   ],
   "source": [
    "list_sentences_train = atr_train.values\n",
    "list_sentences_test = atr_test.values\n",
    "\n",
    "print(len(list_sentences_train))\n",
    "\n",
    "# X_train.sample_1 = X_train.sample_1.astype(str)\n",
    "# X_test.sample_1 = X_test.sample_1.astype(str)\n",
    "\n",
    "list_sentences_train1 = samp1_train.values\n",
    "list_sentences_test1 = samp1_test.values\n",
    "\n",
    "print(list_sentences_train1)\n",
    "\n",
    "# X_train.sample_2 = X_train.sample_2.astype(str)\n",
    "# X_test.sample_2 = X_test.sample_2.astype(str)\n",
    "\n",
    "list_sentences_train2 = samp2_train.values\n",
    "list_sentences_test2 = samp2_test.values\n",
    "\n",
    "print(list_sentences_train2)\n",
    "\n",
    "for i in range(len(list_sentences_train)):\n",
    "    list_sentences_train[i] = str(list_sentences_train[i])\n",
    "    list_sentences_train1[i] = str(list_sentences_train1[i])\n",
    "    list_sentences_train2[i] = str(list_sentences_train2[i])\n",
    "    \n",
    "for i in range(len(list_sentences_test)):\n",
    "    list_sentences_test[i] = str(list_sentences_test[i])\n",
    "    list_sentences_test1[i] = str(list_sentences_test1[i])\n",
    "    list_sentences_test2[i] = str(list_sentences_test2[i])    \n",
    "\n",
    "print(len(list_sentences_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = keras_text.Tokenizer(char_level = True)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "# train data\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "X_t = keras_seq.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "# test data\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_te = keras_seq.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "\n",
    "tokenizer1 = keras_text.Tokenizer(char_level = True)\n",
    "tokenizer1.fit_on_texts(list(list_sentences_train1))\n",
    "# train data\n",
    "list_tokenized_train1 = tokenizer.texts_to_sequences(list_sentences_train1)\n",
    "X_t1 = keras_seq.pad_sequences(list_tokenized_train1, maxlen=maxlen)\n",
    "# test data\n",
    "list_tokenized_test1 = tokenizer.texts_to_sequences(list_sentences_test1)\n",
    "X_te1 = keras_seq.pad_sequences(list_tokenized_test1, maxlen=maxlen)\n",
    "\n",
    "\n",
    "# tokenizer2 = keras_text.Tokenizer(char_level = True)\n",
    "# tokenizer2.fit_on_texts(list(list_sentences_train1))\n",
    "# # train data\n",
    "# list_tokenized_train2 = tokenizer.texts_to_sequences(list_sentences_train2)\n",
    "# X_t2 = keras_seq.pad_sequences(list_tokenized_train2, maxlen=maxlen)\n",
    "# # test data\n",
    "# list_tokenized_test2 = tokenizer.texts_to_sequences(list_sentences_test2)\n",
    "# X_te2 = keras_seq.pad_sequences(list_tokenized_test2, maxlen=maxlen)\n",
    "\n",
    "# print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(numfilters,embed_size):\n",
    "    inp = Input(shape=(None, ))\n",
    "    x = Embedding(input_dim = len(tokenizer.word_counts)+1, output_dim = embed_size)(inp)\n",
    "    prefilt_x = Dropout(0.5)(x)\n",
    "    out_conv = []\n",
    "\n",
    "    x = prefilt_x\n",
    "    for i in range(2):\n",
    "        x = Conv1D(numfilters, kernel_size = 3, activation = 'tanh')(x)\n",
    "        numfilters = numfilters*2\n",
    "    \n",
    "    out_conv += [Dropout(0.5)(GlobalMaxPool1D()(x))]\n",
    "    out_conv += [GlobalMaxPool1D()(x)]\n",
    "    xy = concatenate(out_conv, axis = -1)  \n",
    "    print(xy.shape)\n",
    "    #########################################################\n",
    "    inp1 = Input(shape=(None, ))\n",
    "    x = Embedding(input_dim = len(tokenizer1.word_counts)+1, output_dim = embed_size)(inp1)\n",
    "    prefilt_x = Dropout(0.25)(x)\n",
    "    out_conv = []\n",
    "\n",
    "    x = prefilt_x\n",
    "    for i in range(2):\n",
    "        x = Conv1D(16*2**(i), kernel_size = 3, activation = 'relu')(x)\n",
    "    out_conv += [Dropout(0.5)(GlobalMaxPool1D()(x))]\n",
    "    out_conv += [GlobalMaxPool1D()(x)]\n",
    "    x1 = concatenate(out_conv, axis = -1)\n",
    "    print(x1.shape)\n",
    "    \n",
    "    Str_input = Input(shape=(5122,))\n",
    "    layersfin = keras.layers.concatenate([xy,Str_input])\n",
    "    print(layersfin.shape)\n",
    "    x = Dense(500, activation='relu')(layersfin)\n",
    "    x = Dropout(0.75)(x)\n",
    "    x = Dense(500, activation='relu')(x)\n",
    "    x = Dropout(0.75)(x)\n",
    "    x = Dense(5, activation='softmax')(x)\n",
    "    model = Model(inputs=[inp,Str_input], outputs=[x])\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# model = build_model()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32 # large enough that some other labels come in\n",
    "epochs = 25\n",
    "\n",
    "file_path=\"best_weights.h5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=20)\n",
    "\n",
    "callbacks_list = [checkpoint, early] #early\n",
    "# history = model.fit([X_t,X_t1,structured_data_train], to_categorical(y_train),\n",
    "#                     validation_data=([X_te,X_te1,structured_data_test], to_categorical(y_test)),\n",
    "#                     batch_size=batch_size, epochs=epochs, shuffle = True, callbacks=callbacks_list)\n",
    "\n",
    "# # model = build_model()\n",
    "# # model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0 ...  2 11  2]\n",
      " [ 0  0  0 ... 37 22 39]\n",
      " [ 0  0  0 ... 15  5 23]\n",
      " ...\n",
      " [ 0  0  0 ... 18  5  8]\n",
      " [ 0  0  0 ... 28 16 33]\n",
      " [ 0  0  0 ...  4  1  2]]\n",
      "(7400, 512)\n",
      "7400\n"
     ]
    }
   ],
   "source": [
    "print(X_t)\n",
    "print(X_t.shape)\n",
    "# print(y_train[1851:])\n",
    "print(len(y_train))\n",
    "y_train = y_train.values\n",
    "structured_data_train = structured_data_train.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[[ 0  0  0 ...  3 11 10]\n",
      " [ 0  0  0 ... 22  6 33]\n",
      " [ 0  0  0 ...  4  1  2]\n",
      " ...\n",
      " [ 0  0  0 ... 18  5  8]\n",
      " [ 0  0  0 ... 28 16 33]\n",
      " [ 0  0  0 ...  4  1  2]]\n",
      "(4440, 1)\n",
      "(1480, 1)\n",
      "(4440, 5122)\n",
      "(1480, 5122)\n",
      "(?, 40)\n",
      "(?, 64)\n",
      "(?, 5162)\n",
      "(?, 128)\n",
      "(?, 64)\n",
      "(?, 5250)\n",
      "Train on 5920 samples, validate on 1480 samples\n",
      "Epoch 1/25\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # large enough that some other labels come in\n",
    "epochs = 25\n",
    "\n",
    "k = 5\n",
    "kf = KFold(n_splits=k)\n",
    "\n",
    "# n_filters_grid = [32,64,128,256,512]\n",
    "# embed_size = [64,128,256,512]\n",
    "\n",
    "n_filters_grid = [32]\n",
    "embed_size = [256]\n",
    "models = []\n",
    "\n",
    "avgsc_lst,avgsc_val_lst,avgsc_train_lst = [],[],[]\n",
    "avgsc,avgsc_val,avgsc_train = 0,0,0\n",
    "i=0\n",
    "for train_index, test_index in kf.split(X_t):\n",
    "    file_path= 'best_weights'+str(i)+'.h5'\n",
    "    checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=20)\n",
    "\n",
    "    callbacks_list = [checkpoint, early] #early\n",
    "        \n",
    "    print('\\n')\n",
    "    X_train_cur, X_test_cur = X_t[train_index], X_t[test_index]\n",
    "    y_train_cur, y_test_cur = y_train[train_index], y_train[test_index]\n",
    "    structured_data_train_cur, structured_data_test_cur = structured_data_train[train_index],structured_data_train[test_index]\n",
    "    \n",
    "    X_train_train, X_val,y_train_train,y_val = train_test_split(X_train_cur,y_train_cur, test_size=0.25,random_state=100)\n",
    "    structured_data_train_train,structured_data_val = train_test_split(structured_data_train_cur, test_size=0.25,random_state=100)\n",
    "    \n",
    "    \n",
    "    print(X_train_cur)\n",
    "    \n",
    "    print(y_train_train.shape)\n",
    "    print(y_val.shape)\n",
    "    print(structured_data_train_train.shape)\n",
    "    print(structured_data_val.shape)\n",
    "    \n",
    "    bestPerformingModel = build_model(10,5)\n",
    "    bestscore = 0\n",
    "    for ne in n_filters_grid:\n",
    "        for md in embed_size:\n",
    "#             clf = RandomForestClassifier(n_estimators=ne,max_depth=md)\n",
    "            clf = build_model(ne,md)\n",
    "            history = clf.fit([X_train_cur,structured_data_train_cur], to_categorical(y_train_cur),\n",
    "                    validation_data=([X_test_cur,structured_data_test_cur], to_categorical(y_test_cur)),\n",
    "                    batch_size=batch_size, epochs=epochs, shuffle = True, callbacks=callbacks_list)\n",
    "        \n",
    "            sc = clf.evaluate([X_test_cur,structured_data_test_cur],to_categorical(y_test_cur))\n",
    "            bestPerformingModel = clf\n",
    "            if bestscore < sc:\n",
    "                bestscore = sc\n",
    "                bestPerformingModel = clf\n",
    "\n",
    "#     clf.load_weights('best_weights'+str(i)+'.h5')\n",
    "    print(sc)\n",
    "    avgsc_val_lst.append(sc[1])\n",
    "    \n",
    "    loss, bscr_train = bestPerformingModel.evaluate([X_train_cur,structured_data_train_cur],to_categorical(y_train_cur))\n",
    "    print(loss, bscr_train)\n",
    "    loss, bscr_val = bestPerformingModel.evaluate([X_test_cur,structured_data_test_cur],to_categorical(y_test_cur))\n",
    "    print(loss, bscr_val)    \n",
    "    loss, bscr = bestPerformingModel.evaluate([X_te,structured_data_test],to_categorical(y_test))\n",
    "    print(loss, bscr)\n",
    "    \n",
    "    models.append(clf)\n",
    "    \n",
    "    avgsc = avgsc + bscr\n",
    "    \n",
    "    avgsc_train = avgsc_train + bscr_train\n",
    "    avgsc_val = avgsc_val + bscr_val\n",
    "    avgsc_lst.append(bscr)\n",
    "    avgsc_train_lst.append(bscr_train)\n",
    "    print('The training accuracy is:')\n",
    "    print(bscr_train)\n",
    "    print('The validation accuracy is:')\n",
    "    print(bscr_val)    \n",
    "    print('The test accuracy is:')    \n",
    "    print(bscr)\n",
    "    print('\\n')\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<keras.engine.training.Model object at 0x7f99d0804d90>, <keras.engine.training.Model object at 0x7f99d4cb1ed0>, <keras.engine.training.Model object at 0x7f99d1638610>, <keras.engine.training.Model object at 0x7f99d03ed990>, <keras.engine.training.Model object at 0x7f99b0698cd0>]\n",
      "[0.9905405405405405, 0.9912162162162163, 0.9908783783783783, 0.9896959459459459, 0.9880067567567568]\n",
      "[0.8770270270270271, 0.8675675675675676, 0.8675675675675676, 0.8797297297297297, 0.8736486486486487]\n",
      "[0.8816855757832849, 0.8833063213262347, 0.8827660728119181, 0.8811453272689683, 0.8827660721678917]\n",
      "0.9900675675675675\n",
      "0.8731081081081081\n",
      "0.8823338738716595\n",
      "1851/1851 [==============================] - 0s 76us/step\n",
      "[[1.00000000e+00 2.97056975e-13 1.15593654e-11 1.55729271e-12\n",
      "  6.76322687e-09]\n",
      " [1.00000000e+00 1.13202946e-15 1.22582120e-12 1.07208461e-11\n",
      "  4.43889364e-13]\n",
      " [3.68195265e-06 8.87664100e-06 5.86224178e-06 3.94786093e-06\n",
      "  9.99977589e-01]\n",
      " ...\n",
      " [4.21263212e-06 7.87665613e-07 9.99491215e-01 5.01225411e-04\n",
      "  2.52942004e-06]\n",
      " [9.99551475e-01 6.18178092e-07 4.79089904e-05 2.73953469e-06\n",
      "  3.97273019e-04]\n",
      " [9.96500373e-01 5.01854192e-05 2.71514780e-03 5.73726953e-04\n",
      "  1.60589363e-04]]\n",
      "[[671   0  13   3  19]\n",
      " [  5 120   9   2   7]\n",
      " [ 15   9 384  15   8]\n",
      " [  3   3  17 138   6]\n",
      " [ 37   8  27  11 321]]\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "print(models)\n",
    "print(avgsc_train_lst)\n",
    "print(avgsc_val_lst)\n",
    "print(avgsc_lst)\n",
    "\n",
    "# i = avgsc_lst.index(max(avgsc_lst))\n",
    "# bestPerformingModel = models[i]\n",
    "\n",
    "print(avgsc_train/k)\n",
    "print(avgsc_val/k)\n",
    "print(avgsc/k)\n",
    "\n",
    "\n",
    "loss, bscr = bestPerformingModel.evaluate([X_te,structured_data_test],to_categorical(y_test))\n",
    "y_pred = bestPerformingModel.predict([X_te,structured_data_test])\n",
    "\n",
    "df = DataFrame.from_records(y_pred)\n",
    "# print(df)\n",
    "df.to_csv('cnn_predictions.csv',index=False)\n",
    "\n",
    "print(y_pred)\n",
    "y_pred = [np.argmax(i) for i in y_pred]\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_df = pd.DataFrame({'y_pred':y_pred})\n",
    "# print(y_pred)\n",
    "dfn = pd.concat([y_pred_df,y_test],axis=1)\n",
    "dfn.to_csv('dfny.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "7400/7400 [==============================] - 5s 678us/step\n",
      "(0.1644142657493216, 0.9675675675675676)\n",
      "1851/1851 [==============================] - 0s 123us/step\n",
      "(0.5964181145772878, 0.8849270665471712)\n"
     ]
    }
   ],
   "source": [
    "bestone = load_model('best_weights1.h5')\n",
    "print('---')\n",
    "loss, acc = bestone.evaluate([X_t,structured_data_train],to_categorical(y_train),verbose=1)\n",
    "print(loss, acc)\n",
    "loss, acc = bestone.evaluate([X_te,structured_data_test],to_categorical(y_test),verbose=1)\n",
    "print(loss, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data1_train, data1_test = train_test_split(data1, test_size=0.2,random_state=100)\n",
    "data1_test = data1_test.reset_index(drop=True)\n",
    "atr_test.reset_index(inplace=True,drop=True)\n",
    "\n",
    "y_pred = bestone.predict([X_te,structured_data_test])\n",
    "i=0\n",
    "for x in y_pred:\n",
    "    j=0\n",
    "    for y in x:\n",
    "        y_pred[i][j] = round(y_pred[i][j],3)\n",
    "        j=j+1\n",
    "    i=i+1\n",
    "# print(y_pred)\n",
    "fone =  pd.DataFrame(y_pred)\n",
    "\n",
    "y_pred = [np.argmax(i) for i in y_pred]\n",
    "# print(y_pred)\n",
    "\n",
    "sone =  pd.DataFrame({'y_pred':y_pred})\n",
    "\n",
    "data = pd.concat([atr_test,samp1_test,data1_test,fone,sone,y_test], axis=1)\n",
    "data.to_csv('cnn_pred.csv')\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
